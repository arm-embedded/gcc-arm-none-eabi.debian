diff --git a/gcc/ChangeLog.arm b/gcc/ChangeLog.arm
new file mode 100644
index 0000000..05532b1
--- /dev/null
+++ b/gcc/ChangeLog.arm
@@ -0,0 +1,251 @@
+2014-12-02  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r214214
+	2014-08-20  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/thumb1.md (64bit splitter): Replace const_double_operand
+	with immediate_operand.
+
+2014-12-02  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r211817
+	2014-06-19  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/thumb1.md (define_split): Split 64bit constant in earlier
+	stage.
+
+2014-11-19  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r217687
+	2014-11-17  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/arm.c (arm_issue_rate): Return 2 for cortex-m7.
+	* config/arm/arm.md (generic_sched): Exclude cortex-m7.
+	(generic_vfp): Likewise.
+	* config/arm/cortex-m7.md: Pipeline description for cortex-m7.
+
+2014-11-10  Hale Wang  <hale.wang@arm.com>
+
+	Backport from mainline r217175
+	2014-11-06  Hale Wang  <hale.wang@arm.com>
+
+	* config/arm/arm-cores.def: Add support for
+	-mcpu=cortex-m0.small-multiply,cortex-m0plus.small-multiply,
+	cortex-m1.small-multiply.
+	* config/arm/arm-tables.opt: Regenerate.
+	* config/arm/arm-tune.md: Regenerate.
+	* config/arm/arm.c: Update the rtx-costs for MUL.
+	* config/arm/bpabi.h: Handle
+	-mcpu=cortex-m0.small-multiply,cortex-m0plus.small-multiply,
+	cortex-m1.small-multiply.
+	* doc/invoke.texi: Document
+	-mcpu=cortex-m0.small-multiply,cortex-m0plus.small-multiply,
+	cortex-m1.small-multiply.
+
+2014-11-10  Hale Wang  <hale.wang@arm.com>
+
+	Backport from mainline r217173
+	2014-11-06  Hale Wang  <hale.wang@arm.com>
+
+	* config/arm/arm.c: Add cortex-m7 tune.
+	* config/arm/arm-cores.def: Use cortex-m7 tune.
+
+2014-11-03  Hale Wang  <hale.wang@arm.com>
+
+	* Makefile.in (tree-switch-shortcut): New object file.
+	* common.opt (-ftree-switch-shortcut): New option.
+	* opts.c: Likewise.
+	* params.def: Define the maxinum number of instructions and paths
+	to duplicate when shortcutting a switch.
+	* passes.def (pass_tree_switch_shortcut): New pass.
+	* timevar.def (TV_TREE_SWITCH_SHORTCUT): Define new value.
+	* tree-pass.h (make_pass_tree_switch_shortcut): New pass.
+	* tree-switch-shortcut.c: New file.
+
+2014-10-08  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	Backport from mainline r215444
+	2014-09-22  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* config/arm/arm.c: #include "tm-constrs.h"
+	(thumb1_size_rtx_costs): Adjust rtx costs.
+
+2014-10-08  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	Backport from mainline r215540
+	2014-09-24  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* ira-color.c (assign_hard_reg): Ignore conflict cost if the
+	HARD_REGNO is not availabe for CONFLICT_A.
+
+	testsuite/ChangeLog:
+	2014-09-24  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* gcc.target/arm/pr63210.c: New test.
+
+2014-10-08  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	Backport from mainline r213691
+	2014-08-07  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* tree-ssa-loop-ivopts.c (get_address_cost): Try aligned offset.
+
+	testsuite/ChangeLog
+	2014-08-07  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* gcc.target/arm/get_address_cost_aligned_max_offset.c: New test.
+
+2014-10-06  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r215711
+	2014-09-30  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/arm-cores.def (cortex-m7): New core name.
+	* config/arm/arm-fpus.def (fpv5-sp-d16): New fpu name.
+	(fpv5-d16): Ditto.
+	* config/arm/arm-tables.opt: Regenerated.
+	* config/arm/arm-tune.md: Regenerated.
+	* config/arm/arm.h (TARGET_VFP5): New macro.
+	* config/arm/bpabi.h (BE8_LINK_SPEC): Include cortex-m7.
+	* config/arm/vfp.md (<vrint_pattern><SDF:mode>2,
+	smax<mode>3, smin<mode>3): Enabled for FPU FPv5.
+	* doc/invoke.texi: Document new cpu and fpu names.
+
+2014-10-05  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r212750
+	2014-07-17  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/types.md (alu_reg): Replaced by alu_sreg and alu_dsp_reg.
+	(alus_reg): Renamed to alus_sreg.
+	* config/arm/arm-fixed.md: Change type of non-dsp instructions
+	from alu_reg to alu_sreg.  Change type of dsp instructions from
+	alu_reg to alu_dsp_reg.
+	* config/arm/thumb1.md: Likewise.
+	* config/arm/thumb2.md: Likewise.
+	* config/arm/arm.c (cortexa7_older_only): Use new ALU type names.
+	* config/arm/arm1020e.md (1020alu_op): Replace alu_reg and alus_reg
+	with alu_sreg and alus_sreg.
+	* config/arm/arm1026ejs.md (alu_op): Likewise.
+	* config/arm/arm1136jfs.md (11_alu_op): Likewise.
+	* config/arm/arm926ejs.md (9_alu_op): Likewise.
+	* config/arm/fa526.md (526_alu_op): Likewise.
+	* config/arm/fa606te.md (606te_alu_op): Likewise.
+	* config/arm/fa626te.md (626te_alu_op): Likewise.
+	* config/arm/fa726te.md (726te_alu_op): Likewise.
+	* config/arm/fmp626.md (mp626_alu_op): Likewise.
+	* config/arm/arm.md (core_cycles): Replace alu_reg and alus_reg with
+	alu_sreg, alu_dsp_reg and alus_sreg.
+	* config/arm/cortex-a15.md (cortex_a15_alu): Likewise.
+	* config/arm/cortex-a5.md (cortex_a5_alu): Likewise.
+	* config/arm/cortex-a53.md (cortex_a53_alu): Likewise.
+	* config/arm/cortex-a7.md (cortex_a7_alu_sreg): Likewise.
+	* config/arm/cortex-a8.md (cortex_a8_alu): Likewise.
+	* config/arm/cortex-a9.md (cortex_a9_dp): Likewise.
+	* config/arm/cortex-m4.md (cortex_m4_alu): Likewise.
+	* config/arm/cortex-r4.md (cortex_r4_alu): Likewise.
+	* config/arm/marvell-pj4.md (pj4_alu, pj4_alu_conds): Likewise.
+	* config/aarch64/aarch64.md (*addsi3_aarch64, *addsi3_aarch64_uxtw,
+	subsi3, *adddi3_aarch64, *subsi3_uxtw, subdi3, absdi2, neg<mode>2,
+	*negsi2_uxtw, tlsle_small_<mode>): Rename type alu_reg to alu_sreg.
+	(add<mode>3_compare0, *addsi3_compare0_uxtw, *add<mode>3nr_compare0,
+	sub<mode>3_compare0, *compare_neg<mode>, *neg<mode>2_compare0,
+	subsi3_compare0_uxtw, *negsi2_compare0_uxtw, *cmp<mode>): Rename type
+	alus_reg to alus_sreg.
+
+2014-10-05  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r211443
+	2014-06-11  Terry Guo  <terry.guo@arm.com>
+
+	* config/arm/arm.md (*thumb1_adddi3): Move into new file thumb1.md.
+	(*thumb1_addsi3): Ditto.
+	(*thumb_subdi3): Ditto.
+	(thumb1_subsi3_insn): Ditto.
+	(*thumb_mulsi3): Ditto.
+	(*thumb_mulsi3_v6): Ditto.
+	(*thumb1_andsi3_insn): Ditto.
+	(thumb1_bicsi3): Ditto.
+	(*thumb1_iorsi3_insn): Ditto.
+	(*thumb1_xorsi3_insn): Ditto.
+	(*thumb1_ashlsi3): Ditto.
+	(*thumb1_ashrsi3): Ditto.
+	(*thumb1_lshrsi3): Ditto.
+	(*thumb1_rotrsi3): Ditto.
+	(*thumb1_negdi2): Ditto.
+	(*thumb1_negsi2): Ditto.
+	(*thumb1_abssi2): Ditto.
+	(*thumb1_neg_abssi2): Ditto.
+	(*thumb1_one_cmplsi2): Ditto.
+	(*thumb1_zero_extendhisi2): Ditto.
+	(*thumb1_zero_extendqisi2): Ditto.
+	(*thumb1_zero_extendqisi2_v6): Ditto.
+	(thumb1_extendhisi2): Ditto.
+	(thumb1_extendqisi2): Ditto.
+	(*thumb1_movdi_insn): Ditto.
+	(*thumb1_movsi_insn): Ditto.
+	(*thumb1_movhi_insn): Ditto.
+	(thumb_movhi_clobber): Ditto.
+	(*thumb1_movqi_insn): Ditto.
+	(*thumb1_movhf): Ditto.
+	(*thumb1_movsf_insn): Ditto.
+	(*thumb_movdf_insn): Ditto.
+	(movmem12b): Ditto.
+	(movmem8b): Ditto.
+	(cbranchqi4): Ditto.
+	(cbranchsi4_insn): Ditto.
+	(cbranchsi4_scratch): Ditto.
+	(*negated_cbranchsi4): Ditto.
+	(*tbit_cbranch): Ditto.
+	(*tlobits_cbranch): Ditto.
+	(*tstsi3_cbranch): Ditto.
+	(*cbranchne_decr1): Ditto.
+	(*addsi3_cbranch): Ditto.
+	(*addsi3_cbranch_scratch): Ditto.
+	(*thumb_cmpdi_zero): Ditto.
+	(cstoresi_eq0_thumb1): Ditto.
+	(cstoresi_ne0_thumb1): Ditto.
+	(*cstoresi_eq0_thumb1_insn): Ditto.
+	(*cstoresi_ne0_thumb1_insn): Ditto.
+	(cstoresi_nltu_thumb1): Ditto.
+	(cstoresi_ltu_thumb1): Ditto.
+	(thumb1_addsi3_addgeu): Ditto.
+	(*thumb_jump): Ditto.
+	(*call_reg_thumb1_v5): Ditto.
+	(*call_reg_thumb1): Ditto.
+	(*call_value_reg_thumb1_v5): Ditto.
+	(*call_value_reg_thumb1): Ditto.
+	(*call_insn): Ditto.
+	(*call_value_insn): Ditto.
+	(thumb1_casesi_internal_pic): Ditto.
+	(thumb1_casesi_dispatch): Ditto.
+	(*thumb1_indirect_jump): Ditto.
+	(prologue_thumb1_interwork): Ditto.
+	(*epilogue_insns): Ditto.
+	(consttable_1): Ditto.
+	(consttable_2): Ditto.
+	(tablejump): Ditto.
+	(*thumb1_tablejump): Ditto.
+	(thumb_eh_return): Ditto.
+	(define_peephole2): Two of them are thumb1 only and got moved into
+	new file thumb1.md.
+	(define_split): Six of them are thumb1 only and got moved into new
+	file thumb1.md.
+	* config/arm/thumb1.md: New file comprised of above thumb1 only
+	patterns.
+
+2014-09-30  Zhenqiang Chen  <zhenqiang.chen@arm.com>
+
+	* config/arm/arm.c (arm_option_override): Disable loop2_invariant
+	pass when optimize_size and ira-loop-pressure is not enabled.
+
+2014-05-12  Terry Guo  <terry.guo@arm.com>
+
+	* config.gcc (--with-multilib-list): Accept arm embedded cores.
+	* configure.ac (with_multilib_list): Export for being used in arm
+	embedded multilib fragment.
+	* configure: Regenerated.
+	* Makefile.in (with_multilib_list): Import for being used in
+	multilib fragment.
+	* config/arm/t-rmprofile: New multilib fragment for arm embedded
+	cores.
diff --git a/gcc/Makefile.in b/gcc/Makefile.in
index b47733c..f9e76ad 100644
--- a/gcc/Makefile.in
+++ b/gcc/Makefile.in
@@ -540,6 +540,7 @@ lang_opt_files=@lang_opt_files@ $(srcdir)/c-family/c.opt $(srcdir)/common.opt
 lang_specs_files=@lang_specs_files@
 lang_tree_files=@lang_tree_files@
 target_cpu_default=@target_cpu_default@
+with_multilib_list=@with_multilib_list@
 OBJC_BOEHM_GC=@objc_boehm_gc@
 extra_modes_file=@extra_modes_file@
 extra_opt_files=@extra_opt_files@
@@ -1402,6 +1403,7 @@ OBJS = \
 	tree-scalar-evolution.o \
 	tree-sra.o \
 	tree-switch-conversion.o \
+	tree-switch-shortcut.o \
 	tree-ssa-address.o \
 	tree-ssa-alias.o \
 	tree-ssa-ccp.o \
diff --git a/gcc/common.opt b/gcc/common.opt
index 2259f29..8b2c055 100644
--- a/gcc/common.opt
+++ b/gcc/common.opt
@@ -2197,6 +2197,10 @@ ftree-sra
 Common Report Var(flag_tree_sra) Optimization
 Perform scalar replacement of aggregates
 
+ftree-switch-shortcut
+Common Report Var(flag_tree_switch_shortcut) Init(0) Optimization
+Convert jumps to switch statements into jumps to case statement.
+
 ftree-ter
 Common Report Var(flag_tree_ter) Optimization
 Replace temporary expressions in the SSA->normal pass
diff --git a/gcc/config.gcc b/gcc/config.gcc
index 90d4f71..1e1d671 100644
--- a/gcc/config.gcc
+++ b/gcc/config.gcc
@@ -3619,6 +3619,18 @@ case "${target}" in
 					tmake_file="${tmake_file} arm/t-aprofile"
 					break
 					;;
+				armv6-m | armv7-m | armv7e-m | armv7-r | armv7-a)
+					if test "x$with_arch" != x \
+					    || test "x$with_cpu" != x \
+					    || test "x$with_float" != x \
+					    || test "x$with_fpu" != x \
+					    || test "x$with_mode" != x ; then
+					    echo "Error: You cannot use any of --with-arch/cpu/fpu/float/mode with --with-multilib-list=${with_multilib_list}" 1>&2
+					    exit 1
+					fi
+					tmake_file="${tmake_file} arm/t-rmprofile"
+					break
+					;;
 				default)
 					;;
 				*)
diff --git a/gcc/config/aarch64/aarch64.md b/gcc/config/aarch64/aarch64.md
index 46d9e9f..18f6924 100644
--- a/gcc/config/aarch64/aarch64.md
+++ b/gcc/config/aarch64/aarch64.md
@@ -1072,7 +1072,7 @@
   add\\t%w0, %w1, %2
   add\\t%w0, %w1, %w2
   sub\\t%w0, %w1, #%n2"
-  [(set_attr "type" "alu_imm,alu_reg,alu_imm")]
+  [(set_attr "type" "alu_imm,alu_sreg,alu_imm")]
 )
 
 ;; zero_extend version of above
@@ -1087,7 +1087,7 @@
   add\\t%w0, %w1, %2
   add\\t%w0, %w1, %w2
   sub\\t%w0, %w1, #%n2"
-  [(set_attr "type" "alu_imm,alu_reg,alu_imm")]
+  [(set_attr "type" "alu_imm,alu_sreg,alu_imm")]
 )
 
 (define_insn "*adddi3_aarch64"
@@ -1102,7 +1102,7 @@
   add\\t%x0, %x1, %x2
   sub\\t%x0, %x1, #%n2
   add\\t%d0, %d1, %d2"
-  [(set_attr "type" "alu_imm,alu_reg,alu_imm,neon_add")
+  [(set_attr "type" "alu_imm,alu_sreg,alu_imm,neon_add")
    (set_attr "simd" "*,*,*,yes")]
 )
 
@@ -1119,7 +1119,7 @@
   adds\\t%<w>0, %<w>1, %<w>2
   adds\\t%<w>0, %<w>1, %<w>2
   subs\\t%<w>0, %<w>1, #%n2"
-  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
+  [(set_attr "type" "alus_sreg,alus_imm,alus_imm")]
 )
 
 ;; zero_extend version of above
@@ -1136,7 +1136,7 @@
   adds\\t%w0, %w1, %w2
   adds\\t%w0, %w1, %w2
   subs\\t%w0, %w1, #%n2"
-  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
+  [(set_attr "type" "alus_sreg,alus_imm,alus_imm")]
 )
 
 (define_insn "*adds_mul_imm_<mode>"
@@ -1250,7 +1250,7 @@
   cmn\\t%<w>0, %<w>1
   cmn\\t%<w>0, %<w>1
   cmp\\t%<w>0, #%n1"
-  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
+  [(set_attr "type" "alus_sreg,alus_imm,alus_imm")]
 )
 
 (define_insn "*compare_neg<mode>"
@@ -1260,7 +1260,7 @@
 	 (match_operand:GPI 1 "register_operand" "r")))]
   ""
   "cmn\\t%<w>1, %<w>0"
-  [(set_attr "type" "alus_reg")]
+  [(set_attr "type" "alus_sreg")]
 )
 
 (define_insn "*add_<shift>_<mode>"
@@ -1532,7 +1532,7 @@
 		   (match_operand:SI 2 "register_operand" "r")))]
   ""
   "sub\\t%w0, %w1, %w2"
-  [(set_attr "type" "alu_reg")]
+  [(set_attr "type" "alu_sreg")]
 )
 
 ;; zero_extend version of above
@@ -1543,7 +1543,7 @@
 		   (match_operand:SI 2 "register_operand" "r"))))]
   ""
   "sub\\t%w0, %w1, %w2"
-  [(set_attr "type" "alu_reg")]
+  [(set_attr "type" "alu_sreg")]
 )
 
 (define_insn "subdi3"
@@ -1554,7 +1554,7 @@
   "@
    sub\\t%x0, %x1, %x2
    sub\\t%d0, %d1, %d2"
-  [(set_attr "type" "alu_reg, neon_sub")
+  [(set_attr "type" "alu_sreg, neon_sub")
    (set_attr "simd" "*,yes")]
 )
 
@@ -1568,7 +1568,7 @@
 	(minus:GPI (match_dup 1) (match_dup 2)))]
   ""
   "subs\\t%<w>0, %<w>1, %<w>2"
-  [(set_attr "type" "alus_reg")]
+  [(set_attr "type" "alus_sreg")]
 )
 
 ;; zero_extend version of above
@@ -1581,7 +1581,7 @@
 	(zero_extend:DI (minus:SI (match_dup 1) (match_dup 2))))]
   ""
   "subs\\t%w0, %w1, %w2"
-  [(set_attr "type" "alus_reg")]
+  [(set_attr "type" "alus_sreg")]
 )
 
 (define_insn "*sub_<shift>_<mode>"
@@ -1792,7 +1792,7 @@
 							     GEN_INT (63)))));
     DONE;
   }
-  [(set_attr "type" "alu_reg")]
+  [(set_attr "type" "alu_sreg")]
 )
 
 (define_insn "neg<mode>2"
@@ -1802,7 +1802,7 @@
   "@
    neg\\t%<w>0, %<w>1
    neg\\t%<rtn>0<vas>, %<rtn>1<vas>"
-  [(set_attr "type" "alu_reg, neon_neg<q>")
+  [(set_attr "type" "alu_sreg, neon_neg<q>")
    (set_attr "simd" "*,yes")]
 )
 
@@ -1812,7 +1812,7 @@
 	(zero_extend:DI (neg:SI (match_operand:SI 1 "register_operand" "r"))))]
   ""
   "neg\\t%w0, %w1"
-  [(set_attr "type" "alu_reg")]
+  [(set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*ngc<mode>"
@@ -1842,7 +1842,7 @@
 	(neg:GPI (match_dup 1)))]
   ""
   "negs\\t%<w>0, %<w>1"
-  [(set_attr "type" "alus_reg")]
+  [(set_attr "type" "alus_sreg")]
 )
 
 ;; zero_extend version of above
@@ -1854,7 +1854,7 @@
 	(zero_extend:DI (neg:SI (match_dup 1))))]
   ""
   "negs\\t%w0, %w1"
-  [(set_attr "type" "alus_reg")]
+  [(set_attr "type" "alus_sreg")]
 )
 
 (define_insn "*neg_<shift><mode>3_compare0"
@@ -2091,7 +2091,7 @@
    cmp\\t%<w>0, %<w>1
    cmp\\t%<w>0, %<w>1
    cmn\\t%<w>0, #%n1"
-  [(set_attr "type" "alus_reg,alus_imm,alus_imm")]
+  [(set_attr "type" "alus_sreg,alus_imm,alus_imm")]
 )
 
 (define_insn "*cmp<mode>"
@@ -3521,7 +3521,7 @@
 		  (match_operand 2 "aarch64_valid_symref" "S")))]
   ""
   "add\\t%<w>0, %<w>1, :lo12:%a2"
-  [(set_attr "type" "alu_reg")]
+  [(set_attr "type" "alu_sreg")]
 )
 
 (define_insn "ldr_got_small_<mode>"
@@ -3605,7 +3605,7 @@
 		   UNSPEC_GOTSMALLTLS))]
   ""
   "add\\t%0, %1, #%G2\;add\\t%0, %0, #%L2"
-  [(set_attr "type" "alu_reg")
+  [(set_attr "type" "alu_sreg")
    (set_attr "length" "8")]
 )
 
diff --git a/gcc/config/arm/arm-cores.def b/gcc/config/arm/arm-cores.def
index 56041ec..f65771a 100644
--- a/gcc/config/arm/arm-cores.def
+++ b/gcc/config/arm/arm-cores.def
@@ -137,6 +137,11 @@ ARM_CORE("cortex-m1",		cortexm1, cortexm1,		6M, FL_LDSCHED, v6m)
 ARM_CORE("cortex-m0",		cortexm0, cortexm0,		6M, FL_LDSCHED, v6m)
 ARM_CORE("cortex-m0plus",	cortexm0plus, cortexm0plus,	6M, FL_LDSCHED, v6m)
 
+/* V6M Architecture Processors for small-multiply implementations.  */
+ARM_CORE("cortex-m1.small-multiply",	cortexm1smallmultiply, cortexm1,	6M, FL_LDSCHED | FL_SMALLMUL, v6m)
+ARM_CORE("cortex-m0.small-multiply",	cortexm0smallmultiply, cortexm0,	6M, FL_LDSCHED | FL_SMALLMUL, v6m)
+ARM_CORE("cortex-m0plus.small-multiply",cortexm0plussmallmultiply, cortexm0plus,6M, FL_LDSCHED | FL_SMALLMUL, v6m)
+
 /* V7 Architecture Processors */
 ARM_CORE("generic-armv7-a",	genericv7a, genericv7a,		7A,  FL_LDSCHED, cortex)
 ARM_CORE("cortex-a5",		cortexa5, cortexa5,		7A,  FL_LDSCHED, cortex_a5)
@@ -149,6 +154,7 @@ ARM_CORE("cortex-r4",		cortexr4, cortexr4,		7R,  FL_LDSCHED, cortex)
 ARM_CORE("cortex-r4f",		cortexr4f, cortexr4f,		7R,  FL_LDSCHED, cortex)
 ARM_CORE("cortex-r5",		cortexr5, cortexr5,		7R,  FL_LDSCHED | FL_ARM_DIV, cortex)
 ARM_CORE("cortex-r7",		cortexr7, cortexr7,		7R,  FL_LDSCHED | FL_ARM_DIV, cortex)
+ARM_CORE("cortex-m7",		cortexm7, cortexm7,		7EM, FL_LDSCHED, cortex_m7)
 ARM_CORE("cortex-m4",		cortexm4, cortexm4,		7EM, FL_LDSCHED, v7m)
 ARM_CORE("cortex-m3",		cortexm3, cortexm3,		7M,  FL_LDSCHED, v7m)
 ARM_CORE("marvell-pj4",		marvell_pj4, marvell_pj4,	7A,  FL_LDSCHED, 9e)
diff --git a/gcc/config/arm/arm-fixed.md b/gcc/config/arm/arm-fixed.md
index 4ab9d35..5611ad1 100644
--- a/gcc/config/arm/arm-fixed.md
+++ b/gcc/config/arm/arm-fixed.md
@@ -26,7 +26,7 @@
   "add%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "yes,no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_sreg")])
 
 (define_insn "add<mode>3"
   [(set (match_operand:ADDSUB 0 "s_register_operand" "=r")
@@ -36,7 +36,7 @@
   "sadd<qaddsub_suf>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_dsp_reg")])
 
 (define_insn "usadd<mode>3"
   [(set (match_operand:UQADDSUB 0 "s_register_operand" "=r")
@@ -46,7 +46,7 @@
   "uqadd<qaddsub_suf>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_dsp_reg")])
 
 (define_insn "ssadd<mode>3"
   [(set (match_operand:QADDSUB 0 "s_register_operand" "=r")
@@ -56,7 +56,7 @@
   "qadd<qaddsub_suf>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_dsp_reg")])
 
 (define_insn "sub<mode>3"
   [(set (match_operand:FIXED 0 "s_register_operand" "=l,r")
@@ -66,7 +66,7 @@
   "sub%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "yes,no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_sreg")])
 
 (define_insn "sub<mode>3"
   [(set (match_operand:ADDSUB 0 "s_register_operand" "=r")
@@ -76,7 +76,7 @@
   "ssub<qaddsub_suf>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_dsp_reg")])
 
 (define_insn "ussub<mode>3"
   [(set (match_operand:UQADDSUB 0 "s_register_operand" "=r")
@@ -87,7 +87,7 @@
   "uqsub<qaddsub_suf>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_dsp_reg")])
 
 (define_insn "sssub<mode>3"
   [(set (match_operand:QADDSUB 0 "s_register_operand" "=r")
@@ -97,7 +97,7 @@
   "qsub<qaddsub_suf>%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "no")
-   (set_attr "type" "alu_reg")])
+   (set_attr "type" "alu_dsp_reg")])
 
 ;; Fractional multiplies.
 
diff --git a/gcc/config/arm/arm-fpus.def b/gcc/config/arm/arm-fpus.def
index 85d9693..edd0c35 100644
--- a/gcc/config/arm/arm-fpus.def
+++ b/gcc/config/arm/arm-fpus.def
@@ -37,6 +37,8 @@ ARM_FPU("neon-fp16",	ARM_FP_MODEL_VFP, 3, VFP_REG_D32, true, true, false)
 ARM_FPU("vfpv4",	ARM_FP_MODEL_VFP, 4, VFP_REG_D32, false, true, false)
 ARM_FPU("vfpv4-d16",	ARM_FP_MODEL_VFP, 4, VFP_REG_D16, false, true, false)
 ARM_FPU("fpv4-sp-d16",	ARM_FP_MODEL_VFP, 4, VFP_REG_SINGLE, false, true, false)
+ARM_FPU("fpv5-sp-d16",	ARM_FP_MODEL_VFP, 5, VFP_REG_SINGLE, false, true, false)
+ARM_FPU("fpv5-d16",	ARM_FP_MODEL_VFP, 5, VFP_REG_D16, false, true, false)
 ARM_FPU("neon-vfpv4",	ARM_FP_MODEL_VFP, 4, VFP_REG_D32, true, true, false)
 ARM_FPU("fp-armv8",	ARM_FP_MODEL_VFP, 8, VFP_REG_D32, false, true, false)
 ARM_FPU("neon-fp-armv8",ARM_FP_MODEL_VFP, 8, VFP_REG_D32, true, true, false)
diff --git a/gcc/config/arm/arm-tables.opt b/gcc/config/arm/arm-tables.opt
index bc046a0..9c7e944 100644
--- a/gcc/config/arm/arm-tables.opt
+++ b/gcc/config/arm/arm-tables.opt
@@ -241,6 +241,15 @@ EnumValue
 Enum(processor_type) String(cortex-m0plus) Value(cortexm0plus)
 
 EnumValue
+Enum(processor_type) String(cortex-m1.small-multiply) Value(cortexm1smallmultiply)
+
+EnumValue
+Enum(processor_type) String(cortex-m0.small-multiply) Value(cortexm0smallmultiply)
+
+EnumValue
+Enum(processor_type) String(cortex-m0plus.small-multiply) Value(cortexm0plussmallmultiply)
+
+EnumValue
 Enum(processor_type) String(generic-armv7-a) Value(genericv7a)
 
 EnumValue
@@ -274,6 +283,9 @@ EnumValue
 Enum(processor_type) String(cortex-r7) Value(cortexr7)
 
 EnumValue
+Enum(processor_type) String(cortex-m7) Value(cortexm7)
+
+EnumValue
 Enum(processor_type) String(cortex-m4) Value(cortexm4)
 
 EnumValue
@@ -423,17 +435,23 @@ EnumValue
 Enum(arm_fpu) String(fpv4-sp-d16) Value(11)
 
 EnumValue
-Enum(arm_fpu) String(neon-vfpv4) Value(12)
+Enum(arm_fpu) String(fpv5-sp-d16) Value(12)
+
+EnumValue
+Enum(arm_fpu) String(fpv5-d16) Value(13)
+
+EnumValue
+Enum(arm_fpu) String(neon-vfpv4) Value(14)
 
 EnumValue
-Enum(arm_fpu) String(fp-armv8) Value(13)
+Enum(arm_fpu) String(fp-armv8) Value(15)
 
 EnumValue
-Enum(arm_fpu) String(neon-fp-armv8) Value(14)
+Enum(arm_fpu) String(neon-fp-armv8) Value(16)
 
 EnumValue
-Enum(arm_fpu) String(crypto-neon-fp-armv8) Value(15)
+Enum(arm_fpu) String(crypto-neon-fp-armv8) Value(17)
 
 EnumValue
-Enum(arm_fpu) String(vfp3) Value(16)
+Enum(arm_fpu) String(vfp3) Value(18)
 
diff --git a/gcc/config/arm/arm-tune.md b/gcc/config/arm/arm-tune.md
index 954cab8..84355d6 100644
--- a/gcc/config/arm/arm-tune.md
+++ b/gcc/config/arm/arm-tune.md
@@ -25,10 +25,12 @@
 	arm1176jzs,arm1176jzfs,mpcorenovfp,
 	mpcore,arm1156t2s,arm1156t2fs,
 	cortexm1,cortexm0,cortexm0plus,
+	cortexm1smallmultiply,cortexm0smallmultiply,cortexm0plussmallmultiply,
 	genericv7a,cortexa5,cortexa7,
 	cortexa8,cortexa9,cortexa12,
 	cortexa15,cortexr4,cortexr4f,
-	cortexr5,cortexr7,cortexm4,
-	cortexm3,marvell_pj4,cortexa15cortexa7,
-	cortexa53,cortexa57,cortexa57cortexa53"
+	cortexr5,cortexr7,cortexm7,
+	cortexm4,cortexm3,marvell_pj4,
+	cortexa15cortexa7,cortexa53,cortexa57,
+	cortexa57cortexa53"
 	(const (symbol_ref "((enum attr_tune) arm_tune)")))
diff --git a/gcc/config/arm/arm.c b/gcc/config/arm/arm.c
index 5e2571c..f0da3b3 100644
--- a/gcc/config/arm/arm.c
+++ b/gcc/config/arm/arm.c
@@ -59,6 +59,7 @@
 #include "params.h"
 #include "opts.h"
 #include "dumpfile.h"
+#include "tm-constrs.h"
 
 /* Forward definitions of types.  */
 typedef struct minipool_node    Mnode;
@@ -740,6 +741,8 @@ static int thumb_call_reg_needed;
 #define FL_ARCH8      (1 << 24)       /* Architecture 8.  */
 #define FL_CRC32      (1 << 25)	      /* ARMv8 CRC32 instructions.  */
 
+#define FL_SMALLMUL   (1 << 26)       /* Small multiply supported.  */
+
 #define FL_IWMMXT     (1 << 29)	      /* XScale v2 or "Intel Wireless MMX technology".  */
 #define FL_IWMMXT2    (1 << 30)       /* "Intel Wireless MMX2 technology".  */
 
@@ -907,6 +910,9 @@ int arm_condexec_masklen = 0;
 /* Nonzero if chip supports the ARMv8 CRC instructions.  */
 int arm_arch_crc = 0;
 
+/* Nonzero if the core has a very small, high-latency, multiply unit.  */
+int arm_m_profile_small_mul = 0;
+
 /* The condition codes of the ARM, and the inverse function.  */
 static const char * const arm_condition_codes[] =
 {
@@ -1724,6 +1730,24 @@ const struct tune_params arm_v7m_tune =
   false                                         /* Prefer Neon for 64-bits bitops.  */
 };
 
+/* Cortex-M7 tuning.  */
+
+const struct tune_params arm_cortex_m7_tune =
+{
+  arm_9e_rtx_costs,
+  &v7m_extra_costs,
+  NULL,						/* Sched adj cost.  */
+  0,						/* Constant limit.  */
+  0,						/* Max cond insns.  */
+  ARM_PREFETCH_NOT_BENEFICIAL,
+  true,						/* Prefer constant pool.  */
+  arm_cortex_m_branch_cost,
+  false,					/* Prefer LDRD/STRD.  */
+  {true, true},					/* Prefer non short circuit.  */
+  &arm_default_vec_cost,                        /* Vectorizer costs.  */
+  false                                         /* Prefer Neon for 64-bits bitops.  */
+};
+
 /* The arm_v6m_tune is duplicated from arm_cortex_tune, rather than
    arm_v6t2_tune. It is used for cortex-m0, cortex-m1 and cortex-m0plus.  */
 const struct tune_params arm_v6m_tune =
@@ -2503,6 +2527,7 @@ arm_option_override (void)
   arm_arch_arm_hwdiv = (insn_flags & FL_ARM_DIV) != 0;
   arm_tune_cortex_a9 = (arm_tune == cortexa9) != 0;
   arm_arch_crc = (insn_flags & FL_CRC32) != 0;
+  arm_m_profile_small_mul = (insn_flags & FL_SMALLMUL) != 0;
   if (arm_restrict_it == 2)
     arm_restrict_it = arm_arch8 && TARGET_THUMB2;
 
@@ -2818,6 +2843,13 @@ arm_option_override (void)
   if (TARGET_APCS_FRAME)
     flag_shrink_wrap = false;
 
+  /* Do not move invariants out of loops since it tends to increase register
+     pressure.  The heuristic to estimate register pressure does not fit for
+     ARM.  -fira-loop-pressure tends to get more precise estimation.  But it
+     still need more tuning.  */
+  if (optimize_function_for_size_p (cfun) && !flag_ira_loop_pressure)
+    flag_move_loop_invariants = 0;
+
   /* We only support -mslow-flash-data on armv7-m targets.  */
   if (target_slow_flash_data
       && ((!(arm_arch7 && !arm_arch_notm) && !arm_arch7em)
@@ -8637,7 +8669,13 @@ thumb1_size_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer)
           /* Thumb1 mul instruction can't operate on const. We must Load it
              into a register first.  */
           int const_size = thumb1_size_rtx_costs (XEXP (x, 1), CONST_INT, SET);
-          return COSTS_N_INSNS (1) + const_size;
+	  /* For the targets which have a very small and high-latency multiply
+	     unit, we prefer to synthesize the mult with up to 5 instructions,
+	     giving a good balance between size and performance.  */
+	  if (arm_arch6m && arm_m_profile_small_mul)
+	    return COSTS_N_INSNS (5);
+	  else
+	    return COSTS_N_INSNS (1) + const_size;
         }
       return COSTS_N_INSNS (1);
 
@@ -8645,9 +8683,11 @@ thumb1_size_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer)
       /* A SET doesn't have a mode, so let's look at the SET_DEST to get
 	 the mode.  */
       words = ARM_NUM_INTS (GET_MODE_SIZE (GET_MODE (SET_DEST (x))));
-      return (COSTS_N_INSNS (words)
-              + 4 * ((MEM_P (SET_SRC (x)))
-                     + MEM_P (SET_DEST (x))));
+      return COSTS_N_INSNS (words)
+	     + COSTS_N_INSNS (1) * (satisfies_constraint_J (SET_SRC (x))
+				    || satisfies_constraint_K (SET_SRC (x))
+				       /* thumb1_movdi_insn.  */
+				    || ((words > 1) && MEM_P (SET_SRC (x))));
 
     case CONST_INT:
       if (outer == SET)
@@ -8700,16 +8740,14 @@ thumb1_size_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer)
     case AND:
     case XOR:
     case IOR:
-      /* XXX guess.  */
-      return 8;
+      return COSTS_N_INSNS (1);
 
     case MEM:
-      /* XXX another guess.  */
-      /* Memory costs quite a lot for the first word, but subsequent words
-         load at the equivalent of a single insn each.  */
-      return (10 + 4 * ((GET_MODE_SIZE (mode) - 1) / UNITS_PER_WORD)
+      return (COSTS_N_INSNS (1)
+	      + COSTS_N_INSNS (1)
+		* ((GET_MODE_SIZE (mode) - 1) / UNITS_PER_WORD)
               + ((GET_CODE (x) == SYMBOL_REF && CONSTANT_POOL_ADDRESS_P (x))
-                 ? 4 : 0));
+                 ? COSTS_N_INSNS (1) : 0));
 
     case IF_THEN_ELSE:
       /* XXX a guess.  */
@@ -10969,7 +11007,11 @@ arm_9e_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,
       switch (code)
 	{
 	case MULT:
-	  *total = COSTS_N_INSNS (3);
+	  /* Small multiply: 32 cycles for an integer multiply inst.  */
+	  if (arm_arch6m && arm_m_profile_small_mul)
+	    *total = COSTS_N_INSNS (32);
+	  else
+	    *total = COSTS_N_INSNS (3);
 	  return true;
 
 	default:
@@ -11385,8 +11427,9 @@ cortexa7_older_only (rtx insn)
 
   switch (get_attr_type (insn))
     {
-    case TYPE_ALU_REG:
-    case TYPE_ALUS_REG:
+    case TYPE_ALU_DSP_REG:
+    case TYPE_ALU_SREG:
+    case TYPE_ALUS_SREG:
     case TYPE_LOGIC_REG:
     case TYPE_LOGICS_REG:
     case TYPE_ADC_REG:
@@ -29412,6 +29455,7 @@ arm_issue_rate (void)
     case cortexa57:
       return 3;
 
+    case cortexm7:
     case cortexr4:
     case cortexr4f:
     case cortexr5:
diff --git a/gcc/config/arm/arm.h b/gcc/config/arm/arm.h
index c0f2184..6ba1339 100644
--- a/gcc/config/arm/arm.h
+++ b/gcc/config/arm/arm.h
@@ -298,6 +298,9 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
 /* FPU supports VFPv3 instructions.  */
 #define TARGET_VFP3 (TARGET_VFP && arm_fpu_desc->rev >= 3)
 
+/* FPU supports FPv5 instructions.  */
+#define TARGET_VFP5 (TARGET_VFP && arm_fpu_desc->rev >= 5)
+
 /* FPU only supports VFP single-precision instructions.  */
 #define TARGET_VFP_SINGLE (TARGET_VFP && arm_fpu_desc->regs == VFP_REG_SINGLE)
 
diff --git a/gcc/config/arm/arm.md b/gcc/config/arm/arm.md
index 8119387..b8586da 100644
--- a/gcc/config/arm/arm.md
+++ b/gcc/config/arm/arm.md
@@ -320,8 +320,8 @@
 ; than one on the main cpu execution unit.
 (define_attr "core_cycles" "single,multi"
   (if_then_else (eq_attr "type"
-    "adc_imm, adc_reg, adcs_imm, adcs_reg, adr, alu_ext, alu_imm, alu_reg,\
-    alu_shift_imm, alu_shift_reg, alus_ext, alus_imm, alus_reg,\
+    "adc_imm, adc_reg, adcs_imm, adcs_reg, adr, alu_ext, alu_imm, alu_sreg,\
+    alu_shift_imm, alu_shift_reg, alu_dsp_reg, alus_ext, alus_imm, alus_sreg,\
     alus_shift_imm, alus_shift_reg, bfm, csel, rev, logic_imm, logic_reg,\
     logic_shift_imm, logic_shift_reg, logics_imm, logics_reg,\
     logics_shift_imm, logics_shift_reg, extend, shift_imm, float, fcsel,\
@@ -377,7 +377,11 @@
 
 (define_attr "generic_sched" "yes,no"
   (const (if_then_else
-          (ior (eq_attr "tune" "fa526,fa626,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1020e,arm1026ejs,arm1136js,arm1136jfs,cortexa5,cortexa7,cortexa8,cortexa9,cortexa12,cortexa15,cortexa53,cortexm4,marvell_pj4")
+          (ior (eq_attr "tune" "fa526,fa626,fa606te,fa626te,fmp626,fa726te,\
+                                arm926ejs,arm1020e,arm1026ejs,arm1136js,\
+                                arm1136jfs,cortexa5,cortexa7,cortexa8,\
+                                cortexa9,cortexa12,cortexa15,cortexa53,\
+                                cortexm4,cortexm7,marvell_pj4")
 	       (eq_attr "tune_cortexr4" "yes"))
           (const_string "no")
           (const_string "yes"))))
@@ -385,7 +389,9 @@
 (define_attr "generic_vfp" "yes,no"
   (const (if_then_else
 	  (and (eq_attr "fpu" "vfp")
-	       (eq_attr "tune" "!arm1020e,arm1022e,cortexa5,cortexa7,cortexa8,cortexa9,cortexa53,cortexm4,marvell_pj4")
+	       (eq_attr "tune" "!arm1020e,arm1022e,cortexa5,cortexa7,\
+                                cortexa8,cortexa9,cortexa53,cortexm4,\
+                                cortexm7,marvell_pj4")
 	       (eq_attr "tune_cortexr4" "no"))
 	  (const_string "yes")
 	  (const_string "no"))))
@@ -409,6 +415,7 @@
 (include "cortex-a53.md")
 (include "cortex-r4.md")
 (include "cortex-r4f.md")
+(include "cortex-m7.md")
 (include "cortex-m4.md")
 (include "cortex-m4-fpu.md")
 (include "vfp11.md")
@@ -442,18 +449,6 @@
   "
 )
 
-(define_insn "*thumb1_adddi3"
-  [(set (match_operand:DI          0 "register_operand" "=l")
-	(plus:DI (match_operand:DI 1 "register_operand" "%0")
-		 (match_operand:DI 2 "register_operand" "l")))
-   (clobber (reg:CC CC_REGNUM))
-  ]
-  "TARGET_THUMB1"
-  "add\\t%Q0, %Q0, %Q2\;adc\\t%R0, %R0, %R2"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
 (define_insn_and_split "*arm_adddi3"
   [(set (match_operand:DI          0 "s_register_operand" "=&r,&r,&r,&r,&r")
 	(plus:DI (match_operand:DI 1 "s_register_operand" "%0, 0, r, 0, r")
@@ -614,75 +609,10 @@
    (set_attr "arch" "t2,t2,t2,t2,*,*,*,t2,t2,*,*,a,t2,t2,*")
    (set (attr "type") (if_then_else (match_operand 2 "const_int_operand" "")
 		      (const_string "alu_imm")
-		      (const_string "alu_reg")))
+		      (const_string "alu_sreg")))
  ]
 )
 
-(define_insn_and_split "*thumb1_addsi3"
-  [(set (match_operand:SI          0 "register_operand" "=l,l,l,*rk,*hk,l,k,l,l,l")
-	(plus:SI (match_operand:SI 1 "register_operand" "%0,0,l,*0,*0,k,k,0,l,k")
-		 (match_operand:SI 2 "nonmemory_operand" "I,J,lL,*hk,*rk,M,O,Pa,Pb,Pc")))]
-  "TARGET_THUMB1"
-  "*
-   static const char * const asms[] = 
-   {
-     \"add\\t%0, %0, %2\",
-     \"sub\\t%0, %0, #%n2\",
-     \"add\\t%0, %1, %2\",
-     \"add\\t%0, %0, %2\",
-     \"add\\t%0, %0, %2\",
-     \"add\\t%0, %1, %2\",
-     \"add\\t%0, %1, %2\",
-     \"#\",
-     \"#\",
-     \"#\"
-   };
-   if ((which_alternative == 2 || which_alternative == 6)
-       && CONST_INT_P (operands[2])
-       && INTVAL (operands[2]) < 0)
-     return \"sub\\t%0, %1, #%n2\";
-   return asms[which_alternative];
-  "
-  "&& reload_completed && CONST_INT_P (operands[2])
-   && ((operands[1] != stack_pointer_rtx
-        && (INTVAL (operands[2]) > 255 || INTVAL (operands[2]) < -255))
-       || (operands[1] == stack_pointer_rtx
- 	   && INTVAL (operands[2]) > 1020))"
-  [(set (match_dup 0) (plus:SI (match_dup 1) (match_dup 2)))
-   (set (match_dup 0) (plus:SI (match_dup 0) (match_dup 3)))]
-  {
-    HOST_WIDE_INT offset = INTVAL (operands[2]);
-    if (operands[1] == stack_pointer_rtx)
-      offset -= 1020;
-    else
-      {
-        if (offset > 255)
-	  offset = 255;
-	else if (offset < -255)
-	  offset = -255;
-      }
-    operands[3] = GEN_INT (offset);
-    operands[2] = GEN_INT (INTVAL (operands[2]) - offset);
-  }
-  [(set_attr "length" "2,2,2,2,2,2,2,4,4,4")
-   (set_attr "type" "alus_imm,alus_imm,alus_reg,alus_reg,alus_reg,
-		     alus_reg,alus_reg,multiple,multiple,multiple")]
-)
-
-;; Reloading and elimination of the frame pointer can
-;; sometimes cause this optimization to be missed.
-(define_peephole2
-  [(set (match_operand:SI 0 "arm_general_register_operand" "")
-	(match_operand:SI 1 "const_int_operand" ""))
-   (set (match_dup 0)
-	(plus:SI (match_dup 0) (reg:SI SP_REGNUM)))]
-  "TARGET_THUMB1
-   && (unsigned HOST_WIDE_INT) (INTVAL (operands[1])) < 1024
-   && (INTVAL (operands[1]) & 3) == 0"
-  [(set (match_dup 0) (plus:SI (reg:SI SP_REGNUM) (match_dup 1)))]
-  ""
-)
-
 (define_insn "addsi3_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
@@ -697,7 +627,7 @@
    sub%.\\t%0, %1, #%n2
    add%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type" "alus_imm,alus_imm,alus_reg")]
+   (set_attr "type" "alus_imm,alus_imm,alus_sreg")]
 )
 
 (define_insn "*addsi3_compare0_scratch"
@@ -713,7 +643,7 @@
    cmn%?\\t%0, %1"
   [(set_attr "conds" "set")
    (set_attr "predicable" "yes")
-   (set_attr "type" "alus_imm,alus_imm,alus_reg")]
+   (set_attr "type" "alus_imm,alus_imm,alus_sreg")]
 )
 
 (define_insn "*compare_negsi_si"
@@ -728,7 +658,7 @@
    (set_attr "arch" "t2,*")
    (set_attr "length" "2,4")
    (set_attr "predicable_short_it" "yes,no")
-   (set_attr "type" "alus_reg")]
+   (set_attr "type" "alus_sreg")]
 )
 
 ;; This is the canonicalization of addsi3_compare0_for_combiner when the
@@ -746,7 +676,7 @@
    add%.\\t%0, %1, %3
    sub%.\\t%0, %1, #%n3"
   [(set_attr "conds" "set")
-   (set_attr "type" "alus_reg")]
+   (set_attr "type" "alus_sreg")]
 )
 
 ;; Convert the sequence
@@ -804,7 +734,7 @@
    sub%.\\t%0, %1, #%n2
    add%.\\t%0, %1, %2"
   [(set_attr "conds" "set")
-   (set_attr "type"  "alus_imm,alus_imm,alus_reg")]
+   (set_attr "type"  "alus_imm,alus_imm,alus_sreg")]
 )
 
 (define_insn "*addsi3_compare_op2"
@@ -821,7 +751,7 @@
    add%.\\t%0, %1, %2
    sub%.\\t%0, %1, #%n2"
   [(set_attr "conds" "set")
-   (set_attr "type" "alus_imm,alus_imm,alus_reg")]
+   (set_attr "type" "alus_imm,alus_imm,alus_sreg")]
 )
 
 (define_insn "*compare_addsi2_op0"
@@ -842,7 +772,7 @@
    (set_attr "arch" "t2,t2,*,*,*")
    (set_attr "predicable_short_it" "yes,yes,no,no,no")
    (set_attr "length" "2,2,4,4,4")
-   (set_attr "type" "alus_imm,alus_reg,alus_imm,alus_imm,alus_reg")]
+   (set_attr "type" "alus_imm,alus_sreg,alus_imm,alus_imm,alus_sreg")]
 )
 
 (define_insn "*compare_addsi2_op1"
@@ -863,7 +793,7 @@
    (set_attr "arch" "t2,t2,*,*,*")
    (set_attr "predicable_short_it" "yes,yes,no,no,no")
    (set_attr "length" "2,2,4,4,4")
-   (set_attr "type" "alus_imm,alus_reg,alus_imm,alus_imm,alus_reg")]
+   (set_attr "type" "alus_imm,alus_sreg,alus_imm,alus_imm,alus_sreg")]
  )
 
 (define_insn "*addsi3_carryin_<optab>"
@@ -1095,17 +1025,6 @@
    (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb_subdi3"
-  [(set (match_operand:DI           0 "register_operand" "=l")
-	(minus:DI (match_operand:DI 1 "register_operand"  "0")
-		  (match_operand:DI 2 "register_operand"  "l")))
-   (clobber (reg:CC CC_REGNUM))]
-  "TARGET_THUMB1"
-  "sub\\t%Q0, %Q0, %Q2\;sbc\\t%R0, %R0, %R2"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
 (define_insn_and_split "*subdi_di_zesidi"
   [(set (match_operand:DI           0 "s_register_operand" "=&r,&r")
 	(minus:DI (match_operand:DI 1 "s_register_operand"  "0,r")
@@ -1261,17 +1180,6 @@
   "
 )
 
-(define_insn "thumb1_subsi3_insn"
-  [(set (match_operand:SI           0 "register_operand" "=l")
-	(minus:SI (match_operand:SI 1 "register_operand" "l")
-		  (match_operand:SI 2 "reg_or_int_operand" "lPd")))]
-  "TARGET_THUMB1"
-  "sub\\t%0, %1, %2"
-  [(set_attr "length" "2")
-   (set_attr "conds" "set")
-   (set_attr "type" "alus_reg")]
-)
-
 ; ??? Check Thumb-2 split length
 (define_insn_and_split "*arm_subsi3_insn"
   [(set (match_operand:SI           0 "s_register_operand" "=l,l ,l ,l ,r ,r,r,rk,r")
@@ -1300,7 +1208,7 @@
    (set_attr "arch" "t2,t2,t2,t2,*,*,*,*,*")
    (set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "yes,yes,yes,yes,no,no,no,no,no")
-   (set_attr "type" "alu_reg,alu_reg,alu_reg,alu_reg,alu_imm,alu_imm,alu_reg,alu_reg,multiple")]
+   (set_attr "type" "alu_sreg,alu_sreg,alu_sreg,alu_sreg,alu_imm,alu_imm,alu_sreg,alu_sreg,multiple")]
 )
 
 (define_peephole2
@@ -1330,7 +1238,7 @@
    sub%.\\t%0, %1, %2
    rsb%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "type"  "alus_imm,alus_reg,alus_reg")]
+   (set_attr "type"  "alus_imm,alus_sreg,alus_sreg")]
 )
 
 (define_insn "subsi3_compare"
@@ -1345,7 +1253,7 @@
    sub%.\\t%0, %1, %2
    rsb%.\\t%0, %2, %1"
   [(set_attr "conds" "set")
-   (set_attr "type" "alus_imm,alus_reg,alus_reg")]
+   (set_attr "type" "alus_imm,alus_sreg,alus_sreg")]
 )
 
 (define_expand "subsf3"
@@ -1413,39 +1321,6 @@
    (set_attr "predicable_short_it" "yes,yes,no")]
 )
 
-; Unfortunately with the Thumb the '&'/'0' trick can fails when operands 
-; 1 and 2; are the same, because reload will make operand 0 match 
-; operand 1 without realizing that this conflicts with operand 2.  We fix 
-; this by adding another alternative to match this case, and then `reload' 
-; it ourselves.  This alternative must come first.
-(define_insn "*thumb_mulsi3"
-  [(set (match_operand:SI          0 "register_operand" "=&l,&l,&l")
-	(mult:SI (match_operand:SI 1 "register_operand" "%l,*h,0")
-		 (match_operand:SI 2 "register_operand" "l,l,l")))]
-  "TARGET_THUMB1 && !arm_arch6"
-  "*
-  if (which_alternative < 2)
-    return \"mov\\t%0, %1\;mul\\t%0, %2\";
-  else
-    return \"mul\\t%0, %2\";
-  "
-  [(set_attr "length" "4,4,2")
-   (set_attr "type" "muls")]
-)
-
-(define_insn "*thumb_mulsi3_v6"
-  [(set (match_operand:SI          0 "register_operand" "=l,l,l")
-	(mult:SI (match_operand:SI 1 "register_operand" "0,l,0")
-		 (match_operand:SI 2 "register_operand" "l,0,0")))]
-  "TARGET_THUMB1 && arm_arch6"
-  "@
-   mul\\t%0, %2
-   mul\\t%0, %1
-   mul\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "type" "muls")]
-)
-
 (define_insn "*mulsi3_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV (mult:SI
@@ -2300,16 +2175,6 @@
    (set_attr "type" "logic_imm,logic_imm,logic_reg,logic_reg,logic_imm")]
 )
 
-(define_insn "*thumb1_andsi3_insn"
-  [(set (match_operand:SI         0 "register_operand" "=l")
-	(and:SI (match_operand:SI 1 "register_operand" "%0")
-		(match_operand:SI 2 "register_operand" "l")))]
-  "TARGET_THUMB1"
-  "and\\t%0, %2"
-  [(set_attr "length" "2")
-   (set_attr "type"  "logic_imm")
-   (set_attr "conds" "set")])
-
 (define_insn "*andsi3_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV
@@ -2498,23 +2363,6 @@
    (set_attr "type" "multiple")]
 )
 
-(define_split
-  [(set (match_operand:SI 0 "s_register_operand" "")
-	(zero_extract:SI (match_operand:SI 1 "s_register_operand" "")
-			 (match_operand:SI 2 "const_int_operand" "")
-			 (match_operand:SI 3 "const_int_operand" "")))
-   (clobber (match_operand:SI 4 "s_register_operand" ""))]
-  "TARGET_THUMB1"
-  [(set (match_dup 4) (ashift:SI (match_dup 1) (match_dup 2)))
-   (set (match_dup 0) (lshiftrt:SI (match_dup 4) (match_dup 3)))]
-  "{
-     HOST_WIDE_INT temp = INTVAL (operands[2]);
-
-     operands[2] = GEN_INT (32 - temp - INTVAL (operands[3]));
-     operands[3] = GEN_INT (32 - temp);
-   }"
-)
-
 ;; ??? Use Thumb-2 has bitfield insert/extract instructions.
 (define_split
   [(set (match_operand:SI 0 "s_register_operand" "")
@@ -2540,22 +2388,6 @@
   
 (define_split
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(sign_extract:SI (match_operand:SI 1 "s_register_operand" "")
-			 (match_operand:SI 2 "const_int_operand" "")
-			 (match_operand:SI 3 "const_int_operand" "")))]
-  "TARGET_THUMB1"
-  [(set (match_dup 0) (ashift:SI (match_dup 1) (match_dup 2)))
-   (set (match_dup 0) (ashiftrt:SI (match_dup 0) (match_dup 3)))]
-  "{
-     HOST_WIDE_INT temp = INTVAL (operands[2]);
-
-     operands[2] = GEN_INT (32 - temp - INTVAL (operands[3]));
-     operands[3] = GEN_INT (32 - temp);
-   }"
-)
-
-(define_split
-  [(set (match_operand:SI 0 "s_register_operand" "")
 	(match_operator:SI 1 "shiftable_operator"
 	 [(sign_extract:SI (match_operand:SI 2 "s_register_operand" "")
 			   (match_operand:SI 3 "const_int_operand" "")
@@ -2904,17 +2736,6 @@
    (set_attr "type" "logic_reg")]
 )
 
-(define_insn "thumb1_bicsi3"
-  [(set (match_operand:SI                 0 "register_operand" "=l")
-	(and:SI (not:SI (match_operand:SI 1 "register_operand" "l"))
-		(match_operand:SI         2 "register_operand" "0")))]
-  "TARGET_THUMB1"
-  "bic\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "conds" "set")
-   (set_attr "type" "logics_reg")]
-)
-
 (define_insn "andsi_not_shiftsi_si"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(and:SI (not:SI (match_operator:SI 4 "shift_operator"
@@ -3094,16 +2915,6 @@
    (set_attr "type" "logic_imm,logic_reg,logic_imm,logic_reg,logic_reg")]
 )
 
-(define_insn "*thumb1_iorsi3_insn"
-  [(set (match_operand:SI         0 "register_operand" "=l")
-	(ior:SI (match_operand:SI 1 "register_operand" "%0")
-		(match_operand:SI 2 "register_operand" "l")))]
-  "TARGET_THUMB1"
-  "orr\\t%0, %2"
-  [(set_attr "length" "2")
-   (set_attr "conds" "set")
-   (set_attr "type" "logics_reg")])
-
 (define_peephole2
   [(match_scratch:SI 3 "r")
    (set (match_operand:SI 0 "arm_general_register_operand" "")
@@ -3270,17 +3081,6 @@
    (set_attr "type"  "logic_imm,logic_reg,logic_reg,multiple")]
 )
 
-(define_insn "*thumb1_xorsi3_insn"
-  [(set (match_operand:SI         0 "register_operand" "=l")
-	(xor:SI (match_operand:SI 1 "register_operand" "%0")
-		(match_operand:SI 2 "register_operand" "l")))]
-  "TARGET_THUMB1"
-  "eor\\t%0, %2"
-  [(set_attr "length" "2")
-   (set_attr "conds" "set")
-   (set_attr "type" "logics_reg")]
-)
-
 (define_insn "*xorsi3_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV (xor:SI (match_operand:SI 1 "s_register_operand" "r,r")
@@ -3886,16 +3686,6 @@
   "
 )
 
-(define_insn "*thumb1_ashlsi3"
-  [(set (match_operand:SI            0 "register_operand" "=l,l")
-	(ashift:SI (match_operand:SI 1 "register_operand" "l,0")
-		   (match_operand:SI 2 "nonmemory_operand" "N,l")))]
-  "TARGET_THUMB1"
-  "lsl\\t%0, %1, %2"
-  [(set_attr "length" "2")
-   (set_attr "type" "shift_imm,shift_reg")
-   (set_attr "conds" "set")])
-
 (define_expand "ashrdi3"
   [(set (match_operand:DI              0 "s_register_operand" "")
         (ashiftrt:DI (match_operand:DI 1 "s_register_operand" "")
@@ -3967,16 +3757,6 @@
   "
 )
 
-(define_insn "*thumb1_ashrsi3"
-  [(set (match_operand:SI              0 "register_operand" "=l,l")
-	(ashiftrt:SI (match_operand:SI 1 "register_operand" "l,0")
-		     (match_operand:SI 2 "nonmemory_operand" "N,l")))]
-  "TARGET_THUMB1"
-  "asr\\t%0, %1, %2"
-  [(set_attr "length" "2")
-   (set_attr "type" "shift_imm,shift_reg")
-   (set_attr "conds" "set")])
-
 (define_expand "lshrdi3"
   [(set (match_operand:DI              0 "s_register_operand" "")
         (lshiftrt:DI (match_operand:DI 1 "s_register_operand" "")
@@ -4051,16 +3831,6 @@
   "
 )
 
-(define_insn "*thumb1_lshrsi3"
-  [(set (match_operand:SI              0 "register_operand" "=l,l")
-	(lshiftrt:SI (match_operand:SI 1 "register_operand" "l,0")
-		     (match_operand:SI 2 "nonmemory_operand" "N,l")))]
-  "TARGET_THUMB1"
-  "lsr\\t%0, %1, %2"
-  [(set_attr "length" "2")
-   (set_attr "type" "shift_imm,shift_reg")
-   (set_attr "conds" "set")])
-
 (define_expand "rotlsi3"
   [(set (match_operand:SI              0 "s_register_operand" "")
 	(rotatert:SI (match_operand:SI 1 "s_register_operand" "")
@@ -4098,16 +3868,6 @@
   "
 )
 
-(define_insn "*thumb1_rotrsi3"
-  [(set (match_operand:SI              0 "register_operand" "=l")
-	(rotatert:SI (match_operand:SI 1 "register_operand" "0")
-		     (match_operand:SI 2 "register_operand" "l")))]
-  "TARGET_THUMB1"
-  "ror\\t%0, %0, %2"
-  [(set_attr "type" "shift_reg")
-   (set_attr "length" "2")]
-)
-
 (define_insn "*arm_shiftsi3"
   [(set (match_operand:SI   0 "s_register_operand" "=l,l,r,r")
 	(match_operator:SI  3 "shift_operator"
@@ -4575,16 +4335,6 @@
    (set_attr "type" "multiple")]
 )
 
-(define_insn "*thumb1_negdi2"
-  [(set (match_operand:DI 0 "register_operand" "=&l")
-	(neg:DI (match_operand:DI 1 "register_operand" "l")))
-   (clobber (reg:CC CC_REGNUM))]
-  "TARGET_THUMB1"
-  "mov\\t%R0, #0\;neg\\t%Q0, %Q1\;sbc\\t%R0, %R1"
-  [(set_attr "length" "6")
-   (set_attr "type" "multiple")]
-)
-
 (define_expand "negsi2"
   [(set (match_operand:SI         0 "s_register_operand" "")
 	(neg:SI (match_operand:SI 1 "s_register_operand" "")))]
@@ -4601,16 +4351,7 @@
    (set_attr "predicable_short_it" "yes,no")
    (set_attr "arch" "t2,*")
    (set_attr "length" "4")
-   (set_attr "type" "alu_reg")]
-)
-
-(define_insn "*thumb1_negsi2"
-  [(set (match_operand:SI         0 "register_operand" "=l")
-	(neg:SI (match_operand:SI 1 "register_operand" "l")))]
-  "TARGET_THUMB1"
-  "neg\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "type" "alu_imm")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_expand "negsf2"
@@ -4826,21 +4567,6 @@
    (set_attr "type" "multiple")]
 )
 
-(define_insn_and_split "*thumb1_abssi2"
-  [(set (match_operand:SI 0 "s_register_operand" "=l")
-	(abs:SI (match_operand:SI 1 "s_register_operand" "l")))
-   (clobber (match_scratch:SI 2 "=&l"))]
-  "TARGET_THUMB1"
-  "#"
-  "TARGET_THUMB1 && reload_completed"
-  [(set (match_dup 2) (ashiftrt:SI (match_dup 1) (const_int 31)))
-   (set (match_dup 0) (plus:SI (match_dup 1) (match_dup 2)))
-   (set (match_dup 0) (xor:SI (match_dup 0) (match_dup 2)))]
-  ""
-  [(set_attr "length" "6")
-   (set_attr "type" "multiple")]
-)
-
 (define_insn_and_split "*arm_neg_abssi2"
   [(set (match_operand:SI 0 "s_register_operand" "=r,&r")
 	(neg:SI (abs:SI (match_operand:SI 1 "s_register_operand" "0,r"))))
@@ -4898,21 +4624,6 @@
    (set_attr "type" "multiple")]
 )
 
-(define_insn_and_split "*thumb1_neg_abssi2"
-  [(set (match_operand:SI 0 "s_register_operand" "=l")
-	(neg:SI (abs:SI (match_operand:SI 1 "s_register_operand" "l"))))
-   (clobber (match_scratch:SI 2 "=&l"))]
-  "TARGET_THUMB1"
-  "#"
-  "TARGET_THUMB1 && reload_completed"
-  [(set (match_dup 2) (ashiftrt:SI (match_dup 1) (const_int 31)))
-   (set (match_dup 0) (minus:SI (match_dup 2) (match_dup 1)))
-   (set (match_dup 0) (xor:SI (match_dup 0) (match_dup 2)))]
-  ""
-  [(set_attr "length" "6")
-   (set_attr "type" "multiple")]
-)
-
 (define_expand "abssf2"
   [(set (match_operand:SF         0 "s_register_operand" "")
 	(abs:SF (match_operand:SF 1 "s_register_operand" "")))]
@@ -4982,15 +4693,6 @@
    (set_attr "type" "mvn_reg")]
 )
 
-(define_insn "*thumb1_one_cmplsi2"
-  [(set (match_operand:SI         0 "register_operand" "=l")
-	(not:SI (match_operand:SI 1 "register_operand"  "l")))]
-  "TARGET_THUMB1"
-  "mvn\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "type" "mvn_reg")]
-)
-
 (define_insn "*notsi_compare0"
   [(set (reg:CC_NOOV CC_REGNUM)
 	(compare:CC_NOOV (not:SI (match_operand:SI 1 "s_register_operand" "r"))
@@ -5234,49 +4936,6 @@
   operands[2] = gen_lowpart (SImode, operands[1]);
 })
 
-(define_insn "*thumb1_zero_extendhisi2"
-  [(set (match_operand:SI 0 "register_operand" "=l,l")
-	(zero_extend:SI (match_operand:HI 1 "nonimmediate_operand" "l,m")))]
-  "TARGET_THUMB1"
-{
-  rtx mem;
-
-  if (which_alternative == 0 && arm_arch6)
-    return "uxth\t%0, %1";
-  if (which_alternative == 0)
-    return "#";
-
-  mem = XEXP (operands[1], 0);
-
-  if (GET_CODE (mem) == CONST)
-    mem = XEXP (mem, 0);
-    
-  if (GET_CODE (mem) == PLUS)
-    {
-      rtx a = XEXP (mem, 0);
-
-      /* This can happen due to bugs in reload.  */
-      if (REG_P (a) && REGNO (a) == SP_REGNUM)
-        {
-          rtx ops[2];
-          ops[0] = operands[0];
-          ops[1] = a;
-      
-          output_asm_insn ("mov\t%0, %1", ops);
-
-          XEXP (mem, 0) = operands[0];
-       }
-    }
-    
-  return "ldrh\t%0, %1";
-}
-  [(set_attr_alternative "length"
-			 [(if_then_else (eq_attr "is_arch6" "yes")
-				       (const_int 2) (const_int 4))
-			 (const_int 4)])
-   (set_attr "type" "extend,load_byte")]
-)
-
 (define_insn "*arm_zero_extendhisi2"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
 	(zero_extend:SI (match_operand:HI 1 "nonimmediate_operand" "r,m")))]
@@ -5347,29 +5006,6 @@
     }
 })
 
-(define_insn "*thumb1_zero_extendqisi2"
-  [(set (match_operand:SI 0 "register_operand" "=l,l")
-	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "l,m")))]
-  "TARGET_THUMB1 && !arm_arch6"
-  "@
-   #
-   ldrb\\t%0, %1"
-  [(set_attr "length" "4,2")
-   (set_attr "type" "alu_shift_reg,load_byte")
-   (set_attr "pool_range" "*,32")]
-)
-
-(define_insn "*thumb1_zero_extendqisi2_v6"
-  [(set (match_operand:SI 0 "register_operand" "=l,l")
-	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "l,m")))]
-  "TARGET_THUMB1 && arm_arch6"
-  "@
-   uxtb\\t%0, %1
-   ldrb\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "type" "extend,load_byte")]
-)
-
 (define_insn "*arm_zero_extendqisi2"
   [(set (match_operand:SI 0 "s_register_operand" "=r,r")
 	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "r,m")))]
@@ -5495,80 +5131,6 @@
   operands[2] = simplify_gen_subreg (SImode, operands[1], HImode, 0);
 })
 
-;; We used to have an early-clobber on the scratch register here.
-;; However, there's a bug somewhere in reload which means that this
-;; can be partially ignored during spill allocation if the memory
-;; address also needs reloading; this causes us to die later on when
-;; we try to verify the operands.  Fortunately, we don't really need
-;; the early-clobber: we can always use operand 0 if operand 2
-;; overlaps the address.
-(define_insn "thumb1_extendhisi2"
-  [(set (match_operand:SI 0 "register_operand" "=l,l")
-	(sign_extend:SI (match_operand:HI 1 "nonimmediate_operand" "l,m")))
-   (clobber (match_scratch:SI 2 "=X,l"))]
-  "TARGET_THUMB1"
-  "*
-  {
-    rtx ops[4];
-    rtx mem;
-
-    if (which_alternative == 0 && !arm_arch6)
-      return \"#\";
-    if (which_alternative == 0)
-      return \"sxth\\t%0, %1\";
-
-    mem = XEXP (operands[1], 0);
-
-    /* This code used to try to use 'V', and fix the address only if it was
-       offsettable, but this fails for e.g. REG+48 because 48 is outside the
-       range of QImode offsets, and offsettable_address_p does a QImode
-       address check.  */
-       
-    if (GET_CODE (mem) == CONST)
-      mem = XEXP (mem, 0);
-    
-    if (GET_CODE (mem) == LABEL_REF)
-      return \"ldr\\t%0, %1\";
-    
-    if (GET_CODE (mem) == PLUS)
-      {
-        rtx a = XEXP (mem, 0);
-        rtx b = XEXP (mem, 1);
-
-        if (GET_CODE (a) == LABEL_REF
-	    && CONST_INT_P (b))
-          return \"ldr\\t%0, %1\";
-
-        if (REG_P (b))
-          return \"ldrsh\\t%0, %1\";
-	  
-        ops[1] = a;
-        ops[2] = b;
-      }
-    else
-      {
-        ops[1] = mem;
-        ops[2] = const0_rtx;
-      }
-      
-    gcc_assert (REG_P (ops[1]));
-
-    ops[0] = operands[0];
-    if (reg_mentioned_p (operands[2], ops[1]))
-      ops[3] = ops[0];
-    else
-      ops[3] = operands[2];
-    output_asm_insn (\"mov\\t%3, %2\;ldrsh\\t%0, [%1, %3]\", ops);
-    return \"\";
-  }"
-  [(set_attr_alternative "length"
-			 [(if_then_else (eq_attr "is_arch6" "yes")
-					(const_int 2) (const_int 4))
-			  (const_int 4)])
-   (set_attr "type" "extend,load_byte")
-   (set_attr "pool_range" "*,1018")]
-)
-
 ;; This pattern will only be used when ldsh is not available
 (define_expand "extendhisi2_mem"
   [(set (match_dup 2) (zero_extend:SI (match_operand:HI 1 "" "")))
@@ -5747,105 +5309,6 @@
    (set_attr "predicable_short_it" "no")]
 )
 
-(define_split
-  [(set (match_operand:SI 0 "register_operand" "")
-	(sign_extend:SI (match_operand:QI 1 "memory_operand" "")))]
-  "TARGET_THUMB1 && reload_completed"
-  [(set (match_dup 0) (match_dup 2))
-   (set (match_dup 0) (sign_extend:SI (match_dup 3)))]
-{
-  rtx addr = XEXP (operands[1], 0);
-
-  if (GET_CODE (addr) == CONST)
-    addr = XEXP (addr, 0);
-
-  if (GET_CODE (addr) == PLUS
-      && REG_P (XEXP (addr, 0)) && REG_P (XEXP (addr, 1)))
-    /* No split necessary.  */
-    FAIL;
-
-  if (GET_CODE (addr) == PLUS
-      && !REG_P (XEXP (addr, 0)) && !REG_P (XEXP (addr, 1)))
-    FAIL;
-
-  if (reg_overlap_mentioned_p (operands[0], addr))
-    {
-      rtx t = gen_lowpart (QImode, operands[0]);
-      emit_move_insn (t, operands[1]);
-      emit_insn (gen_thumb1_extendqisi2 (operands[0], t));
-      DONE;
-    }
-
-  if (REG_P (addr))
-    {
-      addr = gen_rtx_PLUS (Pmode, addr, operands[0]);
-      operands[2] = const0_rtx;
-    }
-  else if (GET_CODE (addr) != PLUS)
-    FAIL;
-  else if (REG_P (XEXP (addr, 0)))
-    {
-      operands[2] = XEXP (addr, 1);
-      addr = gen_rtx_PLUS (Pmode, XEXP (addr, 0), operands[0]);
-    }
-  else
-    {
-      operands[2] = XEXP (addr, 0);
-      addr = gen_rtx_PLUS (Pmode, XEXP (addr, 1), operands[0]);
-    }
-
-  operands[3] = change_address (operands[1], QImode, addr);
-})
-
-(define_peephole2
-  [(set (match_operand:SI 0 "register_operand" "")
-	(plus:SI (match_dup 0) (match_operand 1 "const_int_operand")))
-   (set (match_operand:SI 2 "register_operand" "") (const_int 0))
-   (set (match_operand:SI 3 "register_operand" "")
-	(sign_extend:SI (match_operand:QI 4 "memory_operand" "")))]
-  "TARGET_THUMB1
-   && GET_CODE (XEXP (operands[4], 0)) == PLUS
-   && rtx_equal_p (operands[0], XEXP (XEXP (operands[4], 0), 0))
-   && rtx_equal_p (operands[2], XEXP (XEXP (operands[4], 0), 1))
-   && (peep2_reg_dead_p (3, operands[0])
-       || rtx_equal_p (operands[0], operands[3]))
-   && (peep2_reg_dead_p (3, operands[2])
-       || rtx_equal_p (operands[2], operands[3]))"
-  [(set (match_dup 2) (match_dup 1))
-   (set (match_dup 3) (sign_extend:SI (match_dup 4)))]
-{
-  rtx addr = gen_rtx_PLUS (Pmode, operands[0], operands[2]);
-  operands[4] = change_address (operands[4], QImode, addr);
-})
-
-(define_insn "thumb1_extendqisi2"
-  [(set (match_operand:SI 0 "register_operand" "=l,l,l")
-	(sign_extend:SI (match_operand:QI 1 "nonimmediate_operand" "l,V,m")))]
-  "TARGET_THUMB1"
-{
-  rtx addr;
-
-  if (which_alternative == 0 && arm_arch6)
-    return "sxtb\\t%0, %1";
-  if (which_alternative == 0)
-    return "#";
-
-  addr = XEXP (operands[1], 0);
-  if (GET_CODE (addr) == PLUS
-      && REG_P (XEXP (addr, 0)) && REG_P (XEXP (addr, 1)))
-    return "ldrsb\\t%0, %1";
-      
-  return "#";
-}
-  [(set_attr_alternative "length"
-			 [(if_then_else (eq_attr "is_arch6" "yes")
-					(const_int 2) (const_int 4))
-			  (const_int 2)
-			  (if_then_else (eq_attr "is_arch6" "yes")
-					(const_int 4) (const_int 6))])
-   (set_attr "type" "extend,load_byte,load_byte")]
-)
-
 (define_expand "extendsfdf2"
   [(set (match_operand:DF                  0 "s_register_operand" "")
 	(float_extend:DF (match_operand:SF 1 "s_register_operand"  "")))]
@@ -6060,52 +5523,6 @@
   "
 )
 
-;;; ??? This should have alternatives for constants.
-;;; ??? This was originally identical to the movdf_insn pattern.
-;;; ??? The 'i' constraint looks funny, but it should always be replaced by
-;;; thumb_reorg with a memory reference.
-(define_insn "*thumb1_movdi_insn"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=l,l,l,l,>,l, m,*r")
-	(match_operand:DI 1 "general_operand"      "l, I,J,>,l,mi,l,*r"))]
-  "TARGET_THUMB1
-   && (   register_operand (operands[0], DImode)
-       || register_operand (operands[1], DImode))"
-  "*
-  {
-  switch (which_alternative)
-    {
-    default:
-    case 0:
-      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
-	return \"add\\t%0,  %1,  #0\;add\\t%H0, %H1, #0\";
-      return   \"add\\t%H0, %H1, #0\;add\\t%0,  %1,  #0\";
-    case 1:
-      return \"mov\\t%Q0, %1\;mov\\t%R0, #0\";
-    case 2:
-      operands[1] = GEN_INT (- INTVAL (operands[1]));
-      return \"mov\\t%Q0, %1\;neg\\t%Q0, %Q0\;asr\\t%R0, %Q0, #31\";
-    case 3:
-      return \"ldmia\\t%1, {%0, %H0}\";
-    case 4:
-      return \"stmia\\t%0, {%1, %H1}\";
-    case 5:
-      return thumb_load_double_from_address (operands);
-    case 6:
-      operands[2] = gen_rtx_MEM (SImode,
-			     plus_constant (Pmode, XEXP (operands[0], 0), 4));
-      output_asm_insn (\"str\\t%1, %0\;str\\t%H1, %2\", operands);
-      return \"\";
-    case 7:
-      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
-	return \"mov\\t%0, %1\;mov\\t%H0, %H1\";
-      return \"mov\\t%H0, %H1\;mov\\t%0, %1\";
-    }
-  }"
-  [(set_attr "length" "4,4,6,2,2,6,4,4")
-   (set_attr "type" "multiple,multiple,multiple,load2,store2,load2,store2,multiple")
-   (set_attr "pool_range" "*,*,*,*,*,1018,*,*")]
-)
-
 (define_expand "movsi"
   [(set (match_operand:SI 0 "general_operand" "")
         (match_operand:SI 1 "general_operand" ""))]
@@ -6297,100 +5714,25 @@
   DONE;
 })
 
-(define_insn "*thumb1_movsi_insn"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=l,l,l,l,l,>,l, m,*l*h*k")
-	(match_operand:SI 1 "general_operand"      "l, I,J,K,>,l,mi,l,*l*h*k"))]
-  "TARGET_THUMB1
-   && (   register_operand (operands[0], SImode) 
-       || register_operand (operands[1], SImode))"
-  "@
-   mov	%0, %1
-   mov	%0, %1
-   #
-   #
-   ldmia\\t%1, {%0}
-   stmia\\t%0, {%1}
-   ldr\\t%0, %1
-   str\\t%1, %0
-   mov\\t%0, %1"
-  [(set_attr "length" "2,2,4,4,2,2,2,2,2")
-   (set_attr "type" "mov_reg,mov_imm,multiple,multiple,load1,store1,load1,store1,mov_reg")
-   (set_attr "pool_range" "*,*,*,*,*,*,1018,*,*")
-   (set_attr "conds" "set,clob,*,*,nocond,nocond,nocond,nocond,nocond")])
-
-(define_split 
+;; When generating pic, we need to load the symbol offset into a register.
+;; So that the optimizer does not confuse this with a normal symbol load
+;; we use an unspec.  The offset will be loaded from a constant pool entry,
+;; since that is the only type of relocation we can use.
+
+;; Wrap calculation of the whole PIC address in a single pattern for the
+;; benefit of optimizers, particularly, PRE and HOIST.  Calculation of
+;; a PIC address involves two loads from memory, so we want to CSE it
+;; as often as possible.
+;; This pattern will be split into one of the pic_load_addr_* patterns
+;; and a move after GCSE optimizations.
+;;
+;; Note: Update arm.c: legitimize_pic_address() when changing this pattern.
+(define_expand "calculate_pic_address"
   [(set (match_operand:SI 0 "register_operand" "")
-	(match_operand:SI 1 "const_int_operand" ""))]
-  "TARGET_THUMB1 && satisfies_constraint_J (operands[1])"
-  [(set (match_dup 2) (match_dup 1))
-   (set (match_dup 0) (neg:SI (match_dup 2)))]
-  "
-  {
-    operands[1] = GEN_INT (- INTVAL (operands[1]));
-    operands[2] = can_create_pseudo_p () ? gen_reg_rtx (SImode) : operands[0];
-  }"
-)
-
-(define_split 
-  [(set (match_operand:SI 0 "register_operand" "")
-	(match_operand:SI 1 "const_int_operand" ""))]
-  "TARGET_THUMB1 && satisfies_constraint_K (operands[1])"
-  [(set (match_dup 2) (match_dup 1))
-   (set (match_dup 0) (ashift:SI (match_dup 2) (match_dup 3)))]
-  "
-  {
-    unsigned HOST_WIDE_INT val = INTVAL (operands[1]) & 0xffffffffu;
-    unsigned HOST_WIDE_INT mask = 0xff;
-    int i;
-    
-    for (i = 0; i < 25; i++)
-      if ((val & (mask << i)) == val)
-        break;
-
-    /* Don't split if the shift is zero.  */
-    if (i == 0)
-      FAIL;
-
-    operands[1] = GEN_INT (val >> i);
-    operands[2] = can_create_pseudo_p () ? gen_reg_rtx (SImode) : operands[0];
-    operands[3] = GEN_INT (i);
-  }"
-)
-
-;; For thumb1 split imm move [256-510] into mov [1-255] and add #255
-(define_split 
-  [(set (match_operand:SI 0 "register_operand" "")
-	(match_operand:SI 1 "const_int_operand" ""))]
-  "TARGET_THUMB1 && satisfies_constraint_Pe (operands[1])"
-  [(set (match_dup 2) (match_dup 1))
-   (set (match_dup 0) (plus:SI (match_dup 2) (match_dup 3)))]
-  "
-  {
-    operands[1] = GEN_INT (INTVAL (operands[1]) - 255);
-    operands[2] = can_create_pseudo_p () ? gen_reg_rtx (SImode) : operands[0];
-    operands[3] = GEN_INT (255);
-  }"
-)
-
-;; When generating pic, we need to load the symbol offset into a register.
-;; So that the optimizer does not confuse this with a normal symbol load
-;; we use an unspec.  The offset will be loaded from a constant pool entry,
-;; since that is the only type of relocation we can use.
-
-;; Wrap calculation of the whole PIC address in a single pattern for the
-;; benefit of optimizers, particularly, PRE and HOIST.  Calculation of
-;; a PIC address involves two loads from memory, so we want to CSE it
-;; as often as possible.
-;; This pattern will be split into one of the pic_load_addr_* patterns
-;; and a move after GCSE optimizations.
-;;
-;; Note: Update arm.c: legitimize_pic_address() when changing this pattern.
-(define_expand "calculate_pic_address"
-  [(set (match_operand:SI 0 "register_operand" "")
-	(mem:SI (plus:SI (match_operand:SI 1 "register_operand" "")
-			 (unspec:SI [(match_operand:SI 2 "" "")]
-				    UNSPEC_PIC_SYM))))]
-  "flag_pic"
+	(mem:SI (plus:SI (match_operand:SI 1 "register_operand" "")
+			 (unspec:SI [(match_operand:SI 2 "" "")]
+				    UNSPEC_PIC_SYM))))]
+  "flag_pic"
 )
 
 ;; Split calculate_pic_address into pic_load_addr_* and a move.
@@ -6471,7 +5813,7 @@
   return \"add\\t%0, %|pc\";
   "
   [(set_attr "length" "2")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "pic_add_dot_plus_eight"
@@ -6487,7 +5829,7 @@
     return \"add%?\\t%0, %|pc, %1\";
   "
   [(set_attr "predicable" "yes")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "tls_load_dot_plus_eight"
@@ -6876,44 +6218,6 @@
   "
 )
 
-(define_insn "*thumb1_movhi_insn"
-  [(set (match_operand:HI 0 "nonimmediate_operand" "=l,l,m,*r,*h,l")
-	(match_operand:HI 1 "general_operand"       "l,m,l,*h,*r,I"))]
-  "TARGET_THUMB1
-   && (   register_operand (operands[0], HImode)
-       || register_operand (operands[1], HImode))"
-  "*
-  switch (which_alternative)
-    {
-    case 0: return \"add	%0, %1, #0\";
-    case 2: return \"strh	%1, %0\";
-    case 3: return \"mov	%0, %1\";
-    case 4: return \"mov	%0, %1\";
-    case 5: return \"mov	%0, %1\";
-    default: gcc_unreachable ();
-    case 1:
-      /* The stack pointer can end up being taken as an index register.
-          Catch this case here and deal with it.  */
-      if (GET_CODE (XEXP (operands[1], 0)) == PLUS
-	  && REG_P (XEXP (XEXP (operands[1], 0), 0))
-	  && REGNO    (XEXP (XEXP (operands[1], 0), 0)) == SP_REGNUM)
-        {
-	  rtx ops[2];
-          ops[0] = operands[0];
-          ops[1] = XEXP (XEXP (operands[1], 0), 0);
-      
-          output_asm_insn (\"mov	%0, %1\", ops);
-
-          XEXP (XEXP (operands[1], 0), 0) = operands[0];
-    
-	}
-      return \"ldrh	%0, %1\";
-    }"
-  [(set_attr "length" "2,4,2,2,2,2")
-   (set_attr "type" "alus_imm,load1,store1,mov_reg,mov_reg,mov_imm")
-   (set_attr "conds" "clob,nocond,nocond,nocond,nocond,clob")])
-
-
 (define_expand "movhi_bytes"
   [(set (match_dup 2) (zero_extend:SI (match_operand:HI 1 "" "")))
    (set (match_dup 3)
@@ -7003,23 +6307,6 @@
   [(set_attr "predicable" "yes")
    (set_attr "type" "mov_imm,mov_reg,mvn_imm")]
 )
-
-(define_expand "thumb_movhi_clobber"
-  [(set (match_operand:HI     0 "memory_operand"   "")
-	(match_operand:HI     1 "register_operand" ""))
-   (clobber (match_operand:DI 2 "register_operand" ""))]
-  "TARGET_THUMB1"
-  "
-  if (strict_memory_address_p (HImode, XEXP (operands[0], 0))
-      && REGNO (operands[1]) <= LAST_LO_REGNUM)
-    {
-      emit_insn (gen_movhi (operands[0], operands[1]));
-      DONE;
-    }
-  /* XXX Fixme, need to handle other cases here as well.  */
-  gcc_unreachable ();
-  "
-)
 	
 ;; We use a DImode scratch because we may occasionally need an additional
 ;; temporary if the address isn't offsettable -- push_reload doesn't seem
@@ -7147,24 +6434,6 @@
    (set_attr "length" "2,4,4,2,4,2,2,4,4")]
 )
 
-(define_insn "*thumb1_movqi_insn"
-  [(set (match_operand:QI 0 "nonimmediate_operand" "=l,l,m,*r,*h,l")
-	(match_operand:QI 1 "general_operand"      "l, m,l,*h,*r,I"))]
-  "TARGET_THUMB1
-   && (   register_operand (operands[0], QImode)
-       || register_operand (operands[1], QImode))"
-  "@
-   add\\t%0, %1, #0
-   ldrb\\t%0, %1
-   strb\\t%1, %0
-   mov\\t%0, %1
-   mov\\t%0, %1
-   mov\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "type" "alu_imm,load1,store1,mov_reg,mov_imm,mov_imm")
-   (set_attr "pool_range" "*,32,*,*,*,*")
-   (set_attr "conds" "clob,nocond,nocond,nocond,nocond,clob")])
-
 ;; HFmode moves
 (define_expand "movhf"
   [(set (match_operand:HF 0 "general_operand" "")
@@ -7231,40 +6500,6 @@
    (set_attr "predicable" "yes")]
 )
 
-(define_insn "*thumb1_movhf"
-  [(set (match_operand:HF     0 "nonimmediate_operand" "=l,l,m,*r,*h")
-	(match_operand:HF     1 "general_operand"      "l,mF,l,*h,*r"))]
-  "TARGET_THUMB1
-   && (	  s_register_operand (operands[0], HFmode) 
-       || s_register_operand (operands[1], HFmode))"
-  "*
-  switch (which_alternative)
-    {
-    case 1:
-      {
-	rtx addr;
-	gcc_assert (MEM_P (operands[1]));
-	addr = XEXP (operands[1], 0);
-	if (GET_CODE (addr) == LABEL_REF
-	    || (GET_CODE (addr) == CONST
-		&& GET_CODE (XEXP (addr, 0)) == PLUS
-		&& GET_CODE (XEXP (XEXP (addr, 0), 0)) == LABEL_REF
-		&& CONST_INT_P (XEXP (XEXP (addr, 0), 1))))
-	  {
-	    /* Constant pool entry.  */
-	    return \"ldr\\t%0, %1\";
-	  }
-	return \"ldrh\\t%0, %1\";
-      }
-    case 2: return \"strh\\t%1, %0\";
-    default: return \"mov\\t%0, %1\";
-    }
-  "
-  [(set_attr "length" "2")
-   (set_attr "type" "mov_reg,load1,store1,mov_reg,mov_reg")
-   (set_attr "pool_range" "*,1018,*,*,*")
-   (set_attr "conds" "clob,nocond,nocond,nocond,nocond")])
-
 (define_expand "movsf"
   [(set (match_operand:SF 0 "general_operand" "")
 	(match_operand:SF 1 "general_operand" ""))]
@@ -7323,27 +6558,6 @@
    (set_attr "thumb2_neg_pool_range" "*,0,*")]
 )
 
-;;; ??? This should have alternatives for constants.
-(define_insn "*thumb1_movsf_insn"
-  [(set (match_operand:SF     0 "nonimmediate_operand" "=l,l,>,l, m,*r,*h")
-	(match_operand:SF     1 "general_operand"      "l, >,l,mF,l,*h,*r"))]
-  "TARGET_THUMB1
-   && (   register_operand (operands[0], SFmode) 
-       || register_operand (operands[1], SFmode))"
-  "@
-   add\\t%0, %1, #0
-   ldmia\\t%1, {%0}
-   stmia\\t%0, {%1}
-   ldr\\t%0, %1
-   str\\t%1, %0
-   mov\\t%0, %1
-   mov\\t%0, %1"
-  [(set_attr "length" "2")
-   (set_attr "type" "alus_imm,load1,store1,load1,store1,mov_reg,mov_reg")
-   (set_attr "pool_range" "*,*,*,1018,*,*,*")
-   (set_attr "conds" "clob,nocond,nocond,nocond,nocond,nocond,nocond")]
-)
-
 (define_expand "movdf"
   [(set (match_operand:DF 0 "general_operand" "")
 	(match_operand:DF 1 "general_operand" ""))]
@@ -7433,47 +6647,6 @@
    (set_attr "arm_neg_pool_range" "*,*,*,1004,*")
    (set_attr "thumb2_neg_pool_range" "*,*,*,0,*")]
 )
-
-;;; ??? This should have alternatives for constants.
-;;; ??? This was originally identical to the movdi_insn pattern.
-;;; ??? The 'F' constraint looks funny, but it should always be replaced by
-;;; thumb_reorg with a memory reference.
-(define_insn "*thumb_movdf_insn"
-  [(set (match_operand:DF 0 "nonimmediate_operand" "=l,l,>,l, m,*r")
-	(match_operand:DF 1 "general_operand"      "l, >,l,mF,l,*r"))]
-  "TARGET_THUMB1
-   && (   register_operand (operands[0], DFmode)
-       || register_operand (operands[1], DFmode))"
-  "*
-  switch (which_alternative)
-    {
-    default:
-    case 0:
-      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
-	return \"add\\t%0, %1, #0\;add\\t%H0, %H1, #0\";
-      return \"add\\t%H0, %H1, #0\;add\\t%0, %1, #0\";
-    case 1:
-      return \"ldmia\\t%1, {%0, %H0}\";
-    case 2:
-      return \"stmia\\t%0, {%1, %H1}\";
-    case 3:
-      return thumb_load_double_from_address (operands);
-    case 4:
-      operands[2] = gen_rtx_MEM (SImode,
-				 plus_constant (Pmode,
-						XEXP (operands[0], 0), 4));
-      output_asm_insn (\"str\\t%1, %0\;str\\t%H1, %2\", operands);
-      return \"\";
-    case 5:
-      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
-	return \"mov\\t%0, %1\;mov\\t%H0, %H1\";
-      return \"mov\\t%H0, %H1\;mov\\t%0, %1\";
-    }
-  "
-  [(set_attr "length" "4,2,2,6,4,4")
-   (set_attr "type" "multiple,load2,store2,load2,store2,multiple")
-   (set_attr "pool_range" "*,*,*,1018,*,*")]
-)
 
 
 ;; load- and store-multiple insns
@@ -7567,50 +6740,6 @@
     }
   "
 )
-
-;; Thumb block-move insns
-
-(define_insn "movmem12b"
-  [(set (mem:SI (match_operand:SI 2 "register_operand" "0"))
-	(mem:SI (match_operand:SI 3 "register_operand" "1")))
-   (set (mem:SI (plus:SI (match_dup 2) (const_int 4)))
-	(mem:SI (plus:SI (match_dup 3) (const_int 4))))
-   (set (mem:SI (plus:SI (match_dup 2) (const_int 8)))
-	(mem:SI (plus:SI (match_dup 3) (const_int 8))))
-   (set (match_operand:SI 0 "register_operand" "=l")
-	(plus:SI (match_dup 2) (const_int 12)))
-   (set (match_operand:SI 1 "register_operand" "=l")
-	(plus:SI (match_dup 3) (const_int 12)))
-   (clobber (match_scratch:SI 4 "=&l"))
-   (clobber (match_scratch:SI 5 "=&l"))
-   (clobber (match_scratch:SI 6 "=&l"))]
-  "TARGET_THUMB1"
-  "* return thumb_output_move_mem_multiple (3, operands);"
-  [(set_attr "length" "4")
-   ; This isn't entirely accurate...  It loads as well, but in terms of
-   ; scheduling the following insn it is better to consider it as a store
-   (set_attr "type" "store3")]
-)
-
-(define_insn "movmem8b"
-  [(set (mem:SI (match_operand:SI 2 "register_operand" "0"))
-	(mem:SI (match_operand:SI 3 "register_operand" "1")))
-   (set (mem:SI (plus:SI (match_dup 2) (const_int 4)))
-	(mem:SI (plus:SI (match_dup 3) (const_int 4))))
-   (set (match_operand:SI 0 "register_operand" "=l")
-	(plus:SI (match_dup 2) (const_int 8)))
-   (set (match_operand:SI 1 "register_operand" "=l")
-	(plus:SI (match_dup 3) (const_int 8)))
-   (clobber (match_scratch:SI 4 "=&l"))
-   (clobber (match_scratch:SI 5 "=&l"))]
-  "TARGET_THUMB1"
-  "* return thumb_output_move_mem_multiple (2, operands);"
-  [(set_attr "length" "4")
-   ; This isn't entirely accurate...  It loads as well, but in terms of
-   ; scheduling the following insn it is better to consider it as a store
-   (set_attr "type" "store2")]
-)
-
 
 
 ;; Compare & branch insns
@@ -7657,30 +6786,6 @@
     operands[2] = force_reg (SImode, operands[2]);
   ")
 
-;; A pattern to recognize a special situation and optimize for it.
-;; On the thumb, zero-extension from memory is preferrable to sign-extension
-;; due to the available addressing modes.  Hence, convert a signed comparison
-;; with zero into an unsigned comparison with 127 if possible.
-(define_expand "cbranchqi4"
-  [(set (pc) (if_then_else
-	      (match_operator 0 "lt_ge_comparison_operator"
-	       [(match_operand:QI 1 "memory_operand" "")
-	        (match_operand:QI 2 "const0_operand" "")])
-	      (label_ref (match_operand 3 "" ""))
-	      (pc)))]
-  "TARGET_THUMB1"
-{
-  rtx xops[4];
-  xops[1] = gen_reg_rtx (SImode);
-  emit_insn (gen_zero_extendqisi2 (xops[1], operands[1]));
-  xops[2] = GEN_INT (127);
-  xops[0] = gen_rtx_fmt_ee (GET_CODE (operands[0]) == GE ? LEU : GTU,
-			    VOIDmode, xops[1], xops[2]);
-  xops[3] = operands[3];
-  emit_insn (gen_cbranchsi4 (xops[0], xops[1], xops[2], xops[3]));
-  DONE;
-})
-
 (define_expand "cbranchsf4"
   [(set (pc) (if_then_else
 	      (match_operator 0 "expandable_comparison_operator"
@@ -7722,524 +6827,6 @@
    }"
 )
 
-(define_insn "cbranchsi4_insn"
-  [(set (pc) (if_then_else
-	      (match_operator 0 "arm_comparison_operator"
-	       [(match_operand:SI 1 "s_register_operand" "l,l*h")
-	        (match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r")])
-	      (label_ref (match_operand 3 "" ""))
-	      (pc)))]
-  "TARGET_THUMB1"
-{
-  rtx t = cfun->machine->thumb1_cc_insn;
-  if (t != NULL_RTX)
-    {
-      if (!rtx_equal_p (cfun->machine->thumb1_cc_op0, operands[1])
-	  || !rtx_equal_p (cfun->machine->thumb1_cc_op1, operands[2]))
-	t = NULL_RTX;
-      if (cfun->machine->thumb1_cc_mode == CC_NOOVmode)
-	{
-	  if (!noov_comparison_operator (operands[0], VOIDmode))
-	    t = NULL_RTX;
-	}
-      else if (cfun->machine->thumb1_cc_mode != CCmode)
-	t = NULL_RTX;
-    }
-  if (t == NULL_RTX)
-    {
-      output_asm_insn ("cmp\t%1, %2", operands);
-      cfun->machine->thumb1_cc_insn = insn;
-      cfun->machine->thumb1_cc_op0 = operands[1];
-      cfun->machine->thumb1_cc_op1 = operands[2];
-      cfun->machine->thumb1_cc_mode = CCmode;
-    }
-  else
-    /* Ensure we emit the right type of condition code on the jump.  */
-    XEXP (operands[0], 0) = gen_rtx_REG (cfun->machine->thumb1_cc_mode,
-					 CC_REGNUM);
-
-  switch (get_attr_length (insn))
-    {
-    case 4:  return \"b%d0\\t%l3\";
-    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
-    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
-    }
-}
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
-	         (le (minus (match_dup 3) (pc)) (const_int 256)))
-	    (const_int 4)
-	    (if_then_else
-	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
-		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
-		(const_int 6)
-		(const_int 8))))
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "cbranchsi4_scratch"
-  [(set (pc) (if_then_else
-	      (match_operator 4 "arm_comparison_operator"
-	       [(match_operand:SI 1 "s_register_operand" "l,0")
-	        (match_operand:SI 2 "thumb1_cmpneg_operand" "L,J")])
-	      (label_ref (match_operand 3 "" ""))
-	      (pc)))
-   (clobber (match_scratch:SI 0 "=l,l"))]
-  "TARGET_THUMB1"
-  "*
-  output_asm_insn (\"add\\t%0, %1, #%n2\", operands);
-
-  switch (get_attr_length (insn))
-    {
-    case 4:  return \"b%d4\\t%l3\";
-    case 6:  return \"b%D4\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
-    default: return \"b%D4\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
-    }
-  "
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
-	         (le (minus (match_dup 3) (pc)) (const_int 256)))
-	    (const_int 4)
-	    (if_then_else
-	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
-		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
-		(const_int 6)
-		(const_int 8))))
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "*negated_cbranchsi4"
-  [(set (pc)
-	(if_then_else
-	 (match_operator 0 "equality_operator"
-	  [(match_operand:SI 1 "s_register_operand" "l")
-	   (neg:SI (match_operand:SI 2 "s_register_operand" "l"))])
-	 (label_ref (match_operand 3 "" ""))
-	 (pc)))]
-  "TARGET_THUMB1"
-  "*
-  output_asm_insn (\"cmn\\t%1, %2\", operands);
-  switch (get_attr_length (insn))
-    {
-    case 4:  return \"b%d0\\t%l3\";
-    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
-    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
-    }
-  "
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
-	         (le (minus (match_dup 3) (pc)) (const_int 256)))
-	    (const_int 4)
-	    (if_then_else
-	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
-		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
-		(const_int 6)
-		(const_int 8))))
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "*tbit_cbranch"
-  [(set (pc)
-	(if_then_else
-	 (match_operator 0 "equality_operator"
-	  [(zero_extract:SI (match_operand:SI 1 "s_register_operand" "l")
-			    (const_int 1)
-			    (match_operand:SI 2 "const_int_operand" "i"))
-	   (const_int 0)])
-	 (label_ref (match_operand 3 "" ""))
-	 (pc)))
-   (clobber (match_scratch:SI 4 "=l"))]
-  "TARGET_THUMB1"
-  "*
-  {
-  rtx op[3];
-  op[0] = operands[4];
-  op[1] = operands[1];
-  op[2] = GEN_INT (32 - 1 - INTVAL (operands[2]));
-
-  output_asm_insn (\"lsl\\t%0, %1, %2\", op);
-  switch (get_attr_length (insn))
-    {
-    case 4:  return \"b%d0\\t%l3\";
-    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
-    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
-    }
-  }"
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
-	         (le (minus (match_dup 3) (pc)) (const_int 256)))
-	    (const_int 4)
-	    (if_then_else
-	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
-		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
-		(const_int 6)
-		(const_int 8))))
-   (set_attr "type" "multiple")]
-)
-  
-(define_insn "*tlobits_cbranch"
-  [(set (pc)
-	(if_then_else
-	 (match_operator 0 "equality_operator"
-	  [(zero_extract:SI (match_operand:SI 1 "s_register_operand" "l")
-			    (match_operand:SI 2 "const_int_operand" "i")
-			    (const_int 0))
-	   (const_int 0)])
-	 (label_ref (match_operand 3 "" ""))
-	 (pc)))
-   (clobber (match_scratch:SI 4 "=l"))]
-  "TARGET_THUMB1"
-  "*
-  {
-  rtx op[3];
-  op[0] = operands[4];
-  op[1] = operands[1];
-  op[2] = GEN_INT (32 - INTVAL (operands[2]));
-
-  output_asm_insn (\"lsl\\t%0, %1, %2\", op);
-  switch (get_attr_length (insn))
-    {
-    case 4:  return \"b%d0\\t%l3\";
-    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
-    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
-    }
-  }"
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
-	         (le (minus (match_dup 3) (pc)) (const_int 256)))
-	    (const_int 4)
-	    (if_then_else
-	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
-		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
-		(const_int 6)
-		(const_int 8))))
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "*tstsi3_cbranch"
-  [(set (pc)
-	(if_then_else
-	 (match_operator 3 "equality_operator"
-	  [(and:SI (match_operand:SI 0 "s_register_operand" "%l")
-		   (match_operand:SI 1 "s_register_operand" "l"))
-	   (const_int 0)])
-	 (label_ref (match_operand 2 "" ""))
-	 (pc)))]
-  "TARGET_THUMB1"
-  "*
-  {
-  output_asm_insn (\"tst\\t%0, %1\", operands);
-  switch (get_attr_length (insn))
-    {
-    case 4:  return \"b%d3\\t%l2\";
-    case 6:  return \"b%D3\\t.LCB%=\;b\\t%l2\\t%@long jump\\n.LCB%=:\";
-    default: return \"b%D3\\t.LCB%=\;bl\\t%l2\\t%@far jump\\n.LCB%=:\";
-    }
-  }"
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 2) (pc)) (const_int -250))
-	         (le (minus (match_dup 2) (pc)) (const_int 256)))
-	    (const_int 4)
-	    (if_then_else
-	        (and (ge (minus (match_dup 2) (pc)) (const_int -2040))
-		     (le (minus (match_dup 2) (pc)) (const_int 2048)))
-		(const_int 6)
-		(const_int 8))))
-   (set_attr "type" "multiple")]
-)
-  
-(define_insn "*cbranchne_decr1"
-  [(set (pc)
-	(if_then_else (match_operator 3 "equality_operator"
-		       [(match_operand:SI 2 "s_register_operand" "l,l,1,l")
-		        (const_int 0)])
-		      (label_ref (match_operand 4 "" ""))
-		      (pc)))
-   (set (match_operand:SI 0 "thumb_cbrch_target_operand" "=l,*?h,*?m,*?m")
-	(plus:SI (match_dup 2) (const_int -1)))
-   (clobber (match_scratch:SI 1 "=X,l,&l,&l"))]
-  "TARGET_THUMB1"
-  "*
-   {
-     rtx cond[2];
-     cond[0] = gen_rtx_fmt_ee ((GET_CODE (operands[3]) == NE
-				? GEU : LTU),
-			       VOIDmode, operands[2], const1_rtx);
-     cond[1] = operands[4];
-
-     if (which_alternative == 0)
-       output_asm_insn (\"sub\\t%0, %2, #1\", operands);
-     else if (which_alternative == 1)
-       {
-	 /* We must provide an alternative for a hi reg because reload 
-	    cannot handle output reloads on a jump instruction, but we
-	    can't subtract into that.  Fortunately a mov from lo to hi
-	    does not clobber the condition codes.  */
-	 output_asm_insn (\"sub\\t%1, %2, #1\", operands);
-	 output_asm_insn (\"mov\\t%0, %1\", operands);
-       }
-     else
-       {
-	 /* Similarly, but the target is memory.  */
-	 output_asm_insn (\"sub\\t%1, %2, #1\", operands);
-	 output_asm_insn (\"str\\t%1, %0\", operands);
-       }
-
-     switch (get_attr_length (insn) - (which_alternative ? 2 : 0))
-       {
-	 case 4:
-	   output_asm_insn (\"b%d0\\t%l1\", cond);
-	   return \"\";
-	 case 6:
-	   output_asm_insn (\"b%D0\\t.LCB%=\", cond);
-	   return \"b\\t%l4\\t%@long jump\\n.LCB%=:\";
-	 default:
-	   output_asm_insn (\"b%D0\\t.LCB%=\", cond);
-	   return \"bl\\t%l4\\t%@far jump\\n.LCB%=:\";
-       }
-   }
-  "
-  [(set (attr "far_jump")
-        (if_then_else
-	    (ior (and (eq (symbol_ref ("which_alternative"))
-	                  (const_int 0))
-		      (eq_attr "length" "8"))
-		 (eq_attr "length" "10"))
-	    (const_string "yes")
-            (const_string "no")))
-   (set_attr_alternative "length"
-      [
-       ;; Alternative 0
-       (if_then_else
-	 (and (ge (minus (match_dup 4) (pc)) (const_int -250))
-	      (le (minus (match_dup 4) (pc)) (const_int 256)))
-	 (const_int 4)
-	 (if_then_else
-	   (and (ge (minus (match_dup 4) (pc)) (const_int -2040))
-		(le (minus (match_dup 4) (pc)) (const_int 2048)))
-	   (const_int 6)
-	   (const_int 8)))
-       ;; Alternative 1
-       (if_then_else
-	 (and (ge (minus (match_dup 4) (pc)) (const_int -248))
-	      (le (minus (match_dup 4) (pc)) (const_int 256)))
-	 (const_int 6)
-	 (if_then_else
-	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
-		(le (minus (match_dup 4) (pc)) (const_int 2048)))
-	   (const_int 8)
-	   (const_int 10)))
-       ;; Alternative 2
-       (if_then_else
-	 (and (ge (minus (match_dup 4) (pc)) (const_int -248))
-	      (le (minus (match_dup 4) (pc)) (const_int 256)))
-	 (const_int 6)
-	 (if_then_else
-	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
-		(le (minus (match_dup 4) (pc)) (const_int 2048)))
-	   (const_int 8)
-	   (const_int 10)))
-       ;; Alternative 3
-       (if_then_else
-	 (and (ge (minus (match_dup 4) (pc)) (const_int -248))
-	      (le (minus (match_dup 4) (pc)) (const_int 256)))
-	 (const_int 6)
-	 (if_then_else
-	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
-		(le (minus (match_dup 4) (pc)) (const_int 2048)))
-	   (const_int 8)
-	   (const_int 10)))])
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "*addsi3_cbranch"
-  [(set (pc)
-	(if_then_else
-	 (match_operator 4 "arm_comparison_operator"
-	  [(plus:SI
-	    (match_operand:SI 2 "s_register_operand" "%0,l,*l,1,1,1")
-	    (match_operand:SI 3 "reg_or_int_operand" "IJ,lL,*l,lIJ,lIJ,lIJ"))
-	   (const_int 0)])
-	 (label_ref (match_operand 5 "" ""))
-	 (pc)))
-   (set
-    (match_operand:SI 0 "thumb_cbrch_target_operand" "=l,l,*!h,*?h,*?m,*?m")
-    (plus:SI (match_dup 2) (match_dup 3)))
-   (clobber (match_scratch:SI 1 "=X,X,l,l,&l,&l"))]
-  "TARGET_THUMB1
-   && (GET_CODE (operands[4]) == EQ
-       || GET_CODE (operands[4]) == NE
-       || GET_CODE (operands[4]) == GE
-       || GET_CODE (operands[4]) == LT)"
-  "*
-   {
-     rtx cond[3];
-
-     cond[0] = (which_alternative < 2) ? operands[0] : operands[1];
-     cond[1] = operands[2];
-     cond[2] = operands[3];
-
-     if (CONST_INT_P (cond[2]) && INTVAL (cond[2]) < 0)
-       output_asm_insn (\"sub\\t%0, %1, #%n2\", cond);
-     else
-       output_asm_insn (\"add\\t%0, %1, %2\", cond);
-
-     if (which_alternative >= 2
-	 && which_alternative < 4)
-       output_asm_insn (\"mov\\t%0, %1\", operands);
-     else if (which_alternative >= 4)
-       output_asm_insn (\"str\\t%1, %0\", operands);
-
-     switch (get_attr_length (insn) - ((which_alternative >= 2) ? 2 : 0))
-       {
-	 case 4:
-	   return \"b%d4\\t%l5\";
-	 case 6:
-	   return \"b%D4\\t.LCB%=\;b\\t%l5\\t%@long jump\\n.LCB%=:\";
-	 default:
-	   return \"b%D4\\t.LCB%=\;bl\\t%l5\\t%@far jump\\n.LCB%=:\";
-       }
-   }
-  "
-  [(set (attr "far_jump")
-        (if_then_else
-	    (ior (and (lt (symbol_ref ("which_alternative"))
-	                  (const_int 2))
-		      (eq_attr "length" "8"))
-		 (eq_attr "length" "10"))
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length")
-     (if_then_else
-       (lt (symbol_ref ("which_alternative"))
-		       (const_int 2))
-       (if_then_else
-	 (and (ge (minus (match_dup 5) (pc)) (const_int -250))
-	      (le (minus (match_dup 5) (pc)) (const_int 256)))
-	 (const_int 4)
-	 (if_then_else
-	   (and (ge (minus (match_dup 5) (pc)) (const_int -2040))
-		(le (minus (match_dup 5) (pc)) (const_int 2048)))
-	   (const_int 6)
-	   (const_int 8)))
-       (if_then_else
-	 (and (ge (minus (match_dup 5) (pc)) (const_int -248))
-	      (le (minus (match_dup 5) (pc)) (const_int 256)))
-	 (const_int 6)
-	 (if_then_else
-	   (and (ge (minus (match_dup 5) (pc)) (const_int -2038))
-		(le (minus (match_dup 5) (pc)) (const_int 2048)))
-	   (const_int 8)
-	   (const_int 10)))))
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "*addsi3_cbranch_scratch"
-  [(set (pc)
-	(if_then_else
-	 (match_operator 3 "arm_comparison_operator"
-	  [(plus:SI
-	    (match_operand:SI 1 "s_register_operand" "%l,l,l,0")
-	    (match_operand:SI 2 "reg_or_int_operand" "J,l,L,IJ"))
-	   (const_int 0)])
-	 (label_ref (match_operand 4 "" ""))
-	 (pc)))
-   (clobber (match_scratch:SI 0 "=X,X,l,l"))]
-  "TARGET_THUMB1
-   && (GET_CODE (operands[3]) == EQ
-       || GET_CODE (operands[3]) == NE
-       || GET_CODE (operands[3]) == GE
-       || GET_CODE (operands[3]) == LT)"
-  "*
-   {
-     switch (which_alternative)
-       {
-       case 0:
-	 output_asm_insn (\"cmp\t%1, #%n2\", operands);
-	 break;
-       case 1:
-	 output_asm_insn (\"cmn\t%1, %2\", operands);
-	 break;
-       case 2:
-	 if (INTVAL (operands[2]) < 0)
-	   output_asm_insn (\"sub\t%0, %1, %2\", operands);
-	 else
-	   output_asm_insn (\"add\t%0, %1, %2\", operands);
-	 break;
-       case 3:
-	 if (INTVAL (operands[2]) < 0)
-	   output_asm_insn (\"sub\t%0, %0, %2\", operands);
-	 else
-	   output_asm_insn (\"add\t%0, %0, %2\", operands);
-	 break;
-       }
-
-     switch (get_attr_length (insn))
-       {
-	 case 4:
-	   return \"b%d3\\t%l4\";
-	 case 6:
-	   return \"b%D3\\t.LCB%=\;b\\t%l4\\t%@long jump\\n.LCB%=:\";
-	 default:
-	   return \"b%D3\\t.LCB%=\;bl\\t%l4\\t%@far jump\\n.LCB%=:\";
-       }
-   }
-  "
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "8")
-	    (const_string "yes")
-            (const_string "no")))
-   (set (attr "length")
-       (if_then_else
-	 (and (ge (minus (match_dup 4) (pc)) (const_int -250))
-	      (le (minus (match_dup 4) (pc)) (const_int 256)))
-	 (const_int 4)
-	 (if_then_else
-	   (and (ge (minus (match_dup 4) (pc)) (const_int -2040))
-		(le (minus (match_dup 4) (pc)) (const_int 2048)))
-	   (const_int 6)
-	   (const_int 8))))
-   (set_attr "type" "multiple")]
-)
-
-
 ;; Comparison and test insns
 
 (define_insn "*arm_cmpsi_insn"
@@ -8258,7 +6845,7 @@
    (set_attr "length" "2,2,4,4,4")
    (set_attr "predicable" "yes")
    (set_attr "predicable_short_it" "yes,yes,yes,no,no")
-   (set_attr "type" "alus_imm,alus_reg,alus_reg,alus_imm,alus_imm")]
+   (set_attr "type" "alus_imm,alus_sreg,alus_sreg,alus_imm,alus_imm")]
 )
 
 (define_insn "*cmpsi_shiftsi"
@@ -8385,18 +6972,6 @@
    (set_attr "type" "logics_reg")]
 )
 
-(define_insn "*thumb_cmpdi_zero"
-  [(set (reg:CC_Z CC_REGNUM)
-	(compare:CC_Z (match_operand:DI 0 "s_register_operand" "l")
-		      (const_int 0)))
-   (clobber (match_scratch:SI 1 "=l"))]
-  "TARGET_THUMB1"
-  "orr\\t%1, %Q0, %R0"
-  [(set_attr "conds" "set")
-   (set_attr "length" "2")
-   (set_attr "type" "logics_reg")]
-)
-
 ; This insn allows redundant compares to be removed by cse, nothing should
 ; ever appear in the output file since (set (reg x) (reg x)) is a no-op that
 ; is deleted later on. The match_dup will match the mode here, so that
@@ -8726,88 +7301,6 @@
    }"
 )
 
-(define_expand "cstoresi_eq0_thumb1"
-  [(parallel
-    [(set (match_operand:SI 0 "s_register_operand" "")
-	  (eq:SI (match_operand:SI 1 "s_register_operand" "")
-		 (const_int 0)))
-     (clobber (match_dup:SI 2))])]
-  "TARGET_THUMB1"
-  "operands[2] = gen_reg_rtx (SImode);"
-)
-
-(define_expand "cstoresi_ne0_thumb1"
-  [(parallel
-    [(set (match_operand:SI 0 "s_register_operand" "")
-	  (ne:SI (match_operand:SI 1 "s_register_operand" "")
-		 (const_int 0)))
-     (clobber (match_dup:SI 2))])]
-  "TARGET_THUMB1"
-  "operands[2] = gen_reg_rtx (SImode);"
-)
-
-(define_insn "*cstoresi_eq0_thumb1_insn"
-  [(set (match_operand:SI 0 "s_register_operand" "=&l,l")
-	(eq:SI (match_operand:SI 1 "s_register_operand" "l,0")
-	       (const_int 0)))
-   (clobber (match_operand:SI 2 "s_register_operand" "=X,l"))]
-  "TARGET_THUMB1"
-  "@
-   neg\\t%0, %1\;adc\\t%0, %0, %1
-   neg\\t%2, %1\;adc\\t%0, %1, %2"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
-(define_insn "*cstoresi_ne0_thumb1_insn"
-  [(set (match_operand:SI 0 "s_register_operand" "=l")
-	(ne:SI (match_operand:SI 1 "s_register_operand" "0")
-	       (const_int 0)))
-   (clobber (match_operand:SI 2 "s_register_operand" "=l"))]
-  "TARGET_THUMB1"
-  "sub\\t%2, %1, #1\;sbc\\t%0, %1, %2"
-  [(set_attr "length" "4")]
-)
-
-;; Used as part of the expansion of thumb ltu and gtu sequences
-(define_insn "cstoresi_nltu_thumb1"
-  [(set (match_operand:SI 0 "s_register_operand" "=l,l")
-        (neg:SI (ltu:SI (match_operand:SI 1 "s_register_operand" "l,*h")
-			(match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r"))))]
-  "TARGET_THUMB1"
-  "cmp\\t%1, %2\;sbc\\t%0, %0, %0"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
-(define_insn_and_split "cstoresi_ltu_thumb1"
-  [(set (match_operand:SI 0 "s_register_operand" "=l,l")
-        (ltu:SI (match_operand:SI 1 "s_register_operand" "l,*h")
-		(match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r")))]
-  "TARGET_THUMB1"
-  "#"
-  "TARGET_THUMB1"
-  [(set (match_dup 3)
-	(neg:SI (ltu:SI (match_dup 1) (match_dup 2))))
-   (set (match_dup 0) (neg:SI (match_dup 3)))]
-  "operands[3] = gen_reg_rtx (SImode);"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
-;; Used as part of the expansion of thumb les sequence.
-(define_insn "thumb1_addsi3_addgeu"
-  [(set (match_operand:SI 0 "s_register_operand" "=l")
-        (plus:SI (plus:SI (match_operand:SI 1 "s_register_operand" "%0")
-			  (match_operand:SI 2 "s_register_operand" "l"))
-		 (geu:SI (match_operand:SI 3 "s_register_operand" "l")
-			 (match_operand:SI 4 "thumb1_cmp_operand" "lI"))))]
-  "TARGET_THUMB1"
-  "cmp\\t%3, %4\;adc\\t%0, %1, %2"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
 
 ;; Conditional move insns
 
@@ -9027,29 +7520,6 @@
    (set_attr "type" "branch")]
 )
 
-(define_insn "*thumb_jump"
-  [(set (pc)
-	(label_ref (match_operand 0 "" "")))]
-  "TARGET_THUMB1"
-  "*
-  if (get_attr_length (insn) == 2)
-    return \"b\\t%l0\";
-  return \"bl\\t%l0\\t%@ far jump\";
-  "
-  [(set (attr "far_jump")
-        (if_then_else
-	    (eq_attr "length" "4")
-	    (const_string "yes")
-	    (const_string "no")))
-   (set (attr "length") 
-        (if_then_else
-	    (and (ge (minus (match_dup 0) (pc)) (const_int -2044))
-		 (le (minus (match_dup 0) (pc)) (const_int 2048)))
-  	    (const_int 2)
-	    (const_int 4)))
-   (set_attr "type" "branch")]
-)
-
 (define_expand "call"
   [(parallel [(call (match_operand 0 "memory_operand" "")
 	            (match_operand 1 "general_operand" ""))
@@ -9127,37 +7597,6 @@
    (set_attr "type" "call")]
 )
 
-(define_insn "*call_reg_thumb1_v5"
-  [(call (mem:SI (match_operand:SI 0 "register_operand" "l*r"))
-	 (match_operand 1 "" ""))
-   (use (match_operand 2 "" ""))
-   (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1 && arm_arch5 && !SIBLING_CALL_P (insn)"
-  "blx\\t%0"
-  [(set_attr "length" "2")
-   (set_attr "type" "call")]
-)
-
-(define_insn "*call_reg_thumb1"
-  [(call (mem:SI (match_operand:SI 0 "register_operand" "l*r"))
-	 (match_operand 1 "" ""))
-   (use (match_operand 2 "" ""))
-   (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1 && !arm_arch5 && !SIBLING_CALL_P (insn)"
-  "*
-  {
-    if (!TARGET_CALLER_INTERWORKING)
-      return thumb_call_via_reg (operands[0]);
-    else if (operands[1] == const0_rtx)
-      return \"bl\\t%__interwork_call_via_%0\";
-    else if (frame_pointer_needed)
-      return \"bl\\t%__interwork_r7_call_via_%0\";
-    else
-      return \"bl\\t%__interwork_r11_call_via_%0\";
-  }"
-  [(set_attr "type" "call")]
-)
-
 (define_expand "call_value"
   [(parallel [(set (match_operand       0 "" "")
 	           (call (match_operand 1 "memory_operand" "")
@@ -9238,39 +7677,6 @@
    (set_attr "type" "call")]
 )
 
-(define_insn "*call_value_reg_thumb1_v5"
-  [(set (match_operand 0 "" "")
-	(call (mem:SI (match_operand:SI 1 "register_operand" "l*r"))
-	      (match_operand 2 "" "")))
-   (use (match_operand 3 "" ""))
-   (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1 && arm_arch5"
-  "blx\\t%1"
-  [(set_attr "length" "2")
-   (set_attr "type" "call")]
-)
-
-(define_insn "*call_value_reg_thumb1"
-  [(set (match_operand 0 "" "")
-	(call (mem:SI (match_operand:SI 1 "register_operand" "l*r"))
-	      (match_operand 2 "" "")))
-   (use (match_operand 3 "" ""))
-   (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1 && !arm_arch5"
-  "*
-  {
-    if (!TARGET_CALLER_INTERWORKING)
-      return thumb_call_via_reg (operands[1]);
-    else if (operands[2] == const0_rtx)
-      return \"bl\\t%__interwork_call_via_%1\";
-    else if (frame_pointer_needed)
-      return \"bl\\t%__interwork_r7_call_via_%1\";
-    else
-      return \"bl\\t%__interwork_r11_call_via_%1\";
-  }"
-  [(set_attr "type" "call")]
-)
-
 ;; Allow calls to SYMBOL_REFs specially as they are not valid general addresses
 ;; The 'a' causes the operand to be treated as an address, i.e. no '#' output.
 
@@ -9307,33 +7713,6 @@
   [(set_attr "type" "call")]
 )
 
-(define_insn "*call_insn"
-  [(call (mem:SI (match_operand:SI 0 "" ""))
-	 (match_operand:SI 1 "" ""))
-   (use (match_operand 2 "" ""))
-   (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1
-   && GET_CODE (operands[0]) == SYMBOL_REF
-   && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[0]))"
-  "bl\\t%a0"
-  [(set_attr "length" "4")
-   (set_attr "type" "call")]
-)
-
-(define_insn "*call_value_insn"
-  [(set (match_operand 0 "" "")
-	(call (mem:SI (match_operand 1 "" ""))
-	      (match_operand 2 "" "")))
-   (use (match_operand 3 "" ""))
-   (clobber (reg:SI LR_REGNUM))]
-  "TARGET_THUMB1
-   && GET_CODE (operands[1]) == SYMBOL_REF
-   && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[1]))"
-  "bl\\t%a1"
-  [(set_attr "length" "4")
-   (set_attr "type" "call")]
-)
-
 ;; We may also be able to do sibcalls for Thumb, but it's much harder...
 (define_expand "sibcall"
   [(parallel [(call (match_operand 0 "memory_operand" "")
@@ -9727,38 +8106,6 @@
    (set_attr "type" "multiple")]
 )
 
-(define_expand "thumb1_casesi_internal_pic"
-  [(match_operand:SI 0 "s_register_operand" "")
-   (match_operand:SI 1 "thumb1_cmp_operand" "")
-   (match_operand 2 "" "")
-   (match_operand 3 "" "")]
-  "TARGET_THUMB1"
-  {
-    rtx reg0;
-    rtx test = gen_rtx_GTU (VOIDmode, operands[0], operands[1]);
-    emit_jump_insn (gen_cbranchsi4 (test, operands[0], operands[1],
-				    operands[3]));
-    reg0 = gen_rtx_REG (SImode, 0);
-    emit_move_insn (reg0, operands[0]);
-    emit_jump_insn (gen_thumb1_casesi_dispatch (operands[2]/*, operands[3]*/));
-    DONE;
-  }
-)
-
-(define_insn "thumb1_casesi_dispatch"
-  [(parallel [(set (pc) (unspec [(reg:SI 0)
-				 (label_ref (match_operand 0 "" ""))
-;;				 (label_ref (match_operand 1 "" ""))
-]
-			 UNSPEC_THUMB1_CASESI))
-	      (clobber (reg:SI IP_REGNUM))
-              (clobber (reg:SI LR_REGNUM))])]
-  "TARGET_THUMB1"
-  "* return thumb1_output_casesi(operands);"
-  [(set_attr "length" "4")
-   (set_attr "type" "multiple")]
-)
-
 (define_expand "indirect_jump"
   [(set (pc)
 	(match_operand:SI 0 "s_register_operand" ""))]
@@ -9797,17 +8144,6 @@
    (set_attr "predicable" "yes")]
 )
 
-;; NB Never uses BX.
-(define_insn "*thumb1_indirect_jump"
-  [(set (pc)
-	(match_operand:SI 0 "register_operand" "l*r"))]
-  "TARGET_THUMB1"
-  "mov\\tpc, %0"
-  [(set_attr "conds" "clob")
-   (set_attr "length" "2")
-   (set_attr "type" "branch")]
-)
-
 
 ;; Misc insns
 
@@ -11062,10 +9398,10 @@
    (set_attr_alternative "type"
                          [(if_then_else (match_operand 3 "const_int_operand" "")
                                         (const_string "alu_imm" )
-                                        (const_string "alu_reg"))
+                                        (const_string "alu_sreg"))
                           (const_string "alu_imm")
-                          (const_string "alu_reg")
-                          (const_string "alu_reg")])]
+                          (const_string "alu_sreg")
+                          (const_string "alu_sreg")])]
 )
 
 (define_insn "*ifcompare_move_plus"
@@ -11102,7 +9438,7 @@
    sub%D4\\t%0, %2, #%n3\;mov%d4\\t%0, %1"
   [(set_attr "conds" "use")
    (set_attr "length" "4,4,8,8")
-   (set_attr "type" "alu_reg,alu_imm,multiple,multiple")]
+   (set_attr "type" "alu_sreg,alu_imm,multiple,multiple")]
 )
 
 (define_insn "*ifcompare_arith_arith"
@@ -11774,14 +10110,6 @@
   "
 )
 
-(define_insn "prologue_thumb1_interwork"
-  [(unspec_volatile [(const_int 0)] VUNSPEC_THUMB1_INTERWORK)]
-  "TARGET_THUMB1"
-  "* return thumb1_output_interwork ();"
-  [(set_attr "length" "8")
-   (set_attr "type" "multiple")]
-)
-
 ;; Note - although unspec_volatile's USE all hard registers,
 ;; USEs are ignored after relaod has completed.  Thus we need
 ;; to add an unspec of the link register to ensure that flow
@@ -11797,21 +10125,6 @@
    "
 )
 
-(define_insn "*epilogue_insns"
-  [(unspec_volatile [(return)] VUNSPEC_EPILOGUE)]
-  "TARGET_THUMB1"
-  "*
-    return thumb1_unexpanded_epilogue ();
-  "
-  ; Length is absolute worst case
-  [(set_attr "length" "44")
-   (set_attr "type" "block")
-   ;; We don't clobber the conditions, but the potential length of this
-   ;; operation is sufficient to make conditionalizing the sequence 
-   ;; unlikely to be profitable.
-   (set_attr "conds" "clob")]
-)
-
 (define_expand "eh_epilogue"
   [(use (match_operand:SI 0 "register_operand" ""))
    (use (match_operand:SI 1 "register_operand" ""))
@@ -12222,33 +10535,6 @@
   [(set_attr "type" "no_insn")]
 )
 
-(define_insn "consttable_1"
-  [(unspec_volatile [(match_operand 0 "" "")] VUNSPEC_POOL_1)]
-  "TARGET_THUMB1"
-  "*
-  making_const_table = TRUE;
-  assemble_integer (operands[0], 1, BITS_PER_WORD, 1);
-  assemble_zeros (3);
-  return \"\";
-  "
-  [(set_attr "length" "4")
-   (set_attr "type" "no_insn")]
-)
-
-(define_insn "consttable_2"
-  [(unspec_volatile [(match_operand 0 "" "")] VUNSPEC_POOL_2)]
-  "TARGET_THUMB1"
-  "*
-  making_const_table = TRUE;
-  gcc_assert (GET_MODE_CLASS (GET_MODE (operands[0])) != MODE_FLOAT);
-  assemble_integer (operands[0], 2, BITS_PER_WORD, 1);
-  assemble_zeros (2);
-  return \"\";
-  "
-  [(set_attr "length" "4")
-   (set_attr "type" "no_insn")]
-)
-
 (define_insn "consttable_4"
   [(unspec_volatile [(match_operand 0 "" "")] VUNSPEC_POOL_4)]
   "TARGET_EITHER"
@@ -12336,35 +10622,6 @@
    (set_attr "type" "no_insn")]
 )
 
-;; Miscellaneous Thumb patterns
-
-(define_expand "tablejump"
-  [(parallel [(set (pc) (match_operand:SI 0 "register_operand" ""))
-	      (use (label_ref (match_operand 1 "" "")))])]
-  "TARGET_THUMB1"
-  "
-  if (flag_pic)
-    {
-      /* Hopefully, CSE will eliminate this copy.  */
-      rtx reg1 = copy_addr_to_reg (gen_rtx_LABEL_REF (Pmode, operands[1]));
-      rtx reg2 = gen_reg_rtx (SImode);
-
-      emit_insn (gen_addsi3 (reg2, operands[0], reg1));
-      operands[0] = reg2;
-    }
-  "
-)
-
-;; NB never uses BX.
-(define_insn "*thumb1_tablejump"
-  [(set (pc) (match_operand:SI 0 "register_operand" "l*r"))
-   (use (label_ref (match_operand 1 "" "")))]
-  "TARGET_THUMB1"
-  "mov\\t%|pc, %0"
-  [(set_attr "length" "2")
-   (set_attr "type" "no_insn")]
-)
-
 ;; V5 Instructions,
 
 (define_insn "clzsi2"
@@ -12459,22 +10716,6 @@
   }"
 )
 
-(define_insn_and_split "thumb_eh_return"
-  [(unspec_volatile [(match_operand:SI 0 "s_register_operand" "l")]
-		    VUNSPEC_EH_RETURN)
-   (clobber (match_scratch:SI 1 "=&l"))]
-  "TARGET_THUMB1"
-  "#"
-  "&& reload_completed"
-  [(const_int 0)]
-  "
-  {
-    thumb_set_return_address (operands[0], operands[1]);
-    DONE;
-  }"
-  [(set_attr "type" "mov_reg")]
-)
-
 
 ;; TLS support
 
@@ -12822,6 +11063,8 @@
 (include "iwmmxt.md")
 ;; Load the VFP co-processor patterns
 (include "vfp.md")
+;; Thumb-1 patterns
+(include "thumb1.md")
 ;; Thumb-2 patterns
 (include "thumb2.md")
 ;; Neon patterns
diff --git a/gcc/config/arm/arm1020e.md b/gcc/config/arm/arm1020e.md
index 0206ea2..c157730 100644
--- a/gcc/config/arm/arm1020e.md
+++ b/gcc/config/arm/arm1020e.md
@@ -67,7 +67,7 @@
 (define_insn_reservation "1020alu_op" 1 
  (and (eq_attr "tune" "arm1020e,arm1022e")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,\
diff --git a/gcc/config/arm/arm1026ejs.md b/gcc/config/arm/arm1026ejs.md
index 3f290b4..f8c66e9 100644
--- a/gcc/config/arm/arm1026ejs.md
+++ b/gcc/config/arm/arm1026ejs.md
@@ -67,7 +67,7 @@
 (define_insn_reservation "alu_op" 1 
  (and (eq_attr "tune" "arm1026ejs")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,\
diff --git a/gcc/config/arm/arm1136jfs.md b/gcc/config/arm/arm1136jfs.md
index 9e941da..a3c61a8 100644
--- a/gcc/config/arm/arm1136jfs.md
+++ b/gcc/config/arm/arm1136jfs.md
@@ -76,7 +76,7 @@
 (define_insn_reservation "11_alu_op" 2
  (and (eq_attr "tune" "arm1136js,arm1136jfs")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,\
diff --git a/gcc/config/arm/arm926ejs.md b/gcc/config/arm/arm926ejs.md
index 883935d..c6e154a 100644
--- a/gcc/config/arm/arm926ejs.md
+++ b/gcc/config/arm/arm926ejs.md
@@ -59,7 +59,7 @@
 (define_insn_reservation "9_alu_op" 1 
  (and (eq_attr "tune" "arm926ejs")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        alu_shift_imm,alus_shift_imm,\
diff --git a/gcc/config/arm/bpabi.h b/gcc/config/arm/bpabi.h
index 7a576ac..f99e1af 100644
--- a/gcc/config/arm/bpabi.h
+++ b/gcc/config/arm/bpabi.h
@@ -70,10 +70,13 @@
    |mcpu=cortex-a53					\
    |mcpu=cortex-a57					\
    |mcpu=cortex-a57.cortex-a53				\
+   |mcpu=cortex-m1.small-multiply                       \
+   |mcpu=cortex-m0.small-multiply                       \
+   |mcpu=cortex-m0plus.small-multiply			\
    |mcpu=generic-armv7-a                                \
    |march=armv7ve	                                \
    |march=armv7-m|mcpu=cortex-m3                        \
-   |march=armv7e-m|mcpu=cortex-m4                       \
+   |march=armv7e-m|mcpu=cortex-m4|mcpu=cortex-m7        \
    |march=armv6-m|mcpu=cortex-m0                        \
    |march=armv8-a					\
    :%{!r:--be8}}}"
@@ -87,11 +90,14 @@
    |mcpu=cortex-a53					\
    |mcpu=cortex-a57					\
    |mcpu=cortex-a57.cortex-a53				\
+   |mcpu=cortex-m1.small-multiply                       \
+   |mcpu=cortex-m0.small-multiply                       \
+   |mcpu=cortex-m0plus.small-multiply                   \
    |mcpu=marvell-pj4					\
    |mcpu=generic-armv7-a                                \
    |march=armv7ve	                                \
    |march=armv7-m|mcpu=cortex-m3                        \
-   |march=armv7e-m|mcpu=cortex-m4                       \
+   |march=armv7e-m|mcpu=cortex-m4|mcpu=cortex-m7        \
    |march=armv6-m|mcpu=cortex-m0                        \
    |march=armv8-a					\
    :%{!r:--be8}}}"
diff --git a/gcc/config/arm/cortex-a15.md b/gcc/config/arm/cortex-a15.md
index b3f126a..9477d82 100644
--- a/gcc/config/arm/cortex-a15.md
+++ b/gcc/config/arm/cortex-a15.md
@@ -62,9 +62,9 @@
 (define_insn_reservation "cortex_a15_alu" 2
   (and (eq_attr "tune" "cortexa15")
        (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,rev,\
+                        adr,bfm,rev,alu_dsp_reg,\
                         shift_imm,shift_reg,\
                         mov_imm,mov_reg,\
                         mvn_imm,mvn_reg,\
diff --git a/gcc/config/arm/cortex-a5.md b/gcc/config/arm/cortex-a5.md
index eed098e..0cf8ad9 100644
--- a/gcc/config/arm/cortex-a5.md
+++ b/gcc/config/arm/cortex-a5.md
@@ -59,9 +59,9 @@
 (define_insn_reservation "cortex_a5_alu" 2
   (and (eq_attr "tune" "cortexa5")
        (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,rev,\
+                        adr,bfm,rev,alu_dsp_reg,\
                         shift_imm,shift_reg,\
                         mov_imm,mov_reg,mvn_imm,mvn_reg,\
                         mrs,multiple,no_insn"))
diff --git a/gcc/config/arm/cortex-a53.md b/gcc/config/arm/cortex-a53.md
index a629bd6..b000a94 100644
--- a/gcc/config/arm/cortex-a53.md
+++ b/gcc/config/arm/cortex-a53.md
@@ -73,9 +73,9 @@
 (define_insn_reservation "cortex_a53_alu" 2
   (and (eq_attr "tune" "cortexa53")
        (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,csel,rev,\
+                        adr,bfm,csel,rev,alu_dsp_reg,\
                         shift_imm,shift_reg,\
                         mov_imm,mov_reg,mvn_imm,mvn_reg,\
                         mrs,multiple,no_insn"))
diff --git a/gcc/config/arm/cortex-a7.md b/gcc/config/arm/cortex-a7.md
index 8291d7f..de25461 100644
--- a/gcc/config/arm/cortex-a7.md
+++ b/gcc/config/arm/cortex-a7.md
@@ -135,9 +135,9 @@
 ;; with a younger immediate-based instruction.
 (define_insn_reservation "cortex_a7_alu_reg" 2
   (and (eq_attr "tune" "cortexa7")
-       (eq_attr "type" "alu_reg,alus_reg,logic_reg,logics_reg,\
+       (eq_attr "type" "alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        bfm,rev,\
+                        bfm,rev,alu_dsp_reg,\
                         shift_imm,shift_reg,mov_reg,mvn_reg"))
   "cortex_a7_ex1")
 
diff --git a/gcc/config/arm/cortex-a8.md b/gcc/config/arm/cortex-a8.md
index b272472..8ef6c99 100644
--- a/gcc/config/arm/cortex-a8.md
+++ b/gcc/config/arm/cortex-a8.md
@@ -86,9 +86,9 @@
 (define_insn_reservation "cortex_a8_alu" 2
   (and (eq_attr "tune" "cortexa8")
        (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,clz,rbit,rev,\
+                        adr,bfm,clz,rbit,rev,alu_dsp_reg,\
                         shift_imm,shift_reg,\
                         multiple,no_insn"))
   "cortex_a8_default")
diff --git a/gcc/config/arm/cortex-a9.md b/gcc/config/arm/cortex-a9.md
index a888896..feac547 100644
--- a/gcc/config/arm/cortex-a9.md
+++ b/gcc/config/arm/cortex-a9.md
@@ -81,9 +81,9 @@ cortex_a9_p1_e2 + cortex_a9_p0_e1 + cortex_a9_p1_e1")
 (define_insn_reservation "cortex_a9_dp" 2
   (and (eq_attr "tune" "cortexa9")
        (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,rev,\
+                        adr,bfm,rev,alu_dsp_reg,\
                         shift_imm,shift_reg,\
                         mov_imm,mov_reg,mvn_imm,mvn_reg,\
                         mov_shift_reg,mov_shift,\
diff --git a/gcc/config/arm/cortex-m4.md b/gcc/config/arm/cortex-m4.md
index 690ce75..fb9997a 100644
--- a/gcc/config/arm/cortex-m4.md
+++ b/gcc/config/arm/cortex-m4.md
@@ -32,9 +32,9 @@
 (define_insn_reservation "cortex_m4_alu" 1
   (and (eq_attr "tune" "cortexm4")
        (ior (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                             alu_reg,alus_reg,logic_reg,logics_reg,\
+                             alu_sreg,alus_sreg,logic_reg,logics_reg,\
                              adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                             adr,bfm,rev,\
+                             adr,bfm,rev,alu_dsp_reg,\
                              shift_imm,shift_reg,extend,\
                              alu_shift_imm,alus_shift_imm,\
                              logic_shift_imm,logics_shift_imm,\
diff --git a/gcc/config/arm/cortex-m7.md b/gcc/config/arm/cortex-m7.md
new file mode 100644
index 0000000..aab1da1
--- /dev/null
+++ b/gcc/config/arm/cortex-m7.md
@@ -0,0 +1,181 @@
+;; ARM Cortex-M7 pipeline description
+;; Copyright (C) 2014 Free Software Foundation, Inc.
+;;
+;; This file is part of GCC.
+;;
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 3, or (at your option)
+;; any later version.
+;;
+;; GCC is distributed in the hope that it will be useful, but
+;; WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+;; General Public License for more details.
+;;
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+(define_automaton "cortex_m7")
+
+;; We model the dual-issue constraints of this core with
+;; following units.
+
+(define_cpu_unit "cm7_i0, cm7_i1" "cortex_m7")
+(define_cpu_unit "cm7_a0, cm7_a1" "cortex_m7")
+(define_cpu_unit "cm7_branch,cm7_wb,cm7_ext,cm7_shf" "cortex_m7")
+(define_cpu_unit "cm7_lsu" "cortex_m7")
+(define_cpu_unit "cm7_mac" "cortex_m7")
+(define_cpu_unit "cm7_fpu" "cortex_m7")
+
+(define_reservation "cm7_all_units"
+                    "cm7_i0+cm7_i1+cm7_a0+cm7_a1+cm7_branch\
+                     +cm7_wb+cm7_ext+cm7_shf+cm7_lsu+cm7_mac\
+                     +cm7_fpu")
+
+;; Simple alu instruction without inline shift operation.
+(define_insn_reservation "cortex_m7_alu_simple" 2
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
+                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
+                        adr,bfm,rev,\
+                        shift_imm,shift_reg,\
+                        mov_imm,mov_reg,mvn_imm,mvn_reg,\
+                        mov_shift_reg,mov_shift,\
+                        mvn_shift,mvn_shift_reg,\
+                        logic_shift_imm,logics_shift_imm,\
+                        alu_shift_reg,alus_shift_reg,\
+                        logic_shift_reg,logics_shift_reg,\
+                        mrs,clz,f_mcr,f_mrc,multiple,no_insn"))
+  "cm7_i0|cm7_i1,cm7_a0|cm7_a1")
+
+;; Simple alu with inline shift operation.
+(define_insn_reservation "cortex_m7_alu_shift" 2
+   (and (eq_attr "tune" "cortexm7")
+	(eq_attr "type" "alu_shift_imm,alus_shift_imm"))
+   "cm7_i0|cm7_i1,(cm7_a0|cm7_a1)+cm7_shf+cm7_branch")
+
+;; Only one ALU can be used for DSP instructions.
+(define_insn_reservation "cortex_m7_dsp" 2
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "alu_dsp_reg,smlaxy,smlalxy,smulxy"))
+  "cm7_i0|cm7_i1,cm7_a0")
+
+;; The multiply instructions.
+(define_insn_reservation "cortex_m7_multiply" 2
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "mul,muls,umull,smull"))
+   "cm7_i0|cm7_i1,(cm7_a0|cm7_a1)+cm7_wb")
+
+(define_insn_reservation "cortex_m7_idiv" 4
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "sdiv,udiv"))
+   "cm7_all_units*4")
+
+(define_insn_reservation "cortex_m7_alu_extend" 2
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "extend"))
+   "cm7_i0|cm7_i1,(cm7_a0|cm7_a1)+cm7_ext+cm7_branch")
+
+(define_insn_reservation "cortex_m7_mac" 2
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "mla,mlas"))
+   "cm7_i0|cm7_i1,cm7_mac+cm7_wb")
+
+;; The branch instructions.
+(define_insn_reservation "cortex_m7_branch" 0
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "branch,call"))
+   "cm7_i0|cm7_i1,cm7_branch")
+
+;; The load instructions.
+(define_insn_reservation "cortex_m7_load1" 2
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "load_byte, load1"))
+   "cm7_i0|cm7_i1,cm7_lsu")
+
+(define_insn_reservation "cortex_m7_load2" 2
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "load2"))
+   "cm7_all_units")
+
+(define_insn_reservation "cortex_m7_loadm" 2
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "load3,load4"))
+   "cm7_all_units*2")
+
+;; The store instructions.
+(define_insn_reservation "cortex_m7_store1" 0
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "store1"))
+   "cm7_i0|cm7_i1,cm7_lsu+cm7_wb")
+
+(define_insn_reservation "cortex_m7_store2" 0
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "store2"))
+   "cm7_all_units")
+
+(define_insn_reservation "cortex_m7_storem" 0
+   (and (eq_attr "tune" "cortexm7")
+        (eq_attr "type" "store3,store4"))
+   "cm7_all_units*2")
+
+;; The FPU instructions.
+(define_insn_reservation "cortex_m7_fpalu" 3
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "ffariths,ffarithd,fadds,faddd,fmov,fconsts,\
+                        fconstd,fcmpd,f_cvt,f_cvtf2i,f_cvti2f, fcmps,\
+                        fmuls,f_flag"))
+  "cm7_i0|cm7_i1,cm7_fpu")
+
+(define_insn_reservation "cortex_m7_fmacs" 6
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "fmacs,ffmas"))
+  "cm7_i0|cm7_i1,cm7_fpu")
+
+(define_insn_reservation "cortex_m7_fdivs" 16
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "fdivs, fsqrts"))
+  "cm7_i0|cm7_i1, cm7_fpu*5")
+
+(define_insn_reservation "cortex_m7_f_loads" 2
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "f_loads"))
+  "cm7_i0|cm7_i1, cm7_lsu")
+
+(define_insn_reservation "cortex_m7_f_stores" 0
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "f_stores"))
+  "cm7_i0|cm7_i1, cm7_lsu+cm7_wb")
+
+(define_insn_reservation "cortex_m7_fmuld" 6
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "fmuld"))
+  "cm7_i0|cm7_i1,cm7_fpu*3")
+
+(define_insn_reservation "cortex_m7_fmacd" 10
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "fmacd,ffmad"))
+  "cm7_i0|cm7_i1,cm7_fpu*4")
+
+(define_insn_reservation "cortex_m7_fdivd" 31
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "fdivd,fsqrtd"))
+  "cm7_i0|cm7_i1,cm7_fpu*4")
+
+(define_insn_reservation "cortex_m7_f_loadd" 3
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "f_loadd"))
+  "cm7_all_units")
+
+(define_insn_reservation "cortex_m7_f_stored" 0
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "f_stored"))
+  "cm7_all_units")
+
+(define_insn_reservation "cortex_m7_f_mcr" 1
+  (and (eq_attr "tune" "cortexm7")
+       (eq_attr "type" "f_mcrr,f_mrrc"))
+  "cm7_all_units")
diff --git a/gcc/config/arm/cortex-r4.md b/gcc/config/arm/cortex-r4.md
index f000124..9dc906a 100644
--- a/gcc/config/arm/cortex-r4.md
+++ b/gcc/config/arm/cortex-r4.md
@@ -79,7 +79,7 @@
 (define_insn_reservation "cortex_r4_alu" 2
   (and (eq_attr "tune_cortexr4" "yes")
        (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                        alu_reg,alus_reg,logic_reg,logics_reg,\
+                        alu_sreg,alus_sreg,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
                         adr,bfm,rev,\
                         shift_imm,shift_reg,mvn_imm,mvn_reg"))
diff --git a/gcc/config/arm/fa526.md b/gcc/config/arm/fa526.md
index c345fdf..fc58d34 100644
--- a/gcc/config/arm/fa526.md
+++ b/gcc/config/arm/fa526.md
@@ -63,7 +63,7 @@
 (define_insn_reservation "526_alu_op" 1
  (and (eq_attr "tune" "fa526")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,\
diff --git a/gcc/config/arm/fa606te.md b/gcc/config/arm/fa606te.md
index 01ecfc8..432461d 100644
--- a/gcc/config/arm/fa606te.md
+++ b/gcc/config/arm/fa606te.md
@@ -63,7 +63,7 @@
 (define_insn_reservation "606te_alu_op" 1
  (and (eq_attr "tune" "fa606te")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,extend,\
diff --git a/gcc/config/arm/fa626te.md b/gcc/config/arm/fa626te.md
index e615bae..e5ba9b4 100644
--- a/gcc/config/arm/fa626te.md
+++ b/gcc/config/arm/fa626te.md
@@ -69,7 +69,7 @@
 (define_insn_reservation "626te_alu_op" 1
  (and (eq_attr "tune" "fa626,fa626te")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,\
diff --git a/gcc/config/arm/fa726te.md b/gcc/config/arm/fa726te.md
index 225b2cf..5af6736 100644
--- a/gcc/config/arm/fa726te.md
+++ b/gcc/config/arm/fa726te.md
@@ -87,7 +87,7 @@
 (define_insn_reservation "726te_alu_op" 1
  (and (eq_attr "tune" "fa726te")
       (eq_attr "type" "alu_imm,alus_imm,logic_imm,logics_imm,\
-                       alu_reg,alus_reg,logic_reg,logics_reg,\
+                       alu_sreg,alus_sreg,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
                        shift_imm,shift_reg,\
diff --git a/gcc/config/arm/fmp626.md b/gcc/config/arm/fmp626.md
index 439054d..24e9b6a 100644
--- a/gcc/config/arm/fmp626.md
+++ b/gcc/config/arm/fmp626.md
@@ -63,7 +63,7 @@
 ;; ALU operations
 (define_insn_reservation "mp626_alu_op" 1
  (and (eq_attr "tune" "fmp626")
-      (eq_attr "type" "alu_imm,alus_imm,alu_reg,alus_reg,\
+      (eq_attr "type" "alu_imm,alus_imm,alu_sreg,alus_sreg,\
                        logic_imm,logics_imm,logic_reg,logics_reg,\
                        adc_imm,adcs_imm,adc_reg,adcs_reg,\
                        adr,bfm,rev,\
diff --git a/gcc/config/arm/marvell-pj4.md b/gcc/config/arm/marvell-pj4.md
index 0b9d6eb..85f5f05 100644
--- a/gcc/config/arm/marvell-pj4.md
+++ b/gcc/config/arm/marvell-pj4.md
@@ -53,20 +53,20 @@
 
 (define_insn_reservation "pj4_alu" 1
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "alu_imm,alus_imm,alu_reg,alus_reg,\
+       (eq_attr "type" "alu_imm,alus_imm,alu_sreg,alus_sreg,\
                         logic_imm,logics_imm,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,rev,\
+                        adr,bfm,rev,alu_dsp_reg,\
                         shift_imm,shift_reg")
        (not (eq_attr "conds" "set")))
                                "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
 
 (define_insn_reservation "pj4_alu_conds" 4
   (and (eq_attr "tune" "marvell_pj4")
-       (eq_attr "type" "alu_imm,alus_imm,alu_reg,alus_reg,\
+       (eq_attr "type" "alu_imm,alus_imm,alu_sreg,alus_sreg,\
                         logic_imm,logics_imm,logic_reg,logics_reg,\
                         adc_imm,adcs_imm,adc_reg,adcs_reg,\
-                        adr,bfm,rev,\
+                        adr,bfm,rev,alu_dsp_reg,\
                         shift_imm,shift_reg")
        (eq_attr "conds" "set"))
                                "pj4_is,(pj4_alu1,pj4_w1+pj4_cp)|(pj4_alu2,pj4_w2+pj4_cp)")
diff --git a/gcc/config/arm/t-rmprofile b/gcc/config/arm/t-rmprofile
new file mode 100644
index 0000000..2f57f47e
--- /dev/null
+++ b/gcc/config/arm/t-rmprofile
@@ -0,0 +1,89 @@
+# A set of predefined MULTILIB which can be used for different ARM targets.
+# Via the configure option --with-multilib-list, user can customize the
+# final MULTILIB implementation.
+
+comma := ,
+space :=
+space +=
+
+MULTILIB_OPTIONS   = mthumb/marm
+MULTILIB_DIRNAMES  = thumb arm
+MULTILIB_OPTIONS  += march=armv6s-m/march=armv7-m/march=armv7e-m/march=armv7
+MULTILIB_DIRNAMES += armv6-m armv7-m armv7e-m armv7-ar
+MULTILIB_OPTIONS  += mfloat-abi=softfp/mfloat-abi=hard
+MULTILIB_DIRNAMES += softfp fpu
+MULTILIB_OPTIONS  += mfpu=fpv4-sp-d16/mfpu=vfpv3-d16
+MULTILIB_DIRNAMES += fpv4-sp-d16 vfpv3-d16
+
+MULTILIB_MATCHES   = march?armv6s-m=mcpu?cortex-m0
+MULTILIB_MATCHES  += march?armv6s-m=mcpu?cortex-m0plus
+MULTILIB_MATCHES  += march?armv6s-m=mcpu?cortex-m1
+MULTILIB_MATCHES  += march?armv6s-m=march?armv6-m
+MULTILIB_MATCHES  += march?armv7-m=mcpu?cortex-m3
+MULTILIB_MATCHES  += march?armv7e-m=mcpu?cortex-m4
+MULTILIB_MATCHES  += march?armv7e-m=mcpu?cortex-m7
+MULTILIB_MATCHES  += march?armv7=march?armv7-r
+MULTILIB_MATCHES  += march?armv7=march?armv7-a
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-r4
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-r4f
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-r5
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-r7
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-a5
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-a7
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-a8
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-a9
+MULTILIB_MATCHES  += march?armv7=mcpu?cortex-a15
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv3
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv3-fp16
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv3-d16-fp16
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv3xd
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv3xd-fp16
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv4
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?vfpv4-d16
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?neon
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?neon-fp16
+MULTILIB_MATCHES  += mfpu?vfpv3-d16=mfpu?neon-vfpv4
+MULTILIB_MATCHES  += mfpu?fpv4-sp-d16=mfpu?fpv5-sp-d16
+MULTILIB_MATCHES  += mfpu?fpv4-sp-d16=mfpu?fpv5-d16
+
+MULTILIB_EXCEPTIONS =
+MULTILIB_REUSE =
+
+MULTILIB_REQUIRED  = mthumb
+MULTILIB_REQUIRED += marm
+MULTILIB_REQUIRED += mfloat-abi=hard
+
+MULTILIB_OSDIRNAMES  = mthumb=!thumb
+MULTILIB_OSDIRNAMES += marm=!arm
+MULTILIB_OSDIRNAMES += mfloat-abi.hard=!fpu
+
+ifneq (,$(findstring armv6-m,$(subst $(comma),$(space),$(with_multilib_list))))
+MULTILIB_REQUIRED   += mthumb/march=armv6s-m
+MULTILIB_OSDIRNAMES += mthumb/march.armv6s-m=!armv6-m
+endif
+
+ifneq (,$(findstring armv7-m,$(subst $(comma),$(space),$(with_multilib_list))))
+MULTILIB_REQUIRED   += mthumb/march=armv7-m
+MULTILIB_OSDIRNAMES += mthumb/march.armv7-m=!armv7-m
+endif
+
+ifneq (,$(findstring armv7e-m,$(subst $(comma),$(space),$(with_multilib_list))))
+MULTILIB_REQUIRED   += mthumb/march=armv7e-m
+MULTILIB_REQUIRED   += mthumb/march=armv7e-m/mfloat-abi=softfp/mfpu=fpv4-sp-d16
+MULTILIB_REQUIRED   += mthumb/march=armv7e-m/mfloat-abi=hard/mfpu=fpv4-sp-d16
+MULTILIB_OSDIRNAMES += mthumb/march.armv7e-m=!armv7e-m
+MULTILIB_OSDIRNAMES += mthumb/march.armv7e-m/mfloat-abi.hard/mfpu.fpv4-sp-d16=!armv7e-m/fpu
+MULTILIB_OSDIRNAMES += mthumb/march.armv7e-m/mfloat-abi.softfp/mfpu.fpv4-sp-d16=!armv7e-m/softfp
+endif
+
+ifneq (,$(filter armv7 armv7-r armv7-a,$(subst $(comma),$(space),$(with_multilib_list))))
+MULTILIB_REQUIRED   += mthumb/march=armv7
+MULTILIB_REQUIRED   += mthumb/march=armv7/mfloat-abi=softfp/mfpu=vfpv3-d16
+MULTILIB_REQUIRED   += mthumb/march=armv7/mfloat-abi=hard/mfpu=vfpv3-d16
+MULTILIB_OSDIRNAMES += mthumb/march.armv7=!armv7-ar/thumb
+MULTILIB_OSDIRNAMES += mthumb/march.armv7/mfloat-abi.hard/mfpu.vfpv3-d16=!armv7-ar/thumb/fpu
+MULTILIB_OSDIRNAMES += mthumb/march.armv7/mfloat-abi.softfp/mfpu.vfpv3-d16=!armv7-ar/thumb/softfp
+MULTILIB_REUSE      += mthumb/march.armv7=marm/march.armv7
+MULTILIB_REUSE      += mthumb/march.armv7/mfloat-abi.softfp/mfpu.vfpv3-d16=marm/march.armv7/mfloat-abi.softfp/mfpu.vfpv3-d16
+MULTILIB_REUSE      += mthumb/march.armv7/mfloat-abi.hard/mfpu.vfpv3-d16=marm/march.armv7/mfloat-abi.hard/mfpu.vfpv3-d16
+endif
diff --git a/gcc/config/arm/thumb1.md b/gcc/config/arm/thumb1.md
new file mode 100644
index 0000000..cd399d3
--- /dev/null
+++ b/gcc/config/arm/thumb1.md
@@ -0,0 +1,1808 @@
+;; ARM Thumb-1 Machine Description
+;; Copyright (C) 2007-2014 Free Software Foundation, Inc.
+;;
+;; This file is part of GCC.
+;;
+;; GCC is free software; you can redistribute it and/or modify it
+;; under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 3, or (at your option)
+;; any later version.
+;;
+;; GCC is distributed in the hope that it will be useful, but
+;; WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+;; General Public License for more details.
+;;
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.  */
+
+
+;;---------------------------------------------------------------------------
+;; Insn patterns
+;;
+
+(define_insn "*thumb1_adddi3"
+  [(set (match_operand:DI          0 "register_operand" "=l")
+	(plus:DI (match_operand:DI 1 "register_operand" "%0")
+		 (match_operand:DI 2 "register_operand" "l")))
+   (clobber (reg:CC CC_REGNUM))
+  ]
+  "TARGET_THUMB1"
+  "add\\t%Q0, %Q0, %Q2\;adc\\t%R0, %R0, %R2"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "*thumb1_addsi3"
+  [(set (match_operand:SI          0 "register_operand" "=l,l,l,*rk,*hk,l,k,l,l,l")
+	(plus:SI (match_operand:SI 1 "register_operand" "%0,0,l,*0,*0,k,k,0,l,k")
+		 (match_operand:SI 2 "nonmemory_operand" "I,J,lL,*hk,*rk,M,O,Pa,Pb,Pc")))]
+  "TARGET_THUMB1"
+  "*
+   static const char * const asms[] =
+   {
+     \"add\\t%0, %0, %2\",
+     \"sub\\t%0, %0, #%n2\",
+     \"add\\t%0, %1, %2\",
+     \"add\\t%0, %0, %2\",
+     \"add\\t%0, %0, %2\",
+     \"add\\t%0, %1, %2\",
+     \"add\\t%0, %1, %2\",
+     \"#\",
+     \"#\",
+     \"#\"
+   };
+   if ((which_alternative == 2 || which_alternative == 6)
+       && CONST_INT_P (operands[2])
+       && INTVAL (operands[2]) < 0)
+     return \"sub\\t%0, %1, #%n2\";
+   return asms[which_alternative];
+  "
+  "&& reload_completed && CONST_INT_P (operands[2])
+   && ((operands[1] != stack_pointer_rtx
+        && (INTVAL (operands[2]) > 255 || INTVAL (operands[2]) < -255))
+       || (operands[1] == stack_pointer_rtx
+ 	   && INTVAL (operands[2]) > 1020))"
+  [(set (match_dup 0) (plus:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (plus:SI (match_dup 0) (match_dup 3)))]
+  {
+    HOST_WIDE_INT offset = INTVAL (operands[2]);
+    if (operands[1] == stack_pointer_rtx)
+      offset -= 1020;
+    else
+      {
+        if (offset > 255)
+	  offset = 255;
+	else if (offset < -255)
+	  offset = -255;
+      }
+    operands[3] = GEN_INT (offset);
+    operands[2] = GEN_INT (INTVAL (operands[2]) - offset);
+  }
+  [(set_attr "length" "2,2,2,2,2,2,2,4,4,4")
+   (set_attr "type" "alus_imm,alus_imm,alus_sreg,alus_sreg,alus_sreg,
+		     alus_sreg,alus_sreg,multiple,multiple,multiple")]
+)
+
+;; Reloading and elimination of the frame pointer can
+;; sometimes cause this optimization to be missed.
+(define_peephole2
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+	(match_operand:SI 1 "const_int_operand" ""))
+   (set (match_dup 0)
+	(plus:SI (match_dup 0) (reg:SI SP_REGNUM)))]
+  "TARGET_THUMB1
+   && (unsigned HOST_WIDE_INT) (INTVAL (operands[1])) < 1024
+   && (INTVAL (operands[1]) & 3) == 0"
+  [(set (match_dup 0) (plus:SI (reg:SI SP_REGNUM) (match_dup 1)))]
+  ""
+)
+
+(define_insn "*thumb_subdi3"
+  [(set (match_operand:DI           0 "register_operand" "=l")
+	(minus:DI (match_operand:DI 1 "register_operand"  "0")
+		  (match_operand:DI 2 "register_operand"  "l")))
+   (clobber (reg:CC CC_REGNUM))]
+  "TARGET_THUMB1"
+  "sub\\t%Q0, %Q0, %Q2\;sbc\\t%R0, %R0, %R2"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "thumb1_subsi3_insn"
+  [(set (match_operand:SI           0 "register_operand" "=l")
+	(minus:SI (match_operand:SI 1 "register_operand" "l")
+		  (match_operand:SI 2 "reg_or_int_operand" "lPd")))]
+  "TARGET_THUMB1"
+  "sub\\t%0, %1, %2"
+  [(set_attr "length" "2")
+   (set_attr "conds" "set")
+   (set_attr "type" "alus_sreg")]
+)
+
+; Unfortunately with the Thumb the '&'/'0' trick can fails when operands
+; 1 and 2; are the same, because reload will make operand 0 match
+; operand 1 without realizing that this conflicts with operand 2.  We fix
+; this by adding another alternative to match this case, and then `reload'
+; it ourselves.  This alternative must come first.
+(define_insn "*thumb_mulsi3"
+  [(set (match_operand:SI          0 "register_operand" "=&l,&l,&l")
+	(mult:SI (match_operand:SI 1 "register_operand" "%l,*h,0")
+		 (match_operand:SI 2 "register_operand" "l,l,l")))]
+ "TARGET_THUMB1 && !arm_arch6"
+  "*
+  if (which_alternative < 2)
+    return \"mov\\t%0, %1\;mul\\t%0, %2\";
+  else
+    return \"mul\\t%0, %2\";
+  "
+  [(set_attr "length" "4,4,2")
+   (set_attr "type" "muls")]
+)
+
+(define_insn "*thumb_mulsi3_v6"
+  [(set (match_operand:SI          0 "register_operand" "=l,l,l")
+	(mult:SI (match_operand:SI 1 "register_operand" "0,l,0")
+		 (match_operand:SI 2 "register_operand" "l,0,0")))]
+  "TARGET_THUMB1 && arm_arch6"
+  "@
+   mul\\t%0, %2
+   mul\\t%0, %1
+   mul\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "type" "muls")]
+)
+
+(define_insn "*thumb1_andsi3_insn"
+  [(set (match_operand:SI         0 "register_operand" "=l")
+	(and:SI (match_operand:SI 1 "register_operand" "%0")
+		(match_operand:SI 2 "register_operand" "l")))]
+  "TARGET_THUMB1"
+  "and\\t%0, %2"
+  [(set_attr "length" "2")
+   (set_attr "type"  "logic_imm")
+   (set_attr "conds" "set")])
+
+(define_split
+  [(set (match_operand:SI 0 "s_register_operand" "")
+	(zero_extract:SI (match_operand:SI 1 "s_register_operand" "")
+			 (match_operand:SI 2 "const_int_operand" "")
+			 (match_operand:SI 3 "const_int_operand" "")))
+   (clobber (match_operand:SI 4 "s_register_operand" ""))]
+  "TARGET_THUMB1"
+  [(set (match_dup 4) (ashift:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (lshiftrt:SI (match_dup 4) (match_dup 3)))]
+  "{
+     HOST_WIDE_INT temp = INTVAL (operands[2]);
+
+     operands[2] = GEN_INT (32 - temp - INTVAL (operands[3]));
+     operands[3] = GEN_INT (32 - temp);
+   }"
+)
+
+(define_split
+  [(set (match_operand:SI 0 "s_register_operand" "")
+	(sign_extract:SI (match_operand:SI 1 "s_register_operand" "")
+			 (match_operand:SI 2 "const_int_operand" "")
+			 (match_operand:SI 3 "const_int_operand" "")))]
+  "TARGET_THUMB1"
+  [(set (match_dup 0) (ashift:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (ashiftrt:SI (match_dup 0) (match_dup 3)))]
+  "{
+     HOST_WIDE_INT temp = INTVAL (operands[2]);
+
+     operands[2] = GEN_INT (32 - temp - INTVAL (operands[3]));
+     operands[3] = GEN_INT (32 - temp);
+   }"
+)
+
+(define_insn "thumb1_bicsi3"
+  [(set (match_operand:SI                 0 "register_operand" "=l")
+	(and:SI (not:SI (match_operand:SI 1 "register_operand" "l"))
+		(match_operand:SI         2 "register_operand" "0")))]
+  "TARGET_THUMB1"
+  "bic\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "conds" "set")
+   (set_attr "type" "logics_reg")]
+)
+
+(define_insn "*thumb1_iorsi3_insn"
+  [(set (match_operand:SI         0 "register_operand" "=l")
+	(ior:SI (match_operand:SI 1 "register_operand" "%0")
+		(match_operand:SI 2 "register_operand" "l")))]
+  "TARGET_THUMB1"
+  "orr\\t%0, %2"
+  [(set_attr "length" "2")
+   (set_attr "conds" "set")
+   (set_attr "type" "logics_reg")])
+
+(define_insn "*thumb1_xorsi3_insn"
+  [(set (match_operand:SI         0 "register_operand" "=l")
+	(xor:SI (match_operand:SI 1 "register_operand" "%0")
+		(match_operand:SI 2 "register_operand" "l")))]
+  "TARGET_THUMB1"
+  "eor\\t%0, %2"
+  [(set_attr "length" "2")
+   (set_attr "conds" "set")
+   (set_attr "type" "logics_reg")]
+)
+
+(define_insn "*thumb1_ashlsi3"
+  [(set (match_operand:SI            0 "register_operand" "=l,l")
+	(ashift:SI (match_operand:SI 1 "register_operand" "l,0")
+		   (match_operand:SI 2 "nonmemory_operand" "N,l")))]
+  "TARGET_THUMB1"
+  "lsl\\t%0, %1, %2"
+  [(set_attr "length" "2")
+   (set_attr "type" "shift_imm,shift_reg")
+   (set_attr "conds" "set")])
+
+(define_insn "*thumb1_ashrsi3"
+  [(set (match_operand:SI              0 "register_operand" "=l,l")
+	(ashiftrt:SI (match_operand:SI 1 "register_operand" "l,0")
+		     (match_operand:SI 2 "nonmemory_operand" "N,l")))]
+  "TARGET_THUMB1"
+  "asr\\t%0, %1, %2"
+  [(set_attr "length" "2")
+   (set_attr "type" "shift_imm,shift_reg")
+   (set_attr "conds" "set")])
+
+(define_insn "*thumb1_lshrsi3"
+  [(set (match_operand:SI              0 "register_operand" "=l,l")
+	(lshiftrt:SI (match_operand:SI 1 "register_operand" "l,0")
+		     (match_operand:SI 2 "nonmemory_operand" "N,l")))]
+  "TARGET_THUMB1"
+  "lsr\\t%0, %1, %2"
+  [(set_attr "length" "2")
+   (set_attr "type" "shift_imm,shift_reg")
+   (set_attr "conds" "set")])
+
+(define_insn "*thumb1_rotrsi3"
+  [(set (match_operand:SI              0 "register_operand" "=l")
+	(rotatert:SI (match_operand:SI 1 "register_operand" "0")
+		     (match_operand:SI 2 "register_operand" "l")))]
+  "TARGET_THUMB1"
+  "ror\\t%0, %0, %2"
+  [(set_attr "type" "shift_reg")
+   (set_attr "length" "2")]
+)
+
+(define_insn "*thumb1_negdi2"
+  [(set (match_operand:DI 0 "register_operand" "=&l")
+	(neg:DI (match_operand:DI 1 "register_operand" "l")))
+   (clobber (reg:CC CC_REGNUM))]
+  "TARGET_THUMB1"
+  "mov\\t%R0, #0\;neg\\t%Q0, %Q1\;sbc\\t%R0, %R1"
+  [(set_attr "length" "6")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*thumb1_negsi2"
+  [(set (match_operand:SI         0 "register_operand" "=l")
+	(neg:SI (match_operand:SI 1 "register_operand" "l")))]
+  "TARGET_THUMB1"
+  "neg\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "type" "alu_imm")]
+)
+
+(define_insn_and_split "*thumb1_abssi2"
+  [(set (match_operand:SI 0 "s_register_operand" "=l")
+	(abs:SI (match_operand:SI 1 "s_register_operand" "l")))
+   (clobber (match_scratch:SI 2 "=&l"))]
+  "TARGET_THUMB1"
+  "#"
+  "TARGET_THUMB1 && reload_completed"
+  [(set (match_dup 2) (ashiftrt:SI (match_dup 1) (const_int 31)))
+   (set (match_dup 0) (plus:SI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0) (xor:SI (match_dup 0) (match_dup 2)))]
+  ""
+  [(set_attr "length" "6")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "*thumb1_neg_abssi2"
+  [(set (match_operand:SI 0 "s_register_operand" "=l")
+	(neg:SI (abs:SI (match_operand:SI 1 "s_register_operand" "l"))))
+   (clobber (match_scratch:SI 2 "=&l"))]
+  "TARGET_THUMB1"
+  "#"
+  "TARGET_THUMB1 && reload_completed"
+  [(set (match_dup 2) (ashiftrt:SI (match_dup 1) (const_int 31)))
+   (set (match_dup 0) (minus:SI (match_dup 2) (match_dup 1)))
+   (set (match_dup 0) (xor:SI (match_dup 0) (match_dup 2)))]
+  ""
+  [(set_attr "length" "6")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*thumb1_one_cmplsi2"
+  [(set (match_operand:SI         0 "register_operand" "=l")
+	(not:SI (match_operand:SI 1 "register_operand"  "l")))]
+  "TARGET_THUMB1"
+  "mvn\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "type" "mvn_reg")]
+)
+
+(define_insn "*thumb1_zero_extendhisi2"
+  [(set (match_operand:SI 0 "register_operand" "=l,l")
+	(zero_extend:SI (match_operand:HI 1 "nonimmediate_operand" "l,m")))]
+  "TARGET_THUMB1"
+{
+  rtx mem;
+
+  if (which_alternative == 0 && arm_arch6)
+    return "uxth\t%0, %1";
+  if (which_alternative == 0)
+    return "#";
+
+  mem = XEXP (operands[1], 0);
+
+  if (GET_CODE (mem) == CONST)
+    mem = XEXP (mem, 0);
+
+  if (GET_CODE (mem) == PLUS)
+    {
+      rtx a = XEXP (mem, 0);
+
+      /* This can happen due to bugs in reload.  */
+      if (REG_P (a) && REGNO (a) == SP_REGNUM)
+        {
+          rtx ops[2];
+          ops[0] = operands[0];
+          ops[1] = a;
+
+          output_asm_insn ("mov\t%0, %1", ops);
+
+          XEXP (mem, 0) = operands[0];
+       }
+    }
+
+  return "ldrh\t%0, %1";
+}
+  [(set_attr_alternative "length"
+			 [(if_then_else (eq_attr "is_arch6" "yes")
+				       (const_int 2) (const_int 4))
+			 (const_int 4)])
+   (set_attr "type" "extend,load_byte")]
+)
+
+(define_insn "*thumb1_zero_extendqisi2"
+  [(set (match_operand:SI 0 "register_operand" "=l,l")
+	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "l,m")))]
+  "TARGET_THUMB1 && !arm_arch6"
+  "@
+   #
+   ldrb\\t%0, %1"
+  [(set_attr "length" "4,2")
+   (set_attr "type" "alu_shift_reg,load_byte")
+   (set_attr "pool_range" "*,32")]
+)
+
+(define_insn "*thumb1_zero_extendqisi2_v6"
+  [(set (match_operand:SI 0 "register_operand" "=l,l")
+	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "l,m")))]
+  "TARGET_THUMB1 && arm_arch6"
+  "@
+   uxtb\\t%0, %1
+   ldrb\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "type" "extend,load_byte")]
+)
+
+;; We used to have an early-clobber on the scratch register here.
+;; However, there's a bug somewhere in reload which means that this
+;; can be partially ignored during spill allocation if the memory
+;; address also needs reloading; this causes us to die later on when
+;; we try to verify the operands.  Fortunately, we don't really need
+;; the early-clobber: we can always use operand 0 if operand 2
+;; overlaps the address.
+(define_insn "thumb1_extendhisi2"
+  [(set (match_operand:SI 0 "register_operand" "=l,l")
+	(sign_extend:SI (match_operand:HI 1 "nonimmediate_operand" "l,m")))
+   (clobber (match_scratch:SI 2 "=X,l"))]
+  "TARGET_THUMB1"
+  "*
+  {
+    rtx ops[4];
+    rtx mem;
+
+    if (which_alternative == 0 && !arm_arch6)
+      return \"#\";
+    if (which_alternative == 0)
+      return \"sxth\\t%0, %1\";
+
+    mem = XEXP (operands[1], 0);
+
+    /* This code used to try to use 'V', and fix the address only if it was
+       offsettable, but this fails for e.g. REG+48 because 48 is outside the
+       range of QImode offsets, and offsettable_address_p does a QImode
+       address check.  */
+
+    if (GET_CODE (mem) == CONST)
+      mem = XEXP (mem, 0);
+
+    if (GET_CODE (mem) == LABEL_REF)
+      return \"ldr\\t%0, %1\";
+
+    if (GET_CODE (mem) == PLUS)
+      {
+        rtx a = XEXP (mem, 0);
+        rtx b = XEXP (mem, 1);
+
+        if (GET_CODE (a) == LABEL_REF
+	    && CONST_INT_P (b))
+          return \"ldr\\t%0, %1\";
+
+        if (REG_P (b))
+          return \"ldrsh\\t%0, %1\";
+
+        ops[1] = a;
+        ops[2] = b;
+      }
+    else
+      {
+        ops[1] = mem;
+        ops[2] = const0_rtx;
+      }
+
+    gcc_assert (REG_P (ops[1]));
+
+    ops[0] = operands[0];
+    if (reg_mentioned_p (operands[2], ops[1]))
+      ops[3] = ops[0];
+    else
+      ops[3] = operands[2];
+    output_asm_insn (\"mov\\t%3, %2\;ldrsh\\t%0, [%1, %3]\", ops);
+    return \"\";
+  }"
+  [(set_attr_alternative "length"
+			 [(if_then_else (eq_attr "is_arch6" "yes")
+					(const_int 2) (const_int 4))
+			  (const_int 4)])
+   (set_attr "type" "extend,load_byte")
+   (set_attr "pool_range" "*,1018")]
+)
+
+(define_split
+  [(set (match_operand:SI 0 "register_operand" "")
+	(sign_extend:SI (match_operand:QI 1 "memory_operand" "")))]
+  "TARGET_THUMB1 && reload_completed"
+  [(set (match_dup 0) (match_dup 2))
+   (set (match_dup 0) (sign_extend:SI (match_dup 3)))]
+{
+  rtx addr = XEXP (operands[1], 0);
+
+  if (GET_CODE (addr) == CONST)
+    addr = XEXP (addr, 0);
+
+  if (GET_CODE (addr) == PLUS
+      && REG_P (XEXP (addr, 0)) && REG_P (XEXP (addr, 1)))
+    /* No split necessary.  */
+    FAIL;
+
+  if (GET_CODE (addr) == PLUS
+      && !REG_P (XEXP (addr, 0)) && !REG_P (XEXP (addr, 1)))
+    FAIL;
+
+  if (reg_overlap_mentioned_p (operands[0], addr))
+    {
+      rtx t = gen_lowpart (QImode, operands[0]);
+      emit_move_insn (t, operands[1]);
+      emit_insn (gen_thumb1_extendqisi2 (operands[0], t));
+      DONE;
+    }
+
+  if (REG_P (addr))
+    {
+      addr = gen_rtx_PLUS (Pmode, addr, operands[0]);
+      operands[2] = const0_rtx;
+    }
+  else if (GET_CODE (addr) != PLUS)
+    FAIL;
+  else if (REG_P (XEXP (addr, 0)))
+    {
+      operands[2] = XEXP (addr, 1);
+      addr = gen_rtx_PLUS (Pmode, XEXP (addr, 0), operands[0]);
+    }
+  else
+    {
+      operands[2] = XEXP (addr, 0);
+      addr = gen_rtx_PLUS (Pmode, XEXP (addr, 1), operands[0]);
+    }
+
+  operands[3] = change_address (operands[1], QImode, addr);
+})
+
+(define_peephole2
+  [(set (match_operand:SI 0 "register_operand" "")
+	(plus:SI (match_dup 0) (match_operand 1 "const_int_operand")))
+   (set (match_operand:SI 2 "register_operand" "") (const_int 0))
+   (set (match_operand:SI 3 "register_operand" "")
+	(sign_extend:SI (match_operand:QI 4 "memory_operand" "")))]
+  "TARGET_THUMB1
+   && GET_CODE (XEXP (operands[4], 0)) == PLUS
+   && rtx_equal_p (operands[0], XEXP (XEXP (operands[4], 0), 0))
+   && rtx_equal_p (operands[2], XEXP (XEXP (operands[4], 0), 1))
+   && (peep2_reg_dead_p (3, operands[0])
+       || rtx_equal_p (operands[0], operands[3]))
+   && (peep2_reg_dead_p (3, operands[2])
+       || rtx_equal_p (operands[2], operands[3]))"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 3) (sign_extend:SI (match_dup 4)))]
+{
+  rtx addr = gen_rtx_PLUS (Pmode, operands[0], operands[2]);
+  operands[4] = change_address (operands[4], QImode, addr);
+})
+
+(define_insn "thumb1_extendqisi2"
+  [(set (match_operand:SI 0 "register_operand" "=l,l,l")
+	(sign_extend:SI (match_operand:QI 1 "nonimmediate_operand" "l,V,m")))]
+  "TARGET_THUMB1"
+{
+  rtx addr;
+
+  if (which_alternative == 0 && arm_arch6)
+    return "sxtb\\t%0, %1";
+  if (which_alternative == 0)
+    return "#";
+
+  addr = XEXP (operands[1], 0);
+  if (GET_CODE (addr) == PLUS
+      && REG_P (XEXP (addr, 0)) && REG_P (XEXP (addr, 1)))
+    return "ldrsb\\t%0, %1";
+
+  return "#";
+}
+  [(set_attr_alternative "length"
+			 [(if_then_else (eq_attr "is_arch6" "yes")
+					(const_int 2) (const_int 4))
+			  (const_int 2)
+			  (if_then_else (eq_attr "is_arch6" "yes")
+					(const_int 4) (const_int 6))])
+   (set_attr "type" "extend,load_byte,load_byte")]
+)
+
+;;; ??? This should have alternatives for constants.
+;;; ??? This was originally identical to the movdf_insn pattern.
+;;; ??? The 'i' constraint looks funny, but it should always be replaced by
+;;; thumb_reorg with a memory reference.
+(define_insn "*thumb1_movdi_insn"
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=l,l,l,l,>,l, m,*r")
+	(match_operand:DI 1 "general_operand"      "l, I,J,>,l,mi,l,*r"))]
+  "TARGET_THUMB1
+   && (   register_operand (operands[0], DImode)
+       || register_operand (operands[1], DImode))"
+  "*
+  {
+  switch (which_alternative)
+    {
+    default:
+    case 0:
+      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
+	return \"add\\t%0,  %1,  #0\;add\\t%H0, %H1, #0\";
+      return   \"add\\t%H0, %H1, #0\;add\\t%0,  %1,  #0\";
+    case 1:
+      return \"mov\\t%Q0, %1\;mov\\t%R0, #0\";
+    case 2:
+      operands[1] = GEN_INT (- INTVAL (operands[1]));
+      return \"mov\\t%Q0, %1\;neg\\t%Q0, %Q0\;asr\\t%R0, %Q0, #31\";
+    case 3:
+      return \"ldmia\\t%1, {%0, %H0}\";
+    case 4:
+      return \"stmia\\t%0, {%1, %H1}\";
+    case 5:
+      return thumb_load_double_from_address (operands);
+    case 6:
+      operands[2] = gen_rtx_MEM (SImode,
+			     plus_constant (Pmode, XEXP (operands[0], 0), 4));
+      output_asm_insn (\"str\\t%1, %0\;str\\t%H1, %2\", operands);
+      return \"\";
+    case 7:
+      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
+	return \"mov\\t%0, %1\;mov\\t%H0, %H1\";
+      return \"mov\\t%H0, %H1\;mov\\t%0, %1\";
+    }
+  }"
+  [(set_attr "length" "4,4,6,2,2,6,4,4")
+   (set_attr "type" "multiple,multiple,multiple,load2,store2,load2,store2,multiple")
+   (set_attr "pool_range" "*,*,*,*,*,1018,*,*")]
+)
+
+(define_insn "*thumb1_movsi_insn"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=l,l,l,l,l,>,l, m,*l*h*k")
+	(match_operand:SI 1 "general_operand"      "l, I,J,K,>,l,mi,l,*l*h*k"))]
+  "TARGET_THUMB1
+   && (   register_operand (operands[0], SImode)
+       || register_operand (operands[1], SImode))"
+  "@
+   mov	%0, %1
+   mov	%0, %1
+   #
+   #
+   ldmia\\t%1, {%0}
+   stmia\\t%0, {%1}
+   ldr\\t%0, %1
+   str\\t%1, %0
+   mov\\t%0, %1"
+  [(set_attr "length" "2,2,4,4,2,2,2,2,2")
+   (set_attr "type" "mov_reg,mov_imm,multiple,multiple,load1,store1,load1,store1,mov_reg")
+   (set_attr "pool_range" "*,*,*,*,*,*,1018,*,*")
+   (set_attr "conds" "set,clob,*,*,nocond,nocond,nocond,nocond,nocond")])
+
+; Split the load of 64-bit constant into two loads for high and low 32-bit parts respectively
+; to see if we can load them in fewer instructions or fewer cycles.
+; For the small 64-bit integer constants that satisfy constraint J, the instruction pattern
+; thumb1_movdi_insn has a better way to handle them.
+(define_split
+  [(set (match_operand:ANY64 0 "arm_general_register_operand" "")
+       (match_operand:ANY64 1 "immediate_operand" ""))]
+  "TARGET_THUMB1 && reload_completed && !satisfies_constraint_J (operands[1])"
+  [(set (match_dup 0) (match_dup 1))
+   (set (match_dup 2) (match_dup 3))]
+  "
+  operands[2] = gen_highpart (SImode, operands[0]);
+  operands[3] = gen_highpart_mode (SImode, GET_MODE (operands[0]),
+                                  operands[1]);
+  operands[0] = gen_lowpart (SImode, operands[0]);
+  operands[1] = gen_lowpart (SImode, operands[1]);
+  "
+)
+
+(define_split
+  [(set (match_operand:SI 0 "register_operand" "")
+	(match_operand:SI 1 "const_int_operand" ""))]
+  "TARGET_THUMB1 && satisfies_constraint_J (operands[1])"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (neg:SI (match_dup 2)))]
+  "
+  {
+    operands[1] = GEN_INT (- INTVAL (operands[1]));
+    operands[2] = can_create_pseudo_p () ? gen_reg_rtx (SImode) : operands[0];
+  }"
+)
+
+(define_split
+  [(set (match_operand:SI 0 "register_operand" "")
+	(match_operand:SI 1 "const_int_operand" ""))]
+  "TARGET_THUMB1 && satisfies_constraint_K (operands[1])"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (ashift:SI (match_dup 2) (match_dup 3)))]
+  "
+  {
+    unsigned HOST_WIDE_INT val = INTVAL (operands[1]) & 0xffffffffu;
+    unsigned HOST_WIDE_INT mask = 0xff;
+    int i;
+
+    for (i = 0; i < 25; i++)
+      if ((val & (mask << i)) == val)
+        break;
+
+    /* Don't split if the shift is zero.  */
+    if (i == 0)
+      FAIL;
+
+    operands[1] = GEN_INT (val >> i);
+    operands[2] = can_create_pseudo_p () ? gen_reg_rtx (SImode) : operands[0];
+    operands[3] = GEN_INT (i);
+  }"
+)
+
+;; For thumb1 split imm move [256-510] into mov [1-255] and add #255
+(define_split
+  [(set (match_operand:SI 0 "register_operand" "")
+	(match_operand:SI 1 "const_int_operand" ""))]
+  "TARGET_THUMB1 && satisfies_constraint_Pe (operands[1])"
+  [(set (match_dup 2) (match_dup 1))
+   (set (match_dup 0) (plus:SI (match_dup 2) (match_dup 3)))]
+  "
+  {
+    operands[1] = GEN_INT (INTVAL (operands[1]) - 255);
+    operands[2] = can_create_pseudo_p () ? gen_reg_rtx (SImode) : operands[0];
+    operands[3] = GEN_INT (255);
+  }"
+)
+
+(define_insn "*thumb1_movhi_insn"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=l,l,m,*r,*h,l")
+	(match_operand:HI 1 "general_operand"       "l,m,l,*h,*r,I"))]
+  "TARGET_THUMB1
+   && (   register_operand (operands[0], HImode)
+       || register_operand (operands[1], HImode))"
+  "*
+  switch (which_alternative)
+    {
+    case 0: return \"add	%0, %1, #0\";
+    case 2: return \"strh	%1, %0\";
+    case 3: return \"mov	%0, %1\";
+    case 4: return \"mov	%0, %1\";
+    case 5: return \"mov	%0, %1\";
+    default: gcc_unreachable ();
+    case 1:
+      /* The stack pointer can end up being taken as an index register.
+          Catch this case here and deal with it.  */
+      if (GET_CODE (XEXP (operands[1], 0)) == PLUS
+	  && REG_P (XEXP (XEXP (operands[1], 0), 0))
+	  && REGNO    (XEXP (XEXP (operands[1], 0), 0)) == SP_REGNUM)
+        {
+	  rtx ops[2];
+          ops[0] = operands[0];
+          ops[1] = XEXP (XEXP (operands[1], 0), 0);
+
+          output_asm_insn (\"mov	%0, %1\", ops);
+
+          XEXP (XEXP (operands[1], 0), 0) = operands[0];
+
+	}
+      return \"ldrh	%0, %1\";
+    }"
+  [(set_attr "length" "2,4,2,2,2,2")
+   (set_attr "type" "alus_imm,load1,store1,mov_reg,mov_reg,mov_imm")
+   (set_attr "conds" "clob,nocond,nocond,nocond,nocond,clob")])
+
+(define_expand "thumb_movhi_clobber"
+  [(set (match_operand:HI     0 "memory_operand"   "")
+	(match_operand:HI     1 "register_operand" ""))
+   (clobber (match_operand:DI 2 "register_operand" ""))]
+  "TARGET_THUMB1"
+  "
+  if (strict_memory_address_p (HImode, XEXP (operands[0], 0))
+      && REGNO (operands[1]) <= LAST_LO_REGNUM)
+    {
+      emit_insn (gen_movhi (operands[0], operands[1]));
+      DONE;
+    }
+  /* XXX Fixme, need to handle other cases here as well.  */
+  gcc_unreachable ();
+  "
+)
+
+(define_insn "*thumb1_movqi_insn"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=l,l,m,*r,*h,l")
+	(match_operand:QI 1 "general_operand"      "l, m,l,*h,*r,I"))]
+  "TARGET_THUMB1
+   && (   register_operand (operands[0], QImode)
+       || register_operand (operands[1], QImode))"
+  "@
+   add\\t%0, %1, #0
+   ldrb\\t%0, %1
+   strb\\t%1, %0
+   mov\\t%0, %1
+   mov\\t%0, %1
+   mov\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "type" "alu_imm,load1,store1,mov_reg,mov_imm,mov_imm")
+   (set_attr "pool_range" "*,32,*,*,*,*")
+   (set_attr "conds" "clob,nocond,nocond,nocond,nocond,clob")])
+
+(define_insn "*thumb1_movhf"
+  [(set (match_operand:HF     0 "nonimmediate_operand" "=l,l,m,*r,*h")
+	(match_operand:HF     1 "general_operand"      "l,mF,l,*h,*r"))]
+  "TARGET_THUMB1
+   && (	  s_register_operand (operands[0], HFmode)
+       || s_register_operand (operands[1], HFmode))"
+  "*
+  switch (which_alternative)
+    {
+    case 1:
+      {
+	rtx addr;
+	gcc_assert (MEM_P (operands[1]));
+	addr = XEXP (operands[1], 0);
+	if (GET_CODE (addr) == LABEL_REF
+	    || (GET_CODE (addr) == CONST
+		&& GET_CODE (XEXP (addr, 0)) == PLUS
+		&& GET_CODE (XEXP (XEXP (addr, 0), 0)) == LABEL_REF
+		&& CONST_INT_P (XEXP (XEXP (addr, 0), 1))))
+	  {
+	    /* Constant pool entry.  */
+	    return \"ldr\\t%0, %1\";
+	  }
+	return \"ldrh\\t%0, %1\";
+      }
+    case 2: return \"strh\\t%1, %0\";
+    default: return \"mov\\t%0, %1\";
+    }
+  "
+  [(set_attr "length" "2")
+   (set_attr "type" "mov_reg,load1,store1,mov_reg,mov_reg")
+   (set_attr "pool_range" "*,1018,*,*,*")
+   (set_attr "conds" "clob,nocond,nocond,nocond,nocond")])
+;;; ??? This should have alternatives for constants.
+(define_insn "*thumb1_movsf_insn"
+  [(set (match_operand:SF     0 "nonimmediate_operand" "=l,l,>,l, m,*r,*h")
+	(match_operand:SF     1 "general_operand"      "l, >,l,mF,l,*h,*r"))]
+  "TARGET_THUMB1
+   && (   register_operand (operands[0], SFmode)
+       || register_operand (operands[1], SFmode))"
+  "@
+   add\\t%0, %1, #0
+   ldmia\\t%1, {%0}
+   stmia\\t%0, {%1}
+   ldr\\t%0, %1
+   str\\t%1, %0
+   mov\\t%0, %1
+   mov\\t%0, %1"
+  [(set_attr "length" "2")
+   (set_attr "type" "alus_imm,load1,store1,load1,store1,mov_reg,mov_reg")
+   (set_attr "pool_range" "*,*,*,1018,*,*,*")
+   (set_attr "conds" "clob,nocond,nocond,nocond,nocond,nocond,nocond")]
+)
+
+;;; ??? This should have alternatives for constants.
+;;; ??? This was originally identical to the movdi_insn pattern.
+;;; ??? The 'F' constraint looks funny, but it should always be replaced by
+;;; thumb_reorg with a memory reference.
+(define_insn "*thumb_movdf_insn"
+  [(set (match_operand:DF 0 "nonimmediate_operand" "=l,l,>,l, m,*r")
+	(match_operand:DF 1 "general_operand"      "l, >,l,mF,l,*r"))]
+  "TARGET_THUMB1
+   && (   register_operand (operands[0], DFmode)
+       || register_operand (operands[1], DFmode))"
+  "*
+  switch (which_alternative)
+    {
+    default:
+    case 0:
+      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
+	return \"add\\t%0, %1, #0\;add\\t%H0, %H1, #0\";
+      return \"add\\t%H0, %H1, #0\;add\\t%0, %1, #0\";
+    case 1:
+      return \"ldmia\\t%1, {%0, %H0}\";
+    case 2:
+      return \"stmia\\t%0, {%1, %H1}\";
+    case 3:
+      return thumb_load_double_from_address (operands);
+    case 4:
+      operands[2] = gen_rtx_MEM (SImode,
+				 plus_constant (Pmode,
+						XEXP (operands[0], 0), 4));
+      output_asm_insn (\"str\\t%1, %0\;str\\t%H1, %2\", operands);
+      return \"\";
+    case 5:
+      if (REGNO (operands[1]) == REGNO (operands[0]) + 1)
+	return \"mov\\t%0, %1\;mov\\t%H0, %H1\";
+      return \"mov\\t%H0, %H1\;mov\\t%0, %1\";
+    }
+  "
+  [(set_attr "length" "4,2,2,6,4,4")
+   (set_attr "type" "multiple,load2,store2,load2,store2,multiple")
+   (set_attr "pool_range" "*,*,*,1018,*,*")]
+)
+
+
+;; Thumb block-move insns
+
+(define_insn "movmem12b"
+  [(set (mem:SI (match_operand:SI 2 "register_operand" "0"))
+	(mem:SI (match_operand:SI 3 "register_operand" "1")))
+   (set (mem:SI (plus:SI (match_dup 2) (const_int 4)))
+	(mem:SI (plus:SI (match_dup 3) (const_int 4))))
+   (set (mem:SI (plus:SI (match_dup 2) (const_int 8)))
+	(mem:SI (plus:SI (match_dup 3) (const_int 8))))
+   (set (match_operand:SI 0 "register_operand" "=l")
+	(plus:SI (match_dup 2) (const_int 12)))
+   (set (match_operand:SI 1 "register_operand" "=l")
+	(plus:SI (match_dup 3) (const_int 12)))
+   (clobber (match_scratch:SI 4 "=&l"))
+   (clobber (match_scratch:SI 5 "=&l"))
+   (clobber (match_scratch:SI 6 "=&l"))]
+  "TARGET_THUMB1"
+  "* return thumb_output_move_mem_multiple (3, operands);"
+  [(set_attr "length" "4")
+   ; This isn't entirely accurate...  It loads as well, but in terms of
+   ; scheduling the following insn it is better to consider it as a store
+   (set_attr "type" "store3")]
+)
+
+(define_insn "movmem8b"
+  [(set (mem:SI (match_operand:SI 2 "register_operand" "0"))
+	(mem:SI (match_operand:SI 3 "register_operand" "1")))
+   (set (mem:SI (plus:SI (match_dup 2) (const_int 4)))
+	(mem:SI (plus:SI (match_dup 3) (const_int 4))))
+   (set (match_operand:SI 0 "register_operand" "=l")
+	(plus:SI (match_dup 2) (const_int 8)))
+   (set (match_operand:SI 1 "register_operand" "=l")
+	(plus:SI (match_dup 3) (const_int 8)))
+   (clobber (match_scratch:SI 4 "=&l"))
+   (clobber (match_scratch:SI 5 "=&l"))]
+  "TARGET_THUMB1"
+  "* return thumb_output_move_mem_multiple (2, operands);"
+  [(set_attr "length" "4")
+   ; This isn't entirely accurate...  It loads as well, but in terms of
+   ; scheduling the following insn it is better to consider it as a store
+   (set_attr "type" "store2")]
+)
+
+
+;; A pattern to recognize a special situation and optimize for it.
+;; On the thumb, zero-extension from memory is preferrable to sign-extension
+;; due to the available addressing modes.  Hence, convert a signed comparison
+;; with zero into an unsigned comparison with 127 if possible.
+(define_expand "cbranchqi4"
+  [(set (pc) (if_then_else
+	      (match_operator 0 "lt_ge_comparison_operator"
+	       [(match_operand:QI 1 "memory_operand" "")
+	        (match_operand:QI 2 "const0_operand" "")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))]
+  "TARGET_THUMB1"
+{
+  rtx xops[4];
+  xops[1] = gen_reg_rtx (SImode);
+  emit_insn (gen_zero_extendqisi2 (xops[1], operands[1]));
+  xops[2] = GEN_INT (127);
+  xops[0] = gen_rtx_fmt_ee (GET_CODE (operands[0]) == GE ? LEU : GTU,
+			    VOIDmode, xops[1], xops[2]);
+  xops[3] = operands[3];
+  emit_insn (gen_cbranchsi4 (xops[0], xops[1], xops[2], xops[3]));
+  DONE;
+})
+
+(define_insn "cbranchsi4_insn"
+  [(set (pc) (if_then_else
+	      (match_operator 0 "arm_comparison_operator"
+	       [(match_operand:SI 1 "s_register_operand" "l,l*h")
+	        (match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))]
+  "TARGET_THUMB1"
+{
+  rtx t = cfun->machine->thumb1_cc_insn;
+  if (t != NULL_RTX)
+    {
+      if (!rtx_equal_p (cfun->machine->thumb1_cc_op0, operands[1])
+	  || !rtx_equal_p (cfun->machine->thumb1_cc_op1, operands[2]))
+	t = NULL_RTX;
+      if (cfun->machine->thumb1_cc_mode == CC_NOOVmode)
+	{
+	  if (!noov_comparison_operator (operands[0], VOIDmode))
+	    t = NULL_RTX;
+	}
+      else if (cfun->machine->thumb1_cc_mode != CCmode)
+	t = NULL_RTX;
+    }
+  if (t == NULL_RTX)
+    {
+      output_asm_insn ("cmp\t%1, %2", operands);
+      cfun->machine->thumb1_cc_insn = insn;
+      cfun->machine->thumb1_cc_op0 = operands[1];
+      cfun->machine->thumb1_cc_op1 = operands[2];
+      cfun->machine->thumb1_cc_mode = CCmode;
+    }
+  else
+    /* Ensure we emit the right type of condition code on the jump.  */
+    XEXP (operands[0], 0) = gen_rtx_REG (cfun->machine->thumb1_cc_mode,
+					 CC_REGNUM);
+
+  switch (get_attr_length (insn))
+    {
+    case 4:  return \"b%d0\\t%l3\";
+    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
+    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
+    }
+}
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
+	         (le (minus (match_dup 3) (pc)) (const_int 256)))
+	    (const_int 4)
+	    (if_then_else
+	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
+		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
+		(const_int 6)
+		(const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "cbranchsi4_scratch"
+  [(set (pc) (if_then_else
+	      (match_operator 4 "arm_comparison_operator"
+	       [(match_operand:SI 1 "s_register_operand" "l,0")
+	        (match_operand:SI 2 "thumb1_cmpneg_operand" "L,J")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))
+   (clobber (match_scratch:SI 0 "=l,l"))]
+  "TARGET_THUMB1"
+  "*
+  output_asm_insn (\"add\\t%0, %1, #%n2\", operands);
+
+  switch (get_attr_length (insn))
+    {
+    case 4:  return \"b%d4\\t%l3\";
+    case 6:  return \"b%D4\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
+    default: return \"b%D4\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
+    }
+  "
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
+	         (le (minus (match_dup 3) (pc)) (const_int 256)))
+	    (const_int 4)
+	    (if_then_else
+	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
+		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
+		(const_int 6)
+		(const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*negated_cbranchsi4"
+  [(set (pc)
+	(if_then_else
+	 (match_operator 0 "equality_operator"
+	  [(match_operand:SI 1 "s_register_operand" "l")
+	   (neg:SI (match_operand:SI 2 "s_register_operand" "l"))])
+	 (label_ref (match_operand 3 "" ""))
+	 (pc)))]
+  "TARGET_THUMB1"
+  "*
+  output_asm_insn (\"cmn\\t%1, %2\", operands);
+  switch (get_attr_length (insn))
+    {
+    case 4:  return \"b%d0\\t%l3\";
+    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
+    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
+    }
+  "
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
+	         (le (minus (match_dup 3) (pc)) (const_int 256)))
+	    (const_int 4)
+	    (if_then_else
+	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
+		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
+		(const_int 6)
+		(const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*tbit_cbranch"
+  [(set (pc)
+	(if_then_else
+	 (match_operator 0 "equality_operator"
+	  [(zero_extract:SI (match_operand:SI 1 "s_register_operand" "l")
+			    (const_int 1)
+			    (match_operand:SI 2 "const_int_operand" "i"))
+	   (const_int 0)])
+	 (label_ref (match_operand 3 "" ""))
+	 (pc)))
+   (clobber (match_scratch:SI 4 "=l"))]
+  "TARGET_THUMB1"
+  "*
+  {
+  rtx op[3];
+  op[0] = operands[4];
+  op[1] = operands[1];
+  op[2] = GEN_INT (32 - 1 - INTVAL (operands[2]));
+
+  output_asm_insn (\"lsl\\t%0, %1, %2\", op);
+  switch (get_attr_length (insn))
+    {
+    case 4:  return \"b%d0\\t%l3\";
+    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
+    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
+    }
+  }"
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
+	         (le (minus (match_dup 3) (pc)) (const_int 256)))
+	    (const_int 4)
+	    (if_then_else
+	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
+		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
+		(const_int 6)
+		(const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*tlobits_cbranch"
+  [(set (pc)
+	(if_then_else
+	 (match_operator 0 "equality_operator"
+	  [(zero_extract:SI (match_operand:SI 1 "s_register_operand" "l")
+			    (match_operand:SI 2 "const_int_operand" "i")
+			    (const_int 0))
+	   (const_int 0)])
+	 (label_ref (match_operand 3 "" ""))
+	 (pc)))
+   (clobber (match_scratch:SI 4 "=l"))]
+  "TARGET_THUMB1"
+  "*
+  {
+  rtx op[3];
+  op[0] = operands[4];
+  op[1] = operands[1];
+  op[2] = GEN_INT (32 - INTVAL (operands[2]));
+
+  output_asm_insn (\"lsl\\t%0, %1, %2\", op);
+  switch (get_attr_length (insn))
+    {
+    case 4:  return \"b%d0\\t%l3\";
+    case 6:  return \"b%D0\\t.LCB%=\;b\\t%l3\\t%@long jump\\n.LCB%=:\";
+    default: return \"b%D0\\t.LCB%=\;bl\\t%l3\\t%@far jump\\n.LCB%=:\";
+    }
+  }"
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 3) (pc)) (const_int -250))
+	         (le (minus (match_dup 3) (pc)) (const_int 256)))
+	    (const_int 4)
+	    (if_then_else
+	        (and (ge (minus (match_dup 3) (pc)) (const_int -2040))
+		     (le (minus (match_dup 3) (pc)) (const_int 2048)))
+		(const_int 6)
+		(const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*tstsi3_cbranch"
+  [(set (pc)
+	(if_then_else
+	 (match_operator 3 "equality_operator"
+	  [(and:SI (match_operand:SI 0 "s_register_operand" "%l")
+		   (match_operand:SI 1 "s_register_operand" "l"))
+	   (const_int 0)])
+	 (label_ref (match_operand 2 "" ""))
+	 (pc)))]
+  "TARGET_THUMB1"
+  "*
+  {
+  output_asm_insn (\"tst\\t%0, %1\", operands);
+  switch (get_attr_length (insn))
+    {
+    case 4:  return \"b%d3\\t%l2\";
+    case 6:  return \"b%D3\\t.LCB%=\;b\\t%l2\\t%@long jump\\n.LCB%=:\";
+    default: return \"b%D3\\t.LCB%=\;bl\\t%l2\\t%@far jump\\n.LCB%=:\";
+    }
+  }"
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 2) (pc)) (const_int -250))
+	         (le (minus (match_dup 2) (pc)) (const_int 256)))
+	    (const_int 4)
+	    (if_then_else
+	        (and (ge (minus (match_dup 2) (pc)) (const_int -2040))
+		     (le (minus (match_dup 2) (pc)) (const_int 2048)))
+		(const_int 6)
+		(const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*cbranchne_decr1"
+  [(set (pc)
+	(if_then_else (match_operator 3 "equality_operator"
+		       [(match_operand:SI 2 "s_register_operand" "l,l,1,l")
+		        (const_int 0)])
+		      (label_ref (match_operand 4 "" ""))
+		      (pc)))
+   (set (match_operand:SI 0 "thumb_cbrch_target_operand" "=l,*?h,*?m,*?m")
+	(plus:SI (match_dup 2) (const_int -1)))
+   (clobber (match_scratch:SI 1 "=X,l,&l,&l"))]
+  "TARGET_THUMB1"
+  "*
+   {
+     rtx cond[2];
+     cond[0] = gen_rtx_fmt_ee ((GET_CODE (operands[3]) == NE
+				? GEU : LTU),
+			       VOIDmode, operands[2], const1_rtx);
+     cond[1] = operands[4];
+
+     if (which_alternative == 0)
+       output_asm_insn (\"sub\\t%0, %2, #1\", operands);
+     else if (which_alternative == 1)
+       {
+	 /* We must provide an alternative for a hi reg because reload
+	    cannot handle output reloads on a jump instruction, but we
+	    can't subtract into that.  Fortunately a mov from lo to hi
+	    does not clobber the condition codes.  */
+	 output_asm_insn (\"sub\\t%1, %2, #1\", operands);
+	 output_asm_insn (\"mov\\t%0, %1\", operands);
+       }
+     else
+       {
+	 /* Similarly, but the target is memory.  */
+	 output_asm_insn (\"sub\\t%1, %2, #1\", operands);
+	 output_asm_insn (\"str\\t%1, %0\", operands);
+       }
+
+     switch (get_attr_length (insn) - (which_alternative ? 2 : 0))
+       {
+	 case 4:
+	   output_asm_insn (\"b%d0\\t%l1\", cond);
+	   return \"\";
+	 case 6:
+	   output_asm_insn (\"b%D0\\t.LCB%=\", cond);
+	   return \"b\\t%l4\\t%@long jump\\n.LCB%=:\";
+	 default:
+	   output_asm_insn (\"b%D0\\t.LCB%=\", cond);
+	   return \"bl\\t%l4\\t%@far jump\\n.LCB%=:\";
+       }
+   }
+  "
+  [(set (attr "far_jump")
+        (if_then_else
+	    (ior (and (eq (symbol_ref ("which_alternative"))
+	                  (const_int 0))
+		      (eq_attr "length" "8"))
+		 (eq_attr "length" "10"))
+	    (const_string "yes")
+            (const_string "no")))
+   (set_attr_alternative "length"
+      [
+       ;; Alternative 0
+       (if_then_else
+	 (and (ge (minus (match_dup 4) (pc)) (const_int -250))
+	      (le (minus (match_dup 4) (pc)) (const_int 256)))
+	 (const_int 4)
+	 (if_then_else
+	   (and (ge (minus (match_dup 4) (pc)) (const_int -2040))
+		(le (minus (match_dup 4) (pc)) (const_int 2048)))
+	   (const_int 6)
+	   (const_int 8)))
+       ;; Alternative 1
+       (if_then_else
+	 (and (ge (minus (match_dup 4) (pc)) (const_int -248))
+	      (le (minus (match_dup 4) (pc)) (const_int 256)))
+	 (const_int 6)
+	 (if_then_else
+	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
+		(le (minus (match_dup 4) (pc)) (const_int 2048)))
+	   (const_int 8)
+	   (const_int 10)))
+       ;; Alternative 2
+       (if_then_else
+	 (and (ge (minus (match_dup 4) (pc)) (const_int -248))
+	      (le (minus (match_dup 4) (pc)) (const_int 256)))
+	 (const_int 6)
+	 (if_then_else
+	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
+		(le (minus (match_dup 4) (pc)) (const_int 2048)))
+	   (const_int 8)
+	   (const_int 10)))
+       ;; Alternative 3
+       (if_then_else
+	 (and (ge (minus (match_dup 4) (pc)) (const_int -248))
+	      (le (minus (match_dup 4) (pc)) (const_int 256)))
+	 (const_int 6)
+	 (if_then_else
+	   (and (ge (minus (match_dup 4) (pc)) (const_int -2038))
+		(le (minus (match_dup 4) (pc)) (const_int 2048)))
+	   (const_int 8)
+	   (const_int 10)))])
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*addsi3_cbranch"
+  [(set (pc)
+	(if_then_else
+	 (match_operator 4 "arm_comparison_operator"
+	  [(plus:SI
+	    (match_operand:SI 2 "s_register_operand" "%0,l,*l,1,1,1")
+	    (match_operand:SI 3 "reg_or_int_operand" "IJ,lL,*l,lIJ,lIJ,lIJ"))
+	   (const_int 0)])
+	 (label_ref (match_operand 5 "" ""))
+	 (pc)))
+   (set
+    (match_operand:SI 0 "thumb_cbrch_target_operand" "=l,l,*!h,*?h,*?m,*?m")
+    (plus:SI (match_dup 2) (match_dup 3)))
+   (clobber (match_scratch:SI 1 "=X,X,l,l,&l,&l"))]
+  "TARGET_THUMB1
+   && (GET_CODE (operands[4]) == EQ
+       || GET_CODE (operands[4]) == NE
+       || GET_CODE (operands[4]) == GE
+       || GET_CODE (operands[4]) == LT)"
+  "*
+   {
+     rtx cond[3];
+
+     cond[0] = (which_alternative < 2) ? operands[0] : operands[1];
+     cond[1] = operands[2];
+     cond[2] = operands[3];
+
+     if (CONST_INT_P (cond[2]) && INTVAL (cond[2]) < 0)
+       output_asm_insn (\"sub\\t%0, %1, #%n2\", cond);
+     else
+       output_asm_insn (\"add\\t%0, %1, %2\", cond);
+
+     if (which_alternative >= 2
+	 && which_alternative < 4)
+       output_asm_insn (\"mov\\t%0, %1\", operands);
+     else if (which_alternative >= 4)
+       output_asm_insn (\"str\\t%1, %0\", operands);
+
+     switch (get_attr_length (insn) - ((which_alternative >= 2) ? 2 : 0))
+       {
+	 case 4:
+	   return \"b%d4\\t%l5\";
+	 case 6:
+	   return \"b%D4\\t.LCB%=\;b\\t%l5\\t%@long jump\\n.LCB%=:\";
+	 default:
+	   return \"b%D4\\t.LCB%=\;bl\\t%l5\\t%@far jump\\n.LCB%=:\";
+       }
+   }
+  "
+  [(set (attr "far_jump")
+        (if_then_else
+	    (ior (and (lt (symbol_ref ("which_alternative"))
+	                  (const_int 2))
+		      (eq_attr "length" "8"))
+		 (eq_attr "length" "10"))
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+     (if_then_else
+       (lt (symbol_ref ("which_alternative"))
+		       (const_int 2))
+       (if_then_else
+	 (and (ge (minus (match_dup 5) (pc)) (const_int -250))
+	      (le (minus (match_dup 5) (pc)) (const_int 256)))
+	 (const_int 4)
+	 (if_then_else
+	   (and (ge (minus (match_dup 5) (pc)) (const_int -2040))
+		(le (minus (match_dup 5) (pc)) (const_int 2048)))
+	   (const_int 6)
+	   (const_int 8)))
+       (if_then_else
+	 (and (ge (minus (match_dup 5) (pc)) (const_int -248))
+	      (le (minus (match_dup 5) (pc)) (const_int 256)))
+	 (const_int 6)
+	 (if_then_else
+	   (and (ge (minus (match_dup 5) (pc)) (const_int -2038))
+		(le (minus (match_dup 5) (pc)) (const_int 2048)))
+	   (const_int 8)
+	   (const_int 10)))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*addsi3_cbranch_scratch"
+  [(set (pc)
+	(if_then_else
+	 (match_operator 3 "arm_comparison_operator"
+	  [(plus:SI
+	    (match_operand:SI 1 "s_register_operand" "%l,l,l,0")
+	    (match_operand:SI 2 "reg_or_int_operand" "J,l,L,IJ"))
+	   (const_int 0)])
+	 (label_ref (match_operand 4 "" ""))
+	 (pc)))
+   (clobber (match_scratch:SI 0 "=X,X,l,l"))]
+  "TARGET_THUMB1
+   && (GET_CODE (operands[3]) == EQ
+       || GET_CODE (operands[3]) == NE
+       || GET_CODE (operands[3]) == GE
+       || GET_CODE (operands[3]) == LT)"
+  "*
+   {
+     switch (which_alternative)
+       {
+       case 0:
+	 output_asm_insn (\"cmp\t%1, #%n2\", operands);
+	 break;
+       case 1:
+	 output_asm_insn (\"cmn\t%1, %2\", operands);
+	 break;
+       case 2:
+	 if (INTVAL (operands[2]) < 0)
+	   output_asm_insn (\"sub\t%0, %1, %2\", operands);
+	 else
+	   output_asm_insn (\"add\t%0, %1, %2\", operands);
+	 break;
+       case 3:
+	 if (INTVAL (operands[2]) < 0)
+	   output_asm_insn (\"sub\t%0, %0, %2\", operands);
+	 else
+	   output_asm_insn (\"add\t%0, %0, %2\", operands);
+	 break;
+       }
+
+     switch (get_attr_length (insn))
+       {
+	 case 4:
+	   return \"b%d3\\t%l4\";
+	 case 6:
+	   return \"b%D3\\t.LCB%=\;b\\t%l4\\t%@long jump\\n.LCB%=:\";
+	 default:
+	   return \"b%D3\\t.LCB%=\;bl\\t%l4\\t%@far jump\\n.LCB%=:\";
+       }
+   }
+  "
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "8")
+	    (const_string "yes")
+            (const_string "no")))
+   (set (attr "length")
+       (if_then_else
+	 (and (ge (minus (match_dup 4) (pc)) (const_int -250))
+	      (le (minus (match_dup 4) (pc)) (const_int 256)))
+	 (const_int 4)
+	 (if_then_else
+	   (and (ge (minus (match_dup 4) (pc)) (const_int -2040))
+		(le (minus (match_dup 4) (pc)) (const_int 2048)))
+	   (const_int 6)
+	   (const_int 8))))
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*thumb_cmpdi_zero"
+  [(set (reg:CC_Z CC_REGNUM)
+	(compare:CC_Z (match_operand:DI 0 "s_register_operand" "l")
+		      (const_int 0)))
+   (clobber (match_scratch:SI 1 "=l"))]
+  "TARGET_THUMB1"
+  "orr\\t%1, %Q0, %R0"
+  [(set_attr "conds" "set")
+   (set_attr "length" "2")
+   (set_attr "type" "logics_reg")]
+)
+
+(define_expand "cstoresi_eq0_thumb1"
+  [(parallel
+    [(set (match_operand:SI 0 "s_register_operand" "")
+	  (eq:SI (match_operand:SI 1 "s_register_operand" "")
+		 (const_int 0)))
+     (clobber (match_dup:SI 2))])]
+  "TARGET_THUMB1"
+  "operands[2] = gen_reg_rtx (SImode);"
+)
+
+(define_expand "cstoresi_ne0_thumb1"
+  [(parallel
+    [(set (match_operand:SI 0 "s_register_operand" "")
+	  (ne:SI (match_operand:SI 1 "s_register_operand" "")
+		 (const_int 0)))
+     (clobber (match_dup:SI 2))])]
+  "TARGET_THUMB1"
+  "operands[2] = gen_reg_rtx (SImode);"
+)
+
+(define_insn "*cstoresi_eq0_thumb1_insn"
+  [(set (match_operand:SI 0 "s_register_operand" "=&l,l")
+	(eq:SI (match_operand:SI 1 "s_register_operand" "l,0")
+	       (const_int 0)))
+   (clobber (match_operand:SI 2 "s_register_operand" "=X,l"))]
+  "TARGET_THUMB1"
+  "@
+   neg\\t%0, %1\;adc\\t%0, %0, %1
+   neg\\t%2, %1\;adc\\t%0, %1, %2"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*cstoresi_ne0_thumb1_insn"
+  [(set (match_operand:SI 0 "s_register_operand" "=l")
+	(ne:SI (match_operand:SI 1 "s_register_operand" "0")
+	       (const_int 0)))
+   (clobber (match_operand:SI 2 "s_register_operand" "=l"))]
+  "TARGET_THUMB1"
+  "sub\\t%2, %1, #1\;sbc\\t%0, %1, %2"
+  [(set_attr "length" "4")]
+)
+
+;; Used as part of the expansion of thumb ltu and gtu sequences
+(define_insn "cstoresi_nltu_thumb1"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,l")
+        (neg:SI (ltu:SI (match_operand:SI 1 "s_register_operand" "l,*h")
+			(match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r"))))]
+  "TARGET_THUMB1"
+  "cmp\\t%1, %2\;sbc\\t%0, %0, %0"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn_and_split "cstoresi_ltu_thumb1"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,l")
+        (ltu:SI (match_operand:SI 1 "s_register_operand" "l,*h")
+		(match_operand:SI 2 "thumb1_cmp_operand" "lI*h,*r")))]
+  "TARGET_THUMB1"
+  "#"
+  "TARGET_THUMB1"
+  [(set (match_dup 3)
+	(neg:SI (ltu:SI (match_dup 1) (match_dup 2))))
+   (set (match_dup 0) (neg:SI (match_dup 3)))]
+  "operands[3] = gen_reg_rtx (SImode);"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+;; Used as part of the expansion of thumb les sequence.
+(define_insn "thumb1_addsi3_addgeu"
+  [(set (match_operand:SI 0 "s_register_operand" "=l")
+        (plus:SI (plus:SI (match_operand:SI 1 "s_register_operand" "%0")
+			  (match_operand:SI 2 "s_register_operand" "l"))
+		 (geu:SI (match_operand:SI 3 "s_register_operand" "l")
+			 (match_operand:SI 4 "thumb1_cmp_operand" "lI"))))]
+  "TARGET_THUMB1"
+  "cmp\\t%3, %4\;adc\\t%0, %1, %2"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+
+(define_insn "*thumb_jump"
+  [(set (pc)
+	(label_ref (match_operand 0 "" "")))]
+  "TARGET_THUMB1"
+  "*
+  if (get_attr_length (insn) == 2)
+    return \"b\\t%l0\";
+  return \"bl\\t%l0\\t%@ far jump\";
+  "
+  [(set (attr "far_jump")
+        (if_then_else
+	    (eq_attr "length" "4")
+	    (const_string "yes")
+	    (const_string "no")))
+   (set (attr "length")
+        (if_then_else
+	    (and (ge (minus (match_dup 0) (pc)) (const_int -2044))
+		 (le (minus (match_dup 0) (pc)) (const_int 2048)))
+  	    (const_int 2)
+	    (const_int 4)))
+   (set_attr "type" "branch")]
+)
+
+(define_insn "*call_reg_thumb1_v5"
+  [(call (mem:SI (match_operand:SI 0 "register_operand" "l*r"))
+	 (match_operand 1 "" ""))
+   (use (match_operand 2 "" ""))
+   (clobber (reg:SI LR_REGNUM))]
+  "TARGET_THUMB1 && arm_arch5 && !SIBLING_CALL_P (insn)"
+  "blx\\t%0"
+  [(set_attr "length" "2")
+   (set_attr "type" "call")]
+)
+
+(define_insn "*call_reg_thumb1"
+  [(call (mem:SI (match_operand:SI 0 "register_operand" "l*r"))
+	 (match_operand 1 "" ""))
+   (use (match_operand 2 "" ""))
+   (clobber (reg:SI LR_REGNUM))]
+  "TARGET_THUMB1 && !arm_arch5 && !SIBLING_CALL_P (insn)"
+  "*
+  {
+    if (!TARGET_CALLER_INTERWORKING)
+      return thumb_call_via_reg (operands[0]);
+    else if (operands[1] == const0_rtx)
+      return \"bl\\t%__interwork_call_via_%0\";
+    else if (frame_pointer_needed)
+      return \"bl\\t%__interwork_r7_call_via_%0\";
+    else
+      return \"bl\\t%__interwork_r11_call_via_%0\";
+  }"
+  [(set_attr "type" "call")]
+)
+
+(define_insn "*call_value_reg_thumb1_v5"
+  [(set (match_operand 0 "" "")
+	(call (mem:SI (match_operand:SI 1 "register_operand" "l*r"))
+	      (match_operand 2 "" "")))
+   (use (match_operand 3 "" ""))
+   (clobber (reg:SI LR_REGNUM))]
+  "TARGET_THUMB1 && arm_arch5"
+  "blx\\t%1"
+  [(set_attr "length" "2")
+   (set_attr "type" "call")]
+)
+
+(define_insn "*call_value_reg_thumb1"
+  [(set (match_operand 0 "" "")
+	(call (mem:SI (match_operand:SI 1 "register_operand" "l*r"))
+	      (match_operand 2 "" "")))
+   (use (match_operand 3 "" ""))
+   (clobber (reg:SI LR_REGNUM))]
+  "TARGET_THUMB1 && !arm_arch5"
+  "*
+  {
+    if (!TARGET_CALLER_INTERWORKING)
+      return thumb_call_via_reg (operands[1]);
+    else if (operands[2] == const0_rtx)
+      return \"bl\\t%__interwork_call_via_%1\";
+    else if (frame_pointer_needed)
+      return \"bl\\t%__interwork_r7_call_via_%1\";
+    else
+      return \"bl\\t%__interwork_r11_call_via_%1\";
+  }"
+  [(set_attr "type" "call")]
+)
+
+(define_insn "*call_insn"
+  [(call (mem:SI (match_operand:SI 0 "" ""))
+	 (match_operand:SI 1 "" ""))
+   (use (match_operand 2 "" ""))
+   (clobber (reg:SI LR_REGNUM))]
+  "TARGET_THUMB1
+   && GET_CODE (operands[0]) == SYMBOL_REF
+   && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[0]))"
+  "bl\\t%a0"
+  [(set_attr "length" "4")
+   (set_attr "type" "call")]
+)
+
+(define_insn "*call_value_insn"
+  [(set (match_operand 0 "" "")
+	(call (mem:SI (match_operand 1 "" ""))
+	      (match_operand 2 "" "")))
+   (use (match_operand 3 "" ""))
+   (clobber (reg:SI LR_REGNUM))]
+  "TARGET_THUMB1
+   && GET_CODE (operands[1]) == SYMBOL_REF
+   && !arm_is_long_call_p (SYMBOL_REF_DECL (operands[1]))"
+  "bl\\t%a1"
+  [(set_attr "length" "4")
+   (set_attr "type" "call")]
+)
+
+(define_expand "thumb1_casesi_internal_pic"
+  [(match_operand:SI 0 "s_register_operand" "")
+   (match_operand:SI 1 "thumb1_cmp_operand" "")
+   (match_operand 2 "" "")
+   (match_operand 3 "" "")]
+  "TARGET_THUMB1"
+  {
+    rtx reg0;
+    rtx test = gen_rtx_GTU (VOIDmode, operands[0], operands[1]);
+    emit_jump_insn (gen_cbranchsi4 (test, operands[0], operands[1],
+				    operands[3]));
+    reg0 = gen_rtx_REG (SImode, 0);
+    emit_move_insn (reg0, operands[0]);
+    emit_jump_insn (gen_thumb1_casesi_dispatch (operands[2]/*, operands[3]*/));
+    DONE;
+  }
+)
+
+(define_insn "thumb1_casesi_dispatch"
+  [(parallel [(set (pc) (unspec [(reg:SI 0)
+				 (label_ref (match_operand 0 "" ""))
+;;				 (label_ref (match_operand 1 "" ""))
+]
+			 UNSPEC_THUMB1_CASESI))
+	      (clobber (reg:SI IP_REGNUM))
+              (clobber (reg:SI LR_REGNUM))])]
+  "TARGET_THUMB1"
+  "* return thumb1_output_casesi(operands);"
+  [(set_attr "length" "4")
+   (set_attr "type" "multiple")]
+)
+
+;; NB Never uses BX.
+(define_insn "*thumb1_indirect_jump"
+  [(set (pc)
+	(match_operand:SI 0 "register_operand" "l*r"))]
+  "TARGET_THUMB1"
+  "mov\\tpc, %0"
+  [(set_attr "conds" "clob")
+   (set_attr "length" "2")
+   (set_attr "type" "branch")]
+)
+
+
+(define_insn "prologue_thumb1_interwork"
+  [(unspec_volatile [(const_int 0)] VUNSPEC_THUMB1_INTERWORK)]
+  "TARGET_THUMB1"
+  "* return thumb1_output_interwork ();"
+  [(set_attr "length" "8")
+   (set_attr "type" "multiple")]
+)
+
+(define_insn "*epilogue_insns"
+  [(unspec_volatile [(return)] VUNSPEC_EPILOGUE)]
+  "TARGET_THUMB1"
+  "*
+    return thumb1_unexpanded_epilogue ();
+  "
+  ; Length is absolute worst case
+  [(set_attr "length" "44")
+   (set_attr "type" "block")
+   ;; We don't clobber the conditions, but the potential length of this
+   ;; operation is sufficient to make conditionalizing the sequence
+   ;; unlikely to be profitable.
+   (set_attr "conds" "clob")]
+)
+
+(define_insn "consttable_1"
+  [(unspec_volatile [(match_operand 0 "" "")] VUNSPEC_POOL_1)]
+  "TARGET_THUMB1"
+  "*
+  making_const_table = TRUE;
+  assemble_integer (operands[0], 1, BITS_PER_WORD, 1);
+  assemble_zeros (3);
+  return \"\";
+  "
+  [(set_attr "length" "4")
+   (set_attr "type" "no_insn")]
+)
+
+(define_insn "consttable_2"
+  [(unspec_volatile [(match_operand 0 "" "")] VUNSPEC_POOL_2)]
+  "TARGET_THUMB1"
+  "*
+  making_const_table = TRUE;
+  gcc_assert (GET_MODE_CLASS (GET_MODE (operands[0])) != MODE_FLOAT);
+  assemble_integer (operands[0], 2, BITS_PER_WORD, 1);
+  assemble_zeros (2);
+  return \"\";
+  "
+  [(set_attr "length" "4")
+   (set_attr "type" "no_insn")]
+)
+
+;; Miscellaneous Thumb patterns
+(define_expand "tablejump"
+  [(parallel [(set (pc) (match_operand:SI 0 "register_operand" ""))
+	      (use (label_ref (match_operand 1 "" "")))])]
+  "TARGET_THUMB1"
+  "
+  if (flag_pic)
+    {
+      /* Hopefully, CSE will eliminate this copy.  */
+      rtx reg1 = copy_addr_to_reg (gen_rtx_LABEL_REF (Pmode, operands[1]));
+      rtx reg2 = gen_reg_rtx (SImode);
+
+      emit_insn (gen_addsi3 (reg2, operands[0], reg1));
+      operands[0] = reg2;
+    }
+  "
+)
+
+;; NB never uses BX.
+(define_insn "*thumb1_tablejump"
+  [(set (pc) (match_operand:SI 0 "register_operand" "l*r"))
+   (use (label_ref (match_operand 1 "" "")))]
+  "TARGET_THUMB1"
+  "mov\\t%|pc, %0"
+  [(set_attr "length" "2")
+   (set_attr "type" "no_insn")]
+)
+
+(define_insn_and_split "thumb_eh_return"
+  [(unspec_volatile [(match_operand:SI 0 "s_register_operand" "l")]
+		    VUNSPEC_EH_RETURN)
+   (clobber (match_scratch:SI 1 "=&l"))]
+  "TARGET_THUMB1"
+  "#"
+  "&& reload_completed"
+  [(const_int 0)]
+  "
+  {
+    thumb_set_return_address (operands[0], operands[1]);
+    DONE;
+  }"
+  [(set_attr "type" "mov_reg")]
+)
+
diff --git a/gcc/config/arm/thumb2.md b/gcc/config/arm/thumb2.md
index d84938f..f6b32b1 100644
--- a/gcc/config/arm/thumb2.md
+++ b/gcc/config/arm/thumb2.md
@@ -1117,7 +1117,7 @@
   "%I3%!\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*thumb2_shiftsi3_short"
@@ -1171,7 +1171,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*thumb2_subsi_short"
@@ -1183,7 +1183,7 @@
   "sub%!\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_peephole2
@@ -1236,7 +1236,7 @@
   "
   [(set_attr "conds" "set")
    (set_attr "length" "2,2,4")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*thumb2_addsi3_compare0_scratch"
@@ -1261,7 +1261,7 @@
   "
   [(set_attr "conds" "set")
    (set_attr "length" "2,2,4,4")
-   (set_attr "type" "alus_imm,alus_reg,alus_imm,alus_reg")]
+   (set_attr "type" "alus_imm,alus_sreg,alus_imm,alus_sreg")]
 )
 
 (define_insn "*thumb2_mulsi_short"
@@ -1367,7 +1367,7 @@
   "neg%!\t%0, %1"
   [(set_attr "predicable" "yes")
    (set_attr "length" "2")
-   (set_attr "type" "alu_reg")]
+   (set_attr "type" "alu_sreg")]
 )
 
 (define_insn "*orsi_notsi_si"
diff --git a/gcc/config/arm/types.md b/gcc/config/arm/types.md
index efbf7a7..ff74aaa 100644
--- a/gcc/config/arm/types.md
+++ b/gcc/config/arm/types.md
@@ -35,17 +35,19 @@
 ; alu_imm            any arithmetic instruction that doesn't have a shifted
 ;                    operand and has an immediate operand.  This
 ;                    excludes MOV, MVN and RSB(S) immediate.
-; alu_reg            any arithmetic instruction that doesn't have a shifted
+; alu_sreg           any arithmetic instruction that doesn't have a shifted
 ;                    or an immediate operand.  This excludes
-;                    MOV and MVN but includes MOVT.  This is also the default.
+;                    MOV and MVN but includes MOVT.  This also excludes
+;                    DSP-kind instructions.  This is also the default.
 ; alu_shift_imm      any arithmetic instruction that has a source operand
 ;                    shifted by a constant.  This excludes simple shifts.
 ; alu_shift_reg      as alu_shift_imm, with the shift amount specified in a
 ;                    register.
+; alu_dsp_reg        any DSP-kind instruction like QSUB8.
 ; alus_ext           From ARMv8-A: as alu_ext, setting condition flags.
 ;                    AArch64 Only.
 ; alus_imm           as alu_imm, setting condition flags.
-; alus_reg           as alu_reg, setting condition flags.
+; alus_sreg          as alu_sreg, setting condition flags.
 ; alus_shift_imm     as alu_shift_imm, setting condition flags.
 ; alus_shift_reg     as alu_shift_reg, setting condition flags.
 ; bfm                bitfield move operation.
@@ -540,12 +542,13 @@
   adr,\
   alu_ext,\
   alu_imm,\
-  alu_reg,\
+  alu_sreg,\
   alu_shift_imm,\
   alu_shift_reg,\
+  alu_dsp_reg,\
   alus_ext,\
   alus_imm,\
-  alus_reg,\
+  alus_sreg,\
   alus_shift_imm,\
   alus_shift_reg,\
   bfm,\
diff --git a/gcc/config/arm/vfp.md b/gcc/config/arm/vfp.md
index 7646484..160fa5c 100644
--- a/gcc/config/arm/vfp.md
+++ b/gcc/config/arm/vfp.md
@@ -1285,7 +1285,7 @@
         (unspec:SDF [(match_operand:SDF 1
 		         "register_operand" "<F_constraint>")]
          VRINT))]
-  "TARGET_HARD_FLOAT && TARGET_FPU_ARMV8 <vfp_double_cond>"
+  "TARGET_HARD_FLOAT && TARGET_VFP5 <vfp_double_cond>"
   "vrint<vrint_variant>%?.<V_if_elem>\\t%<V_reg>0, %<V_reg>1"
   [(set_attr "predicable" "<vrint_predicable>")
    (set_attr "predicable_short_it" "no")
@@ -1304,7 +1304,7 @@
   [(set (match_operand:SDF 0 "register_operand" "=<F_constraint>")
         (smax:SDF (match_operand:SDF 1 "register_operand" "<F_constraint>")
 		  (match_operand:SDF 2 "register_operand" "<F_constraint>")))]
-  "TARGET_HARD_FLOAT && TARGET_FPU_ARMV8 <vfp_double_cond>"
+  "TARGET_HARD_FLOAT && TARGET_VFP5 <vfp_double_cond>"
   "vmaxnm.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "f_minmax<vfp_type>")
    (set_attr "conds" "unconditional")]
@@ -1314,7 +1314,7 @@
   [(set (match_operand:SDF 0 "register_operand" "=<F_constraint>")
         (smin:SDF (match_operand:SDF 1 "register_operand" "<F_constraint>")
 		  (match_operand:SDF 2 "register_operand" "<F_constraint>")))]
-  "TARGET_HARD_FLOAT && TARGET_FPU_ARMV8 <vfp_double_cond>"
+  "TARGET_HARD_FLOAT && TARGET_VFP5 <vfp_double_cond>"
   "vminnm.<V_if_elem>\\t%<V_reg>0, %<V_reg>1, %<V_reg>2"
   [(set_attr "type" "f_minmax<vfp_type>")
    (set_attr "conds" "unconditional")]
diff --git a/gcc/configure b/gcc/configure
index f48dd18..58007c5 100755
--- a/gcc/configure
+++ b/gcc/configure
@@ -761,6 +761,7 @@ LN
 LN_S
 AWK
 SET_MAKE
+with_multilib_list
 REPORT_BUGS_TEXI
 REPORT_BUGS_TO
 PKGVERSION
@@ -1688,7 +1689,7 @@ Optional Packages:
   --with-specs=SPECS      add SPECS to driver command-line processing
   --with-pkgversion=PKG   Use PKG in the version string in place of "GCC"
   --with-bugurl=URL       Direct users to URL to report a bug
-  --with-multilib-list    select multilibs (AArch64, SH and x86-64 only)
+  --with-multilib-list    select multilibs (ARM, SH and x86-64 only)
   --with-gnu-ld           assume the C compiler uses GNU ld default=no
   --with-libiconv-prefix[=DIR]  search for libiconv in DIR/include and DIR/lib
   --without-libiconv-prefix     don't search for libiconv in includedir and libdir
@@ -7301,6 +7302,7 @@ else
 fi
 
 
+
 # -------------------------
 # Checks for other programs
 # -------------------------
diff --git a/gcc/configure.ac b/gcc/configure.ac
index e54df10..25d32ea 100644
--- a/gcc/configure.ac
+++ b/gcc/configure.ac
@@ -835,9 +835,10 @@ esac],
 [enable_languages=c])
 
 AC_ARG_WITH(multilib-list,
-[AS_HELP_STRING([--with-multilib-list], [select multilibs (AArch64, SH and x86-64 only)])],
+[AS_HELP_STRING([--with-multilib-list], [select multilibs (ARM, SH and x86-64 only)])],
 :,
 with_multilib_list=default)
+AC_SUBST(with_multilib_list)
 
 # -------------------------
 # Checks for other programs
diff --git a/gcc/ira-color.c b/gcc/ira-color.c
index 1f4c96e..0562367 100644
--- a/gcc/ira-color.c
+++ b/gcc/ira-color.c
@@ -1711,6 +1711,7 @@ assign_hard_reg (ira_allocno_t a, bool retry_p)
         {
 	  ira_allocno_t conflict_a = OBJECT_ALLOCNO (conflict_obj);
 	  enum reg_class conflict_aclass;
+	  allocno_color_data_t data = ALLOCNO_COLOR_DATA (conflict_a);
 
 	  /* Reload can give another class so we need to check all
 	     allocnos.  */
@@ -1782,7 +1783,12 @@ assign_hard_reg (ira_allocno_t a, bool retry_p)
 		    hard_regno = ira_class_hard_regs[aclass][j];
 		    ira_assert (hard_regno >= 0);
 		    k = ira_class_hard_reg_index[conflict_aclass][hard_regno];
-		    if (k < 0)
+		    if (k < 0
+			   /* If HARD_REGNO is not available for CONFLICT_A,
+			      the conflict would be ignored, since HARD_REGNO
+			      will never be assigned to CONFLICT_A.  */
+			|| !TEST_HARD_REG_BIT (data->profitable_hard_regs,
+					       hard_regno))
 		      continue;
 		    full_costs[j] -= conflict_costs[k];
 		  }
diff --git a/gcc/params.def b/gcc/params.def
index c656eaf..09af9fc 100644
--- a/gcc/params.def
+++ b/gcc/params.def
@@ -1054,6 +1054,20 @@ DEFPARAM (PARAM_MAX_SLSR_CANDIDATE_SCAN,
 	  "strength reduction",
 	  50, 1, 999999)
 
+/* Maximum number of instructions to duplicate when shortcutting a switch.  */
+DEFPARAM (PARAM_MAX_SWITCH_INSNS,
+	  "max-switch-insns",
+	  "Maximum number of instructions to duplicate when "
+	  "shortcutting a switch statement",
+	  100, 1, 999999)
+
+/* Maximum number of paths to duplicate when shortcutting a switch.  */
+DEFPARAM (PARAM_MAX_SWITCH_PATHS,
+	  "max-switch-paths",
+	  "Maximum number of new paths to create when"
+	  " shortcutting a switch statement",
+	  50, 1, 999999)
+
 DEFPARAM (PARAM_ASAN_STACK,
          "asan-stack",
          "Enable asan stack protection",
diff --git a/gcc/passes.def b/gcc/passes.def
index c98b048..8fdef89 100644
--- a/gcc/passes.def
+++ b/gcc/passes.def
@@ -152,6 +152,7 @@ along with GCC; see the file COPYING3.  If not see
       NEXT_PASS (pass_call_cdce);
       NEXT_PASS (pass_cselim);
       NEXT_PASS (pass_tree_ifcombine);
+      NEXT_PASS (pass_tree_switch_shortcut);
       NEXT_PASS (pass_phiopt);
       NEXT_PASS (pass_tail_recursion);
       NEXT_PASS (pass_ch);
diff --git a/gcc/testsuite/ChangeLog.arm b/gcc/testsuite/ChangeLog.arm
new file mode 100644
index 0000000..3b85694
--- /dev/null
+++ b/gcc/testsuite/ChangeLog.arm
@@ -0,0 +1,26 @@
+2014-12-02  Terry Guo  <terry.guo@arm.com>
+
+	Backport from mainline r211817
+	2014-06-19  Terry Guo  <terry.guo@arm.com>
+
+	* gcc.target/arm/thumb1-load-64bit-constant-1.c: New test.
+	* gcc.target/arm/thumb1-load-64bit-constant-2.c: Ditto.
+	* gcc.target/arm/thumb1-load-64bit-constant-3.c: Ditto.
+
+2014-11-10  Hale Wang  <hale.wang@arm.com>
+
+	Backport from mainline r217175
+	2014-11-06  Hale Wang  <hale.wang@arm.com>
+
+	* gcc.target/arm/small-multiply-m0-1.c: New test for
+	target -mcpu=cortex-m0.small-multiply.
+	* gcc.target/arm/small-multiply-m0-2.c: Likewise.
+	* gcc.target/arm/small-multiply-m0-3.c: Likewise.
+	* gcc.target/arm/small-multiply-m0plus-1.c: New test for
+	target -mcpu=cortex-m0plus.small-multiply.
+	* gcc.target/arm/small-multiply-m0plus-2.c: Likewise.
+	* gcc.target/arm/small-multiply-m0plus-3.c: Likewise.
+	* gcc.target/arm/small-multiply-m1-1.c: New test for
+	target -mcpu=cortex-m1.small-multiply.
+	* gcc.target/arm/small-multiply-m1-2.c: Likewise.
+	* gcc.target/arm/small-multiply-m1-3.c: Likewise.
diff --git a/gcc/testsuite/gcc.target/arm/get_address_cost_aligned_max_offset.c b/gcc/testsuite/gcc.target/arm/get_address_cost_aligned_max_offset.c
new file mode 100644
index 0000000..cc3e2f7
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/get_address_cost_aligned_max_offset.c
@@ -0,0 +1,28 @@
+/* { dg-do compile } */
+/* { dg-options "-mthumb -O2" }  */
+/* { dg-require-effective-target arm_thumb1_ok } */
+
+unsigned int
+test (const short p16[6 * 64])
+{
+  unsigned int i = 6;
+  unsigned int ret = 0;
+
+  do
+    {
+      unsigned long long *p64 = (unsigned long long*) p16;
+      unsigned int *p32 = (unsigned int*) p16;
+      ret += ret;
+      if (p16[1] || p32[1])
+	ret++;
+      else if (p64[1] | p64[2] | p64[3])
+	ret++;
+      p16 += 64;
+      i--;
+    } while (i != 0);
+
+  return ret;
+}
+
+/* { dg-final { scan-assembler-not "#22" } } */
+/* { dg-final { scan-assembler-not "#14" } } */
diff --git a/gcc/testsuite/gcc.target/arm/pr63210.c b/gcc/testsuite/gcc.target/arm/pr63210.c
new file mode 100644
index 0000000..c3ae928
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/pr63210.c
@@ -0,0 +1,12 @@
+/* { dg-do assemble } */
+/* { dg-options "-mthumb -Os " }  */
+/* { dg-require-effective-target arm_thumb1_ok } */
+
+int foo1 (int c);
+int foo2 (int c);
+
+int test (int c)
+{
+  return (foo1 (c) || foo2 (c));
+}
+/* { dg-final { object-size text <= 28 } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m0-1.c b/gcc/testsuite/gcc.target/arm/small-multiply-m0-1.c
new file mode 100644
index 0000000..77ec603
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m0-1.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m0.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m0.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m0.small-multiply -mthumb -O2" } */
+
+int
+test (int a)
+{
+  return a * 0x123456;
+}
+
+/* { dg-final { scan-assembler-not "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m0-2.c b/gcc/testsuite/gcc.target/arm/small-multiply-m0-2.c
new file mode 100644
index 0000000..c89b3ba
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m0-2.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m0.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m0.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m0.small-multiply -mthumb -Os" } */
+
+int
+test (int a)
+{
+  return a * 0x123456;
+}
+
+/* { dg-final { scan-assembler "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m0-3.c b/gcc/testsuite/gcc.target/arm/small-multiply-m0-3.c
new file mode 100644
index 0000000..b2df109
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m0-3.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m0.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m0.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m0.small-multiply -mthumb -Os" } */
+
+int
+test (int a)
+{
+  return a * 0x13;
+}
+
+/* { dg-final { scan-assembler-not "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-1.c b/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-1.c
new file mode 100644
index 0000000..08a450b
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-1.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m0plus.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m0plus.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m0plus.small-multiply -mthumb -O2" } */
+
+int
+test (int a)
+{
+  return a * 0x123456;
+}
+
+/* { dg-final { scan-assembler-not "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-2.c b/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-2.c
new file mode 100644
index 0000000..17b52d3
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-2.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m0plus.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m0plus.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m0plus.small-multiply -mthumb -Os" } */
+
+int
+test (int a)
+{
+  return a * 0x123456;
+}
+
+/* { dg-final { scan-assembler "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-3.c b/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-3.c
new file mode 100644
index 0000000..af69c75
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m0plus-3.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m0plus.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m0plus.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m0plus.small-multiply -mthumb -Os" } */
+
+int
+test (int a)
+{
+  return a * 0x13;
+}
+
+/* { dg-final { scan-assembler-not "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m1-1.c b/gcc/testsuite/gcc.target/arm/small-multiply-m1-1.c
new file mode 100644
index 0000000..d265aaf
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m1-1.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m1.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m1.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m1.small-multiply -mthumb -O2" } */
+
+int
+test (int a)
+{
+  return a * 0x123456;
+}
+
+/* { dg-final { scan-assembler-not "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m1-2.c b/gcc/testsuite/gcc.target/arm/small-multiply-m1-2.c
new file mode 100644
index 0000000..c50891c
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m1-2.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m1.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m1.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m1.small-multiply -mthumb -Os" } */
+
+int
+test (int a)
+{
+  return a * 0x123456;
+}
+
+/* { dg-final { scan-assembler "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/small-multiply-m1-3.c b/gcc/testsuite/gcc.target/arm/small-multiply-m1-3.c
new file mode 100644
index 0000000..1da21a6
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/small-multiply-m1-3.c
@@ -0,0 +1,12 @@
+/* { dg-do compile } */
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-skip-if "Test is specific to cortex-m1.small-multiply" { arm*-*-* } { "-mcpu=*" } { "-mcpu=cortex-m1.small-multiply" } } */
+/* { dg-options "-mcpu=cortex-m1.small-multiply -mthumb -Os" } */
+
+int
+test (int a)
+{
+  return a * 0x13;
+}
+
+/* { dg-final { scan-assembler-not "\[\\t \]+mul" } } */
diff --git a/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-1.c b/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-1.c
new file mode 100644
index 0000000..9537aaf
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-1.c
@@ -0,0 +1,14 @@
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-do compile } */
+/* { dg-options "-Os" } */
+/* { dg-skip-if "" { ! { arm_thumb1 } } } */
+
+extern long long madd (long long a, long long b);
+
+long long
+foo ()
+{
+  return madd (0x0000000100000001LL, 0x0000011100000001LL);
+}
+
+/* { dg-final { scan-assembler-not "ldr" } } */
diff --git a/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-2.c b/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-2.c
new file mode 100644
index 0000000..836682b
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-2.c
@@ -0,0 +1,14 @@
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-do compile } */
+/* { dg-options "-O2" } */
+/* { dg-skip-if "" { ! { arm_thumb1 } } } */
+
+extern long long madd (long long a);
+
+long long
+foo ()
+{
+  return madd (0x0000000100000001LL);
+}
+
+/* { dg-final { scan-assembler-not "ldr" } } */
diff --git a/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-3.c b/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-3.c
new file mode 100644
index 0000000..cf4d0be
--- /dev/null
+++ b/gcc/testsuite/gcc.target/arm/thumb1-load-64bit-constant-3.c
@@ -0,0 +1,14 @@
+/* { dg-require-effective-target arm_thumb1_ok } */
+/* { dg-do compile } */
+/* { dg-options "-Os" } */
+/* { dg-skip-if "" { ! { arm_thumb1 } } } */
+
+long long
+foo (int len)
+{
+  return (long long) (((long long) 1 << len) - 1);
+}
+
+/* { dg-final { scan-assembler-not "ldr" } } */
+/* { dg-final { scan-assembler-times "neg" 1 } } */
+
diff --git a/gcc/timevar.def b/gcc/timevar.def
index 9faf98b..b643d15 100644
--- a/gcc/timevar.def
+++ b/gcc/timevar.def
@@ -167,6 +167,7 @@ DEFTIMEVAR (TV_TREE_LOOP_IVCANON     , "tree canonical iv")
 DEFTIMEVAR (TV_SCEV_CONST            , "scev constant prop")
 DEFTIMEVAR (TV_TREE_LOOP_UNSWITCH    , "tree loop unswitching")
 DEFTIMEVAR (TV_COMPLETE_UNROLL       , "complete unrolling")
+DEFTIMEVAR (TV_TREE_SWITCH_SHORTCUT  , "switch statement shortcuts")
 DEFTIMEVAR (TV_TREE_PARALLELIZE_LOOPS, "tree parallelize loops")
 DEFTIMEVAR (TV_TREE_VECTORIZATION    , "tree vectorization")
 DEFTIMEVAR (TV_TREE_SLP_VECTORIZATION, "tree slp vectorization")
diff --git a/gcc/tree-pass.h b/gcc/tree-pass.h
index 3b86770..0e2a40d 100644
--- a/gcc/tree-pass.h
+++ b/gcc/tree-pass.h
@@ -585,6 +585,7 @@ extern gimple_opt_pass *make_pass_early_inline (gcc::context *ctxt);
 extern gimple_opt_pass *make_pass_inline_parameters (gcc::context *ctxt);
 extern gimple_opt_pass *make_pass_update_address_taken (gcc::context *ctxt);
 extern gimple_opt_pass *make_pass_convert_switch (gcc::context *ctxt);
+extern gimple_opt_pass *make_pass_tree_switch_shortcut (gcc::context *ctxt);
 
 /* Current optimization pass.  */
 extern opt_pass *current_pass;
diff --git a/gcc/tree-ssa-loop-ivopts.c b/gcc/tree-ssa-loop-ivopts.c
index 78f036e..407f4f5 100644
--- a/gcc/tree-ssa-loop-ivopts.c
+++ b/gcc/tree-ssa-loop-ivopts.c
@@ -3285,6 +3285,18 @@ get_address_cost (bool symbol_present, bool var_present,
 	  XEXP (addr, 1) = gen_int_mode (off, address_mode);
 	  if (memory_address_addr_space_p (mem_mode, addr, as))
 	    break;
+	  /* For some TARGET, like ARM THUMB1, the offset should be nature
+	     aligned.  Try an aligned offset if address_mode is not QImode.  */
+	  off = (address_mode == QImode)
+		? 0
+		: ((unsigned HOST_WIDE_INT) 1 << i)
+		    - GET_MODE_SIZE (address_mode);
+	  if (off > 0)
+	    {
+	      XEXP (addr, 1) = gen_int_mode (off, address_mode);
+	      if (memory_address_addr_space_p (mem_mode, addr, as))
+		break;
+	    }
 	}
       if (i == -1)
         off = 0;
diff --git a/gcc/tree-switch-shortcut.c b/gcc/tree-switch-shortcut.c
new file mode 100644
index 0000000..6f85a0c
--- /dev/null
+++ b/gcc/tree-switch-shortcut.c
@@ -0,0 +1,444 @@
+/* Switch shortcutting optimization for GNU C
+   Copyright (C) 2014 Free Software Foundation, Inc.
+   Contributed by Steve Ellcey (steve.ellcey@imgtec.com).
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+/* This file implements an optimization where, when a variable is set
+   to a constant value and there is a path that leads from that definition
+   to a switch statement that uses that variable as its controlling expression
+   we duplicate the blocks on this path and change the jump to the switch
+   statement with a direct jump to the label of the switch block that control
+   would goto based on the value of the variable.  This can come up in
+   loops/switch statements that implement state machines.
+
+   Example (modified from PR 54742):
+
+   foo(char *str) {
+     int sum=0;
+     int state=0;
+     char *s=str;
+     for (; *s; s++) {
+       char c=*s;
+       <CODE BLOCK 1>
+       switch (state) {
+         case 0:
+           if (c == '+')       { state = 1; sum += 9; }
+           else if (c != '-')  { state = 2; sum += 3; }
+           break;
+         case 1:
+           if (c == '+')       { state = 2; sum += 4; }
+           else if (c == '-')  { state = 0; sum += 7; }
+           break;
+         case 2:
+           if (c == '+')       { state = 0; sum += 8; }
+           else if (c == '-')  { state = 1; sum += 2; }
+           break;
+       }
+       <CODE BLOCK 2>
+     }
+     return state;
+   }
+
+  This pass will convert the code inside 'case 0' to something like:
+
+    case 0:
+      if (c == '+')      { state = 1; sum += 9;
+                           <CODE BLOCK 2>
+                           s++; if (!s) goto loop_exit;
+                           <CODE BLOCK 1>
+                           goto case_1; }
+      else if (c != '-') { state = 2; sum += 3;
+                           <CODE BLOCK 2>
+                           s++; if (!s) goto loop_exit;
+                           <CODE BLOCK 1>
+                           goto case_2; }
+      else               { <CODE BLOCK 2>
+			   s++; if (!s) goto exit;
+                           <CODE BLOCK 1>
+                           goto case_0; }
+
+Similar transformations would apply to the other parts of the switch
+statement.  This obviously can lead to a lot of code duplication but
+it can also result in faster code since we are replacing two jumps
+(one indirect) with a single direct jump.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "params.h"
+#include "tree.h"
+#include "tree-pass.h"
+#include "basic-block.h"
+#include "tree-ssa-alias.h"
+#include "tree-cfg.h"
+#include "tree-ssa-operands.h"
+#include "tree-into-ssa.h"
+#include "tree-inline.h"
+#include "pointer-set.h"
+#include "internal-fn.h"
+#include "gimple-expr.h"
+#include "is-a.h"
+#include "gimple.h"
+#include "gimple-iterator.h"
+#include "gimple-ssa.h"
+#include "cfgloop.h"
+#include "context.h"
+
+/* Helper function for find_path, visited_bbs is used to make sure we don't
+   fall into an infinite loop.  */
+
+static int
+find_path_1 (basic_block start_bb, basic_block end_bb,
+	     struct pointer_set_t *visited_bbs)
+{
+  edge_iterator ei;
+  edge e;
+
+  if (start_bb == end_bb) return 1;
+
+  if (!pointer_set_insert (visited_bbs, start_bb))
+    {
+      FOR_EACH_EDGE (e, ei, start_bb->succs)
+	if (find_path_1 (e->dest, end_bb, visited_bbs))
+	  return 1;
+    }
+  return 0;
+}
+
+/* Return 1 if there is a path from start_bb to end_bb and 0 if there
+   is not.  There may be multiple paths from start_bb to end_bb.  */
+
+static int
+find_path (basic_block start_bb, basic_block end_bb)
+{
+  edge_iterator ei;
+  edge e;
+  struct pointer_set_t *visited_bbs;
+  int p = 0;
+
+  if (start_bb == end_bb) return 1;
+
+  visited_bbs = pointer_set_create ();
+  if (!pointer_set_insert (visited_bbs, start_bb))
+    {
+      FOR_EACH_EDGE (e, ei, start_bb->succs)
+	if (find_path_1 (e->dest, end_bb, visited_bbs))
+	  {
+	    p = 1;
+	    break;
+	  }
+    }
+  pointer_set_destroy (visited_bbs);
+  return p;
+}
+
+
+/* We save the paths we want to copy in bbs_list_array.  n_bbs_list is the
+   number of paths saved, bbs_list_array[i] is the list of basic blocks in
+   one path.  Each path starts with the block where a variable is assigned
+   a constant value (bbs_list_array[i][0]) and ends with the switch statement
+   block (bbs_list_array[i][bbs_list_size[i]-2]) followed by the block that
+   the switch statement is going to go to given the constant value of the
+   variable (bbs_list_array[i][bbs_list_size[i]-1]).  */
+
+struct path_info
+{
+  basic_block **bbs_list_array;
+  int *val_array;
+  int *bbs_list_size;
+  int max_path_count;
+  int max_insn_count;
+  int n_bbs_list;
+};
+
+/* bbs_list[0] is the block with the switch statement,
+   bbs_list[n-1] is the block where the switch statement variable is assigned
+     a constant value,
+   The entries in between make a (reverse) path between the two.
+
+   We don't want to change bb_list, we want to leave that alone and
+   and copy the path to bbs_list_array so that we wind up with a list (array)
+   of paths that we want to update.  We also want to add the block that the
+   switch is going to go to on to the list so that we know which exit from
+   the switch statement is important.  */
+
+static void
+save_new_path (basic_block *bbs_list, int n, tree val, path_info *pi)
+{
+  int i;
+  int insn_count;
+  basic_block bb;
+  edge switch_taken_edge;
+  gimple_stmt_iterator gsi;
+
+  if (n <= 1) return;
+
+  if (pi->n_bbs_list >= pi->max_path_count)
+    return;
+
+  /* Put the blocks in 'correct' order and add in where we want to go after
+     the switch statement, We want to leave bbs_list untouched for future
+     calls.  */
+
+  pi->bbs_list_array[pi->n_bbs_list] = XNEWVEC (basic_block, n+1);
+  for (i = 0; i < n; i++)
+    pi->bbs_list_array[pi->n_bbs_list][i] = bbs_list[n-i-1];
+
+  switch_taken_edge = find_taken_edge (bbs_list[0], val);
+  pi->bbs_list_array[pi->n_bbs_list][n] = switch_taken_edge->dest;
+
+  pi->bbs_list_size[pi->n_bbs_list] = n + 1;
+  pi->val_array[pi->n_bbs_list] = (int) TREE_INT_CST_LOW (val);
+
+  /* Count how many instructions are in the blocks we are going to
+     duplicate and if there are too many do not save this path
+     (return without incrementing n_bbs_list).  */
+
+  insn_count = 0;
+  for (i = 1; i < n; i++)
+    {
+      bb = pi->bbs_list_array[pi->n_bbs_list][i];
+      for (gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&gsi))
+	insn_count += estimate_num_insns (gsi_stmt (gsi), &eni_size_weights);
+    }
+
+  if (insn_count > pi->max_insn_count)
+    return;
+
+  pi->n_bbs_list = pi->n_bbs_list + 1;
+}
+
+/* switch_stmt is a switch statement whose switch index expression
+   is the variable expr.  We trace the value of the variable back
+   through any phi nodes looking for places where it gets a constant
+   value and save the path in bbs_list.  Then we call save_new_path
+   to create a list of such paths.  */
+
+static void
+process_switch (tree expr, gimple switch_stmt,
+		struct pointer_set_t *visited_phis,
+	        basic_block *bbs_list, int n,
+		path_info *pi)
+{
+  gimple def_stmt;
+  tree var;
+  unsigned int i;
+  edge e;
+  edge_iterator ei;
+  basic_block bbx;
+  basic_block var_bb;
+  int e_count;
+
+  gcc_assert (gimple_code (switch_stmt) == GIMPLE_SWITCH);
+  var = SSA_NAME_VAR (expr);
+  def_stmt = SSA_NAME_DEF_STMT (expr);
+  var_bb = gimple_bb (def_stmt);
+
+  if (var == NULL || var_bb == NULL) return;
+
+  /* We have a variable definition (var) that is defined in var_bb,
+     We want to put the path from var_bb to the current bb into the
+     bbs_list.  If there is more then one path, skip this and don't
+     try to do the optimization.  */
+
+  bbx = bbs_list[n-1];
+  while (bbx != var_bb)
+    {
+      e_count = 0;
+      FOR_EACH_EDGE (e, ei, bbx->preds)
+	if (find_path (var_bb, e->src))
+	  {
+	    bbs_list[n] = e->src;
+	    n = n + 1;
+	    e_count = e_count + 1;
+	  }
+      if (e_count != 1) return;
+      bbx = bbs_list[n-1];
+    }
+
+  if (gimple_code (def_stmt) == GIMPLE_PHI
+      && !pointer_set_insert (visited_phis, def_stmt))
+    {
+      for (i = 0; i < gimple_phi_num_args (def_stmt); i++)
+	{
+	  tree arg = gimple_phi_arg_def (def_stmt, i);
+	  if (arg && TREE_CODE (arg) == INTEGER_CST)
+	    {
+	      /* const char *name = IDENTIFIER_POINTER (DECL_NAME (var)); */
+	      bbs_list[n] = gimple_phi_arg_edge (def_stmt, i)->src;
+	      save_new_path (bbs_list, n + 1, arg, pi);
+	    }
+	  else if (arg && TREE_CODE (arg) == SSA_NAME)
+	    {
+	      bbs_list[n] = gimple_phi_arg_edge (def_stmt, i)->src;
+	      process_switch (arg, switch_stmt, visited_phis, bbs_list, n+1, pi);
+	    }
+	}
+    }
+}
+
+/* Find paths that lead from blocks where a variable is assigned a constant
+   value to a switch statement where that variable is used as the switch
+   index.  Save the paths in bbs_list_array so that they can be processed
+   by copy_switch_paths.  */
+
+static unsigned int
+find_switch_shortcuts (path_info *pi)
+{
+  basic_block bb;
+  struct pointer_set_t *visited_phis;
+  basic_block *bbs_list;
+  int n = 1;
+
+  bbs_list = XNEWVEC (basic_block, n_basic_blocks_for_fn (cfun));
+  visited_phis = pointer_set_create ();
+  FOR_EACH_BB_FN (bb, cfun)
+    {
+      gimple stmt = last_stmt (bb);
+      if (stmt && gimple_code (stmt) == GIMPLE_SWITCH)
+	{
+	  tree op = gimple_switch_index (stmt);
+	  tree var = SSA_NAME_VAR (op);
+	  if (var)
+	    {
+	      bbs_list[0] = bb;
+	      process_switch (op, stmt, visited_phis, bbs_list, n, pi);
+	    }
+	}
+    }
+  pointer_set_destroy (visited_phis);
+  XDELETEVEC (bbs_list);
+  return 0;
+}
+
+/* Call gimple_duplicate_sese_region to douplicate the blocks in bb_list.
+   We free and recalculate all ssa and dominance information afterwords
+   because the region being copied is not really SESE and so we cannot
+   trust gimple_duplicate_sese_region to correctly update the dataflow
+   information.  */
+
+static void
+duplicate_blocks (basic_block *bb_list, int bb_count)
+{
+  edge orig_edge, exit_edge;
+  loop_p loop;
+
+  orig_edge = find_edge (bb_list[0], bb_list[1]);
+  exit_edge = find_edge (bb_list[bb_count-2], bb_list[bb_count-1]);
+  /* Earlier block duplications may have removed the path that we
+     saved earlier and are trying to duplicate here.  */
+  if (orig_edge != NULL && exit_edge != NULL)
+    {
+      gimple_duplicate_sese_region (orig_edge, exit_edge, &bb_list[1],
+				    bb_count-2, NULL, false);
+      free_dominance_info (CDI_DOMINATORS);
+      update_ssa (TODO_update_ssa);
+      calculate_dominance_info (CDI_DOMINATORS);
+      loops_state_set (LOOPS_NEED_FIXUP);
+    }
+}
+
+/* Go through the paths saved in bbs_list_array and make copies of them.  */
+
+static void
+copy_switch_paths (path_info *pi)
+{
+  int i;
+
+  /* Process each path in bbs_list_size.  */
+  for (i = 0; i < pi->n_bbs_list; i++)
+    {
+    /* For each path in bbs_list_size loop through and copy each block in
+       the path (except the first on where the constant is assigned and
+       the final one where the switch statement goes to.  */
+
+    if (!single_pred_p (pi->bbs_list_array[i][1]))
+      duplicate_blocks (pi->bbs_list_array[i], pi->bbs_list_size[i]);
+    }
+}
+
+
+/* Main entry for the tree if-conversion pass.  */
+
+namespace {
+
+const pass_data pass_data_tree_switch_shortcut =
+{
+  GIMPLE_PASS, /* type */
+  "switch_shortcut", /* name */
+  OPTGROUP_NONE, /* optinfo_flags */
+  true,		/* gate */
+  true,		/* execute */
+  TV_TREE_SWITCH_SHORTCUT, /* tv_id */
+  ( PROP_cfg | PROP_ssa ), /* properties_required */
+  0, /* properties_provided */
+  0, /* properties_destroyed */
+  0, /* todo_flags_start */
+  TODO_update_ssa, /* todo_flags_finish */
+};
+
+class pass_tree_switch_shortcut : public gimple_opt_pass
+{
+public:
+  pass_tree_switch_shortcut (gcc::context *ctxt)
+    : gimple_opt_pass (pass_data_tree_switch_shortcut, ctxt)
+  {}
+
+  /* opt_pass methods: */
+  bool gate ()
+    {
+      return flag_tree_switch_shortcut;
+    }
+  unsigned int execute ();
+
+}; // class pass_tree_switch_shortcut
+
+unsigned int
+pass_tree_switch_shortcut::execute ()
+{
+  int i;
+  path_info *pi;
+
+  pi = XNEW (path_info);
+  pi->n_bbs_list = 0;
+  pi->max_insn_count = PARAM_VALUE (PARAM_MAX_SWITCH_INSNS);
+  pi->max_path_count = PARAM_VALUE (PARAM_MAX_SWITCH_PATHS);
+  loop_optimizer_init (LOOPS_NORMAL);
+  pi->val_array = XNEWVEC (int, pi->max_path_count);
+  pi->bbs_list_size = XNEWVEC (int, pi->max_path_count);
+  pi->bbs_list_array = XNEWVEC (basic_block *, pi->max_path_count);
+  find_switch_shortcuts (pi);
+  copy_switch_paths (pi);
+  XDELETEVEC (pi->val_array);
+  XDELETEVEC (pi->bbs_list_size);
+  for (i = 0; i < pi->n_bbs_list; i++)
+    XDELETEVEC (pi->bbs_list_array[i]);
+  XDELETEVEC (pi->bbs_list_array);
+  loop_optimizer_finalize ();
+  XDELETE (pi);
+  return 0;
+}
+
+} // anon namespace
+
+gimple_opt_pass *
+make_pass_tree_switch_shortcut (gcc::context *ctxt)
+{
+  return new pass_tree_switch_shortcut (ctxt);
+}
diff --git a/libcpp/ChangeLog.arm b/libcpp/ChangeLog.arm
new file mode 100644
index 0000000..f80896f
--- /dev/null
+++ b/libcpp/ChangeLog.arm
@@ -0,0 +1,8 @@
+2014-11-14  Terry Guo  <terry.guo@arm.com>
+
+	Backport trunk patch at r210264
+	2014-05-09  Joey Ye  <joey.ye@arm.com>
+
+	* files.c (find_file_in_dir): Always try to shorten for DOS
+	non-system headers.
+	* init.c (ENABLE_CANONICAL_SYSTEM_HEADERS): Default enabled for DOS.
diff --git a/libcpp/files.c b/libcpp/files.c
index 149f06d..4764f36 100644
--- a/libcpp/files.c
+++ b/libcpp/files.c
@@ -387,8 +387,14 @@ find_file_in_dir (cpp_reader *pfile, _cpp_file *file, bool *invalid_pch)
       char *copy;
       void **pp;
 
-      /* We try to canonicalize system headers.  */
-      if (CPP_OPTION (pfile, canonical_system_headers) && file->dir->sysp)
+      /* We try to canonicalize system headers.  For DOS based file
+       * system, we always try to shorten non-system headers, as DOS
+       * has a tighter constraint on max path length.  */
+      if (CPP_OPTION (pfile, canonical_system_headers) && file->dir->sysp
+#ifdef HAVE_DOS_BASED_FILE_SYSTEM
+	  || !file->dir->sysp
+#endif
+	 )
 	{
 	  char * canonical_path = maybe_shorter_path (path);
 	  if (canonical_path)
diff --git a/libcpp/init.c b/libcpp/init.c
index f10413a..b809515 100644
--- a/libcpp/init.c
+++ b/libcpp/init.c
@@ -27,8 +27,12 @@ along with this program; see the file COPYING3.  If not see
 #include "filenames.h"
 
 #ifndef ENABLE_CANONICAL_SYSTEM_HEADERS
+#ifdef HAVE_DOS_BASED_FILE_SYSTEM
+#define ENABLE_CANONICAL_SYSTEM_HEADERS 1
+#else
 #define ENABLE_CANONICAL_SYSTEM_HEADERS 0
 #endif
+#endif
 
 static void init_library (void);
 static void mark_named_operators (cpp_reader *, int);
diff --git a/libgcc/ChangeLog.arm b/libgcc/ChangeLog.arm
new file mode 100644
index 0000000..27b4eab
--- /dev/null
+++ b/libgcc/ChangeLog.arm
@@ -0,0 +1,3 @@
+2014-11-27  Hale Wang  <hale.wang@arm.com>
+	
+	* config/arm/lib1funcs.S: Add new wrapper.
diff --git a/libgcc/config/arm/lib1funcs.S b/libgcc/config/arm/lib1funcs.S
index b617137..de66c81 100644
--- a/libgcc/config/arm/lib1funcs.S
+++ b/libgcc/config/arm/lib1funcs.S
@@ -306,34 +306,12 @@ LSYM(Lend_fde):
 #ifdef __ARM_EABI__
 .macro THUMB_LDIV0 name signed
 #if defined(__ARM_ARCH_6M__)
-	.ifc \signed, unsigned
-	cmp	r0, #0
-	beq	1f
-	mov	r0, #0
-	mvn	r0, r0		@ 0xffffffff
-1:
-	.else
-	cmp	r0, #0
-	beq	2f
-	blt	3f
+
+	push	{r0, lr}
 	mov	r0, #0
-	mvn	r0, r0
-	lsr	r0, r0, #1	@ 0x7fffffff
-	b	2f
-3:	mov	r0, #0x80
-	lsl	r0, r0, #24	@ 0x80000000
-2:
-	.endif
-	push	{r0, r1, r2}
-	ldr	r0, 4f
-	adr	r1, 4f
-	add	r0, r1
-	str	r0, [sp, #8]
-	@ We know we are not on armv4t, so pop pc is safe.
-	pop	{r0, r1, pc}
-	.align	2
-4:
-	.word	__aeabi_idiv0 - 4b
+	bl	SYM(__aeabi_idiv0)
+	pop	{r1, pc}
+
 #elif defined(__thumb2__)
 	.syntax unified
 	.ifc \signed, unsigned
@@ -927,7 +905,158 @@ LSYM(Lover7):
 	add	dividend, work
   .endif
 LSYM(Lgot_result):
-.endm	
+.endm
+
+#if defined(__prefer_thumb__) && !defined(__OPTIMIZE_SIZE__)
+.macro BranchToDiv n, label
+	lsr	curbit, dividend, \n
+	cmp	curbit, divisor
+	blo	\label
+.endm
+
+.macro DoDiv n
+	lsr	curbit, dividend, \n
+	cmp	curbit, divisor
+	bcc	1f
+	lsl	curbit, divisor, \n
+	sub	dividend, dividend, curbit
+
+1:	adc	result, result
+.endm
+
+.macro THUMB1_Div_Positive
+	mov	result, #0
+	BranchToDiv #1, LSYM(Lthumb1_div1)
+	BranchToDiv #4, LSYM(Lthumb1_div4)
+	BranchToDiv #8, LSYM(Lthumb1_div8)
+	BranchToDiv #12, LSYM(Lthumb1_div12)
+	BranchToDiv #16, LSYM(Lthumb1_div16)
+LSYM(Lthumb1_div_large_positive):
+	mov	result, #0xff
+	lsl	divisor, divisor, #8
+	rev	result, result
+	lsr	curbit, dividend, #16
+	cmp	curbit, divisor
+	blo	1f
+	asr	result, #8
+	lsl	divisor, divisor, #8
+	beq	LSYM(Ldivbyzero_waypoint)
+
+1:	lsr	curbit, dividend, #12
+	cmp	curbit, divisor
+	blo	LSYM(Lthumb1_div12)
+	b	LSYM(Lthumb1_div16)
+LSYM(Lthumb1_div_loop):
+	lsr	divisor, divisor, #8
+LSYM(Lthumb1_div16):
+	Dodiv	#15
+	Dodiv	#14
+	Dodiv	#13
+	Dodiv	#12
+LSYM(Lthumb1_div12):
+	Dodiv	#11
+	Dodiv	#10
+	Dodiv	#9
+	Dodiv	#8
+	bcs	LSYM(Lthumb1_div_loop)
+LSYM(Lthumb1_div8):
+	Dodiv	#7
+	Dodiv	#6
+	Dodiv	#5
+LSYM(Lthumb1_div5):
+	Dodiv	#4
+LSYM(Lthumb1_div4):
+	Dodiv	#3
+LSYM(Lthumb1_div3):
+	Dodiv	#2
+LSYM(Lthumb1_div2):
+	Dodiv	#1
+LSYM(Lthumb1_div1):
+	sub	divisor, dividend, divisor
+	bcs	1f
+	cpy	divisor, dividend
+
+1:	adc	result, result
+	cpy	dividend, result
+	RET
+
+LSYM(Ldivbyzero_waypoint):
+	b	LSYM(Ldiv0)
+.endm
+
+.macro THUMB1_Div_Negative
+	lsr	result, divisor, #31
+	beq	1f
+	neg	divisor, divisor
+
+1:	asr	curbit, dividend, #32
+	bcc	2f
+	neg	dividend, dividend
+
+2:	eor	curbit, result
+	mov	result, #0
+	cpy	ip, curbit
+	BranchToDiv #4, LSYM(Lthumb1_div_negative4)
+	BranchToDiv #8, LSYM(Lthumb1_div_negative8)
+LSYM(Lthumb1_div_large):
+	mov	result, #0xfc
+	lsl	divisor, divisor, #6
+	rev	result, result
+	lsr	curbit, dividend, #8
+	cmp	curbit, divisor
+	blo	LSYM(Lthumb1_div_negative8)
+
+	lsl	divisor, divisor, #6
+	asr	result, result, #6
+	cmp	curbit, divisor
+	blo	LSYM(Lthumb1_div_negative8)
+
+	lsl	divisor, divisor, #6
+	asr	result, result, #6
+	cmp	curbit, divisor
+	blo	LSYM(Lthumb1_div_negative8)
+
+	lsl	divisor, divisor, #6
+	beq	LSYM(Ldivbyzero_negative)
+	asr	result, result, #6
+	b	LSYM(Lthumb1_div_negative8)
+LSYM(Lthumb1_div_negative_loop):
+	lsr	divisor, divisor, #6
+LSYM(Lthumb1_div_negative8):
+	DoDiv	#7
+	DoDiv	#6
+	DoDiv	#5
+	DoDiv	#4
+LSYM(Lthumb1_div_negative4):
+	DoDiv	#3
+	DoDiv	#2
+	bcs	LSYM(Lthumb1_div_negative_loop)
+	DoDiv	#1
+	sub	divisor, dividend, divisor
+	bcs	1f
+	cpy	divisor, dividend
+
+1:	cpy	curbit, ip
+	adc	result, result
+	asr	curbit, curbit, #1
+	cpy	dividend, result
+	bcc	2f
+	neg	dividend, dividend
+	cmp	curbit, #0
+
+2:	bpl	3f
+	neg	divisor, divisor
+
+3:	RET
+
+LSYM(Ldivbyzero_negative):
+	cpy	curbit, ip
+	asr	curbit, curbit, #1
+	bcc	LSYM(Ldiv0)
+	neg	dividend, dividend
+.endm
+#endif /* ARM Thumb version.  */
+
 /* ------------------------------------------------------------------------ */
 /*		Start of the Real Functions				    */
 /* ------------------------------------------------------------------------ */
@@ -937,6 +1066,7 @@ LSYM(Lgot_result):
 
 	FUNC_START udivsi3
 	FUNC_ALIAS aeabi_uidiv udivsi3
+#if defined(__OPTIMIZE_SIZE__)
 
 	cmp	divisor, #0
 	beq	LSYM(Ldiv0)
@@ -954,6 +1084,13 @@ LSYM(udivsi3_skip_div0_test):
 	pop	{ work }
 	RET
 
+#else
+
+LSYM(udivsi3_skip_div0_test):
+	THUMB1_Div_Positive
+
+#endif
+
 #elif defined(__ARM_ARCH_EXT_IDIV__)
 
 	ARM_FUNC_START udivsi3
@@ -1066,7 +1203,7 @@ LSYM(Lover10):
 	RET
 	
 #else  /* ARM version.  */
-	
+
 	FUNC_START umodsi3
 
 	subs	r2, r1, #1			@ compare divisor with 1
@@ -1091,8 +1228,9 @@ LSYM(Lover10):
 
 #if defined(__prefer_thumb__)
 
-	FUNC_START divsi3	
+	FUNC_START divsi3
 	FUNC_ALIAS aeabi_idiv divsi3
+#if defined(__OPTIMIZE_SIZE__)
 
 	cmp	divisor, #0
 	beq	LSYM(Ldiv0)
@@ -1115,7 +1253,7 @@ LSYM(Lover11):
 	blo	LSYM(Lgot_result)
 
 	THUMB_DIV_MOD_BODY 0
-	
+
 	mov	r0, result
 	mov	work, ip
 	cmp	work, #0
@@ -1124,6 +1262,20 @@ LSYM(Lover11):
 LSYM(Lover12):
 	pop	{ work }
 	RET
+#else
+
+LSYM(divsi3_skip_div0_test):
+	cpy	curbit, dividend
+	orr	curbit, divisor
+	bmi	LSYM(Lthumb1_div_negative)
+
+LSYM(Lthumb1_div_positive):
+	THUMB1_Div_Positive
+
+LSYM(Lthumb1_div_negative):
+	THUMB1_Div_Negative
+
+#endif /* __OPTIMIZE_SIZE__ */
 
 #elif defined(__ARM_ARCH_EXT_IDIV__)
 
@@ -1136,8 +1288,8 @@ LSYM(Lover12):
 	RET
 
 #else /* ARM/Thumb-2 version.  */
-	
-	ARM_FUNC_START divsi3	
+
+	ARM_FUNC_START divsi3
 	ARM_FUNC_ALIAS aeabi_idiv divsi3
 
 	cmp	r1, #0
